{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# MNIST Training using PyTorch and Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and MXNet. This notebook example shows how to:\n",
    "- use Horovod with PyTorch\n",
    "- run distributed parallel training\n",
    "- leverage Spot instances\n",
    "- save checkpoints.\n",
    "\n",
    "You may have to raise the number of parallel spot instances in your account to be able to run it (the code uses **6 p3.2xlarge instances in parallel** during the hyperparameter optimization).\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on ml.p3.2xlarge notebook instances._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the [Amazon SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import time\n",
    "import numpy as np\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from IPython.display import HTML\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'sagemaker-pytorch-dist-demo' # or use `sagemaker_session.default_bucket()`\n",
    "prefix = 'sagemaker/DEMO-pytorch-mnist'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n",
    "In this example, we will ues MNIST dataset. MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.MNIST('data', download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-pytorch-dist-demo/sagemaker/DEMO-pytorch-mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
    "print(f'input spec (in this case, just an S3 path): {inputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train\n",
    "### Training script\n",
    "The `mnist_ckppoint.py` script provides the code we need for training a SageMaker model.\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_NUM_GPUS`: The number of gpus available in the current container.\n",
    "* `SM_CURRENT_HOST`: The name of the current container on the container network.\n",
    "* `SM_HOSTS`: JSON encoded list containing all the hosts .\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "This script uses Horovod framework for distributed training where Horovod-related lines are commented with `Horovod:`. For example, `hvd.broadcast_parameters`, `hvd.DistributedOptimizer` and etc.\n",
    "\n",
    "For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#from __future__ import print_function\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mhorovod.torch\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mhvd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn.functional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data.distributed\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m NLLLoss\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel_def\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Net\r\n",
      "\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir, **kwargs):\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data sampler and data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    dataset = datasets.MNIST(training_dir, train=\u001b[36mTrue\u001b[39;49;00m, transform=transforms.Compose([\r\n",
      "        transforms.ToTensor(),\r\n",
      "        transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\r\n",
      "    ]))\r\n",
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=hvd.size(), rank=hvd.rank())\r\n",
      "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, **kwargs)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_loader\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(test_batch_size, training_dir, **kwargs):\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet test data sampler and data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    dataset = datasets.MNIST(training_dir, train=\u001b[36mFalse\u001b[39;49;00m, transform=transforms.Compose([\r\n",
      "        transforms.ToTensor(),\r\n",
      "        transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\r\n",
      "    ]))\r\n",
      "    test_sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=hvd.size(), rank=hvd.rank())\r\n",
      "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=test_batch_size, sampler=test_sampler, **kwargs)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m test_loader\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_checkpoint\u001b[39;49;00m(state, filename=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints/checkpoint.pth.tar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving checkpoint for epoch {state[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m]} with accuracy {state[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mbest_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m]:.3f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    torch.save(state, filename)  \u001b[37m# save checkpoint\u001b[39;49;00m\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_checkpoint\u001b[39;49;00m(filename=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints/checkpoint.pth.tar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m (torch.load(filename) \u001b[34mif\u001b[39;49;00m os.path.exists(filename) \u001b[34melse\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\r\n",
      "    logger.debug(f\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - {args.num_gpus}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Horovod: initialize library\u001b[39;49;00m\r\n",
      "    hvd.init()\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "    model = Net()\r\n",
      "\r\n",
      "    start_epoch = \u001b[34m1\u001b[39;49;00m\r\n",
      "    best_accuracy = \u001b[34m0\u001b[39;49;00m\r\n",
      "    checkpoint = load_checkpoint()\r\n",
      "    \u001b[34mif\u001b[39;49;00m checkpoint:\r\n",
      "        start_epoch = checkpoint[\u001b[33m'\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        best_accuracy = checkpoint[\u001b[33m'\u001b[39;49;00m\u001b[33mbest_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        model.load_state_dict(checkpoint[\u001b[33m'\u001b[39;49;00m\u001b[33mstate_dict\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33mResuming from epoch {start_epoch} with best accuracy of {best_accuracy:.3f} so far\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting training from the beginning\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33mTest checkpoint is {args.test_checkpoint}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Horovod: pin GPU to local local rank\u001b[39;49;00m\r\n",
      "    torch.cuda.set_device(hvd.local_rank())\r\n",
      "    torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "    \u001b[37m# Horovod: limit number of CPU threads to be used per worker\u001b[39;49;00m\r\n",
      "    torch.set_num_threads(\u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mTrue\u001b[39;49;00m}\r\n",
      "\r\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, **kwargs)\r\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir, **kwargs)\r\n",
      "\r\n",
      "    logger.debug(f\u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses {len(train_loader.sampler)}/{len(train_loader.dataset)} \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "                 f\u001b[33m\"\u001b[39;49;00m\u001b[33m({100. * len(train_loader.sampler) / len(train_loader.dataset):.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    logger.debug(f\u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses {len(test_loader.sampler)}/{len(test_loader.dataset)} \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "                 f\u001b[33m\"\u001b[39;49;00m\u001b[33m({100. * len(test_loader.sampler) / len(test_loader.dataset):.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of test data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    lr_scaler = hvd.size()\r\n",
      "    model.cuda()\r\n",
      "\r\n",
      "    \u001b[37m# Horovod: scale learning rate by lr_scaler.\u001b[39;49;00m\r\n",
      "    optimizer = optim.SGD(model.parameters(), lr=args.lr * lr_scaler, momentum=args.momentum)\r\n",
      "\r\n",
      "    \u001b[37m# Horovod: broadcast parameters & optimizer state.\u001b[39;49;00m\r\n",
      "    hvd.broadcast_parameters(model.state_dict(), root_rank=\u001b[34m0\u001b[39;49;00m)\r\n",
      "    hvd.broadcast_optimizer_state(optimizer, root_rank=\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Horovod: wrap optimizer with DistributedOptimizer.\u001b[39;49;00m\r\n",
      "    optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\r\n",
      "\r\n",
      "    accuracy = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(start_epoch, args.epochs + \u001b[34m1\u001b[39;49;00m):\r\n",
      "        \u001b[34massert\u001b[39;49;00m (args.test_checkpoint != epoch), \u001b[33m\"\u001b[39;49;00m\u001b[33mInterrupting the training for checkpoint testing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        is_best = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "        model.train()\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\r\n",
      "            data, target = data.cuda(), target.cuda()\r\n",
      "            optimizer.zero_grad()\r\n",
      "            output = model(data)\r\n",
      "            loss = F.nll_loss(output, target)\r\n",
      "            loss.backward()\r\n",
      "            optimizer.step()\r\n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                logger.info(f\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.sampler)} \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                            f\u001b[33m'\u001b[39;49;00m\u001b[33m({100. * batch_idx / len(train_loader):.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: {loss.item():.6f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        accuracy = test(epoch, model, test_loader)\r\n",
      "        \u001b[34mif\u001b[39;49;00m (best_accuracy < accuracy) \u001b[35mand\u001b[39;49;00m (hvd.rank() == \u001b[34m0\u001b[39;49;00m):\r\n",
      "            best_accuracy = accuracy\r\n",
      "            is_best = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "            save_checkpoint(\r\n",
      "                {\r\n",
      "                    \u001b[33m'\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: epoch,\r\n",
      "                    \u001b[33m'\u001b[39;49;00m\u001b[33mstate_dict\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: model.state_dict(),\r\n",
      "                    \u001b[33m'\u001b[39;49;00m\u001b[33mbest_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: best_accuracy\r\n",
      "                })\r\n",
      "    save_model(model, args.model_dir)\r\n",
      "    os.remove(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints/checkpoint.pth.tar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_metric_average\u001b[39;49;00m(val, name):\r\n",
      "    tensor = torch.tensor(val)\r\n",
      "    avg_tensor = hvd.allreduce(tensor, name=name)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m avg_tensor.item()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(epoch, model, test_loader):\r\n",
      "    model.eval()\r\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "    test_accuracy = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\r\n",
      "            data, target = data.cuda(), target.cuda()\r\n",
      "            output = model(data)\r\n",
      "            test_loss += F.nll_loss(output, target, size_average=\u001b[36mFalse\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\r\n",
      "            pred = output.max(\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[36mTrue\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m]  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\r\n",
      "            test_accuracy += pred.eq(target.view_as(pred)).sum().item()\r\n",
      "\r\n",
      "    \u001b[37m# Horovod: use test_sampler to determine the number of examples in this worker's partition.\u001b[39;49;00m\r\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.sampler)\r\n",
      "    test_accuracy /= \u001b[36mlen\u001b[39;49;00m(test_loader.sampler)\r\n",
      "\r\n",
      "    \u001b[37m# Horovod: average metric values across workers.\u001b[39;49;00m\r\n",
      "    test_loss = _metric_average(test_loss, \u001b[33m'\u001b[39;49;00m\u001b[33mavg_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    test_accuracy = _metric_average(test_accuracy, \u001b[33m'\u001b[39;49;00m\u001b[33mavg_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    logger.info(f\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch: {epoch}\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mTest set: Average loss: {test_loss:.4f}, Accuracy: {100 * test_accuracy:.2f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m test_accuracy\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\r\n",
      "    path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\r\n",
      "    torch.save(model.cpu().state_dict(), path)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for testing (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-checkpoint\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mEnables container interrupt test for checkpoint restore at the Nth epoch (0 for no interruption)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m42\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 42)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mgloo\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Container Environment\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    train(parser.parse_args())\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/mnist_ckpoint.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Run training in SageMaker\n",
    "\n",
    "The `PyTorch` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. In this case we are going to run our training job on 2 ```ml.p3.xlarge``` instances. But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/)). The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the `mnist_ckpoint.py` script above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    {\n",
    "        \"Name\": \"Test Accuracy\",\n",
    "        \"Regex\": \"Test set:.+Accuracy: (\\d+(?:\\.\\d+))%\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"Test Loss\",\n",
    "        \"Regex\": \"Test set:.+Average loss: (\\d+(?:\\.\\d+)),.+\" \n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"Epoch\",\n",
    "        \"Regex\": 'Train Epoch: (\\d+)'\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"Epoch completion\",\n",
    "        \"Regex\": 'Train Epoch: \\d+ \\[\\d+/\\d+ \\((\\d+%)\\)\\]'\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"Train loss\",\n",
    "        \"Regex\": 'Train Epoch: \\d+ \\[\\d+/\\d+ \\(\\d+%\\)\\] Loss: (\\d+\\.\\d+)'\n",
    "    },\n",
    "]\n",
    "\n",
    "estimator = PyTorch(entry_point='mnist_ckpoint.py',\n",
    "                    source_dir='code',\n",
    "                    role=role,\n",
    "                    framework_version='1.3.1',\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    metric_definitions=metrics,\n",
    "                    train_use_spot_instances=True,\n",
    "                    train_max_wait=25*60*60,\n",
    "                    train_instance_count=2,\n",
    "                    checkpoint_s3_uri=f's3://{bucket}/checkpoints',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 5,\n",
    "                        'backend': 'nccl',\n",
    "                        'test-checkpoint': 4\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In order to demonstrate the checkpoint resume capability, we'll tell the script to raise an exception at the beginning of the fourth epoch (hyperparameter `'test-checkpoint': 4`) above.\n",
    "\n",
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n",
    "\n",
    "**The next cell is designed to fail at the 4th epoch. When it does, just resume the notebook execution from the one after it**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-17 11:14:06 Starting - Starting the training job...\n",
      "2020-02-17 11:14:08 Starting - Launching requested ML instances......\n",
      "2020-02-17 11:15:16 Starting - Preparing the instances for training......\n",
      "2020-02-17 11:16:20 Downloading - Downloading input data...\n",
      "2020-02-17 11:16:53 Training - Downloading the training image.......\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-02-17 11:18:10,314 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-02-17 11:18:10,339 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-02-17 11:18:13,059 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-02-17 11:18:13,084 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:18:16,110 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:18:16,392 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-02-17 11:18:16,392 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-02-17 11:18:16,393 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-02-17 11:18:16,393 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35m2020-02-17 11:18:16,559 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpon2qeijr/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22891 sha256=de99dcc797bbde5d18b67f83fb29563056298a0654d32b79d03fdd30f087ef2e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-iamm7lqm/wheels/5e/0b/42/4d632d16b53de2f7c74d38116c0cea98b8325fdbd758af4865\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[35m2020-02-17 11:18:16,839 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-02-17 11:18:16,840 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-02-17 11:18:16,840 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-02-17 11:18:16,840 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmpxslb65_o/module_dir\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:18:18,525 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"test-checkpoint\": 4,\n",
      "        \"backend\": \"nccl\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-02-17-11-14-06-031\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-14-06-031/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_ckpoint\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_ckpoint.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"nccl\",\"epochs\":5,\"test-checkpoint\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist_ckpoint.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist_ckpoint\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-14-06-031/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"nccl\",\"epochs\":5,\"test-checkpoint\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-02-17-11-14-06-031\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-14-06-031/source/sourcedir.tar.gz\",\"module_name\":\"mnist_ckpoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_ckpoint.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"nccl\",\"--epochs\",\"5\",\"--test-checkpoint\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TEST-CHECKPOINT=4\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python mnist_ckpoint.py --backend nccl --epochs 5 --test-checkpoint 4\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22887 sha256=d0809f7c75a2b165b362825a719cc52f08d7c2b2c3fb61a296ce3f1e40a23a86\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uc3jrtf6/wheels/6f/17/1c/a94732e8eb25a1b7fd7e82b0611a4dd21f5dbbcf1b41a6a70b\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[35mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[35mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2020-02-17 11:18:19,083 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"test-checkpoint\": 4,\n",
      "        \"backend\": \"nccl\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2020-02-17-11-14-06-031\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-14-06-031/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_ckpoint\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_ckpoint.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"nccl\",\"epochs\":5,\"test-checkpoint\":4}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist_ckpoint.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist_ckpoint\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-14-06-031/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"nccl\",\"epochs\":5,\"test-checkpoint\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2020-02-17-11-14-06-031\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-14-06-031/source/sourcedir.tar.gz\",\"module_name\":\"mnist_ckpoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_ckpoint.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"nccl\",\"--epochs\",\"5\",\"--test-checkpoint\",\"4\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_TEST-CHECKPOINT=4\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python mnist_ckpoint.py --backend nccl --epochs 5 --test-checkpoint 4\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-02-17 11:18:12 Training - Training image download completed. Training in progress.\u001b[34mINFO:__main__:Train Epoch: 1 [640/60000 (1%)] Loss: 2.304615\u001b[0m\n",
      "\u001b[34mStarting training from the beginning\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [640/60000 (1%)] Loss: 2.304614\u001b[0m\n",
      "\u001b[35mStarting training from the beginning\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.312132\u001b[0m\n",
      "\u001b[35mTest checkpoint is 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.312131\u001b[0m\n",
      "\u001b[34mTest checkpoint is 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1920/60000 (3%)] Loss: 2.291755\u001b[0m\n",
      "\u001b[34mGet train data sampler and data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.294058\u001b[0m\n",
      "\u001b[34mGet test data sampler and data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/60000 (5%)] Loss: 2.291983\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 2.259225\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [4480/60000 (7%)] Loss: 2.270091\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:18:23.525 algo-1:46 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 2.273199\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:18:23.525 algo-1:46 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [1920/60000 (3%)] Loss: 2.291734\u001b[0m\n",
      "\u001b[35mGet train data sampler and data loader\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.293818\u001b[0m\n",
      "\u001b[35mGet test data sampler and data loader\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [3200/60000 (5%)] Loss: 2.291899\u001b[0m\n",
      "\u001b[35mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 2.259277\u001b[0m\n",
      "\u001b[35mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [4480/60000 (7%)] Loss: 2.269309\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:18:23.539 algo-2:46 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 2.272827\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:18:23.539 algo-2:46 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [5760/60000 (10%)] Loss: 2.240711\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:18:23.539 algo-2:46 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 2.218936\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:18:23.540 algo-2:46 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5760/60000 (10%)] Loss: 2.240260\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:18:23.526 algo-1:46 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 2.219095\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:18:23.526 algo-1:46 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7040/60000 (12%)] Loss: 2.242404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [640/60000 (1%)] Loss: 2.304615\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 2.145658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1280/60000 (2%)] Loss: 2.312131\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8320/60000 (14%)] Loss: 2.101541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1920/60000 (3%)] Loss: 2.291755\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 2.152929\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2560/60000 (4%)] Loss: 2.294058\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [9600/60000 (16%)] Loss: 2.026896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/60000 (5%)] Loss: 2.291983\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [7040/60000 (12%)] Loss: 2.241756\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [640/60000 (1%)] Loss: 2.304614\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 2.143724\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [1280/60000 (2%)] Loss: 2.312132\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [8320/60000 (14%)] Loss: 2.101925\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [1920/60000 (3%)] Loss: 2.291734\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 2.151304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [2560/60000 (4%)] Loss: 2.293818\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [9600/60000 (16%)] Loss: 2.023522\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [3200/60000 (5%)] Loss: 2.291899\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [10240/60000 (17%)] Loss: 1.827971\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [3840/60000 (6%)] Loss: 2.259277\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [10880/60000 (18%)] Loss: 1.706553\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [4480/60000 (7%)] Loss: 2.269309\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10240/60000 (17%)] Loss: 1.830962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3840/60000 (6%)] Loss: 2.259225\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10880/60000 (18%)] Loss: 1.715244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [4480/60000 (7%)] Loss: 2.270091\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [11520/60000 (19%)] Loss: 1.782036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5120/60000 (9%)] Loss: 2.273199\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12160/60000 (20%)] Loss: 1.597787\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5760/60000 (10%)] Loss: 2.240260\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)] Loss: 1.538395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)] Loss: 2.219095\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [13440/60000 (22%)] Loss: 1.445780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7040/60000 (12%)] Loss: 2.242404\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14080/60000 (23%)] Loss: 1.310982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7680/60000 (13%)] Loss: 2.145658\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [11520/60000 (19%)] Loss: 1.780761\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [5120/60000 (9%)] Loss: 2.272827\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12160/60000 (20%)] Loss: 1.596794\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [5760/60000 (10%)] Loss: 2.240711\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)] Loss: 1.530881\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [6400/60000 (11%)] Loss: 2.218936\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [13440/60000 (22%)] Loss: 1.444683\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [7040/60000 (12%)] Loss: 2.241756\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [14080/60000 (23%)] Loss: 1.316386\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [7680/60000 (13%)] Loss: 2.143724\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [14720/60000 (25%)] Loss: 0.963505\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [8320/60000 (14%)] Loss: 2.101925\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [15360/60000 (26%)] Loss: 1.094985\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [8960/60000 (15%)] Loss: 2.151304\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14720/60000 (25%)] Loss: 0.964348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8320/60000 (14%)] Loss: 2.101541\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [15360/60000 (26%)] Loss: 1.097690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8960/60000 (15%)] Loss: 2.152929\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16000/60000 (27%)] Loss: 1.141092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [9600/60000 (16%)] Loss: 2.026896\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16640/60000 (28%)] Loss: 1.193740\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10240/60000 (17%)] Loss: 1.830962\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17280/60000 (29%)] Loss: 0.852487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10880/60000 (18%)] Loss: 1.715244\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17920/60000 (30%)] Loss: 1.241178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [11520/60000 (19%)] Loss: 1.782036\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [18560/60000 (31%)] Loss: 1.007236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12160/60000 (20%)] Loss: 1.597787\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [16000/60000 (27%)] Loss: 1.142972\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [9600/60000 (16%)] Loss: 2.023522\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [16640/60000 (28%)] Loss: 1.189747\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [10240/60000 (17%)] Loss: 1.827971\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [17280/60000 (29%)] Loss: 0.858050\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [10880/60000 (18%)] Loss: 1.706553\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [17920/60000 (30%)] Loss: 1.255457\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [11520/60000 (19%)] Loss: 1.780761\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [18560/60000 (31%)] Loss: 1.009188\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12160/60000 (20%)] Loss: 1.596794\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)] Loss: 0.837179\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/60000 (21%)] Loss: 1.530881\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)] Loss: 0.827731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)] Loss: 1.538395\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19840/60000 (33%)] Loss: 1.074296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [13440/60000 (22%)] Loss: 1.445780\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [20480/60000 (34%)] Loss: 0.738656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14080/60000 (23%)] Loss: 1.310982\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [21120/60000 (35%)] Loss: 0.595940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14720/60000 (25%)] Loss: 0.964348\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [21760/60000 (36%)] Loss: 0.913359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [15360/60000 (26%)] Loss: 1.097690\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [22400/60000 (37%)] Loss: 0.932164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16000/60000 (27%)] Loss: 1.141092\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [23040/60000 (38%)] Loss: 0.958393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16640/60000 (28%)] Loss: 1.193740\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19840/60000 (33%)] Loss: 1.074096\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [13440/60000 (22%)] Loss: 1.444683\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [20480/60000 (34%)] Loss: 0.720260\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [14080/60000 (23%)] Loss: 1.316386\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [21120/60000 (35%)] Loss: 0.594824\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [14720/60000 (25%)] Loss: 0.963505\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [21760/60000 (36%)] Loss: 0.920185\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [15360/60000 (26%)] Loss: 1.094985\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [22400/60000 (37%)] Loss: 0.945955\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [16000/60000 (27%)] Loss: 1.142972\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [23040/60000 (38%)] Loss: 0.959031\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [16640/60000 (28%)] Loss: 1.189747\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [23680/60000 (39%)] Loss: 0.872368\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [17280/60000 (29%)] Loss: 0.858050\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [24320/60000 (41%)] Loss: 0.917179\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [17920/60000 (30%)] Loss: 1.255457\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [23680/60000 (39%)] Loss: 0.889941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [17280/60000 (29%)] Loss: 0.852487\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [24320/60000 (41%)] Loss: 0.931165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [17920/60000 (30%)] Loss: 1.241178\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [24960/60000 (42%)] Loss: 0.691616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [18560/60000 (31%)] Loss: 1.007236\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)] Loss: 0.781822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)] Loss: 0.827731\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [26240/60000 (44%)] Loss: 0.632963\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19840/60000 (33%)] Loss: 1.074296\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [26880/60000 (45%)] Loss: 0.524712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [20480/60000 (34%)] Loss: 0.738656\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [27520/60000 (46%)] Loss: 0.707324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [21120/60000 (35%)] Loss: 0.595940\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [28160/60000 (47%)] Loss: 0.641527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [21760/60000 (36%)] Loss: 0.913359\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [24960/60000 (42%)] Loss: 0.691484\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [18560/60000 (31%)] Loss: 1.009188\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)] Loss: 0.780470\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19200/60000 (32%)] Loss: 0.837179\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [26240/60000 (44%)] Loss: 0.626546\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19840/60000 (33%)] Loss: 1.074096\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [26880/60000 (45%)] Loss: 0.522827\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [20480/60000 (34%)] Loss: 0.720260\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [27520/60000 (46%)] Loss: 0.702136\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [21120/60000 (35%)] Loss: 0.594824\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [28160/60000 (47%)] Loss: 0.639676\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [21760/60000 (36%)] Loss: 0.920185\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [28800/60000 (48%)] Loss: 0.618777\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [22400/60000 (37%)] Loss: 0.945955\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [28800/60000 (48%)] Loss: 0.604308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [22400/60000 (37%)] Loss: 0.932164\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [29440/60000 (49%)] Loss: 0.602750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [23040/60000 (38%)] Loss: 0.958393\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [30080/60000 (50%)] Loss: 0.771171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [23680/60000 (39%)] Loss: 0.889941\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [30720/60000 (51%)] Loss: 0.965248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [24320/60000 (41%)] Loss: 0.931165\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [31360/60000 (52%)] Loss: 0.612834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [24960/60000 (42%)] Loss: 0.691616\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)] Loss: 0.602342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.781822\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32640/60000 (54%)] Loss: 0.617510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [26240/60000 (44%)] Loss: 0.632963\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [29440/60000 (49%)] Loss: 0.600116\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [23040/60000 (38%)] Loss: 0.959031\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [30080/60000 (50%)] Loss: 0.755639\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [23680/60000 (39%)] Loss: 0.872368\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [30720/60000 (51%)] Loss: 0.974858\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [24320/60000 (41%)] Loss: 0.917179\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [31360/60000 (52%)] Loss: 0.594564\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [24960/60000 (42%)] Loss: 0.691484\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)] Loss: 0.585964\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.780470\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [32640/60000 (54%)] Loss: 0.585063\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [26240/60000 (44%)] Loss: 0.626546\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [33280/60000 (55%)] Loss: 0.461115\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [26880/60000 (45%)] Loss: 0.522827\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [33280/60000 (55%)] Loss: 0.439259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [26880/60000 (45%)] Loss: 0.524712\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [33920/60000 (57%)] Loss: 0.545377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [27520/60000 (46%)] Loss: 0.707324\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [34560/60000 (58%)] Loss: 0.601812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [28160/60000 (47%)] Loss: 0.641527\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [35200/60000 (59%)] Loss: 0.790327\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [28800/60000 (48%)] Loss: 0.604308\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [35840/60000 (60%)] Loss: 0.681572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [29440/60000 (49%)] Loss: 0.602750\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [36480/60000 (61%)] Loss: 0.702572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [30080/60000 (50%)] Loss: 0.771171\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [37120/60000 (62%)] Loss: 0.947972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [30720/60000 (51%)] Loss: 0.965248\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [33920/60000 (57%)] Loss: 0.537313\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [27520/60000 (46%)] Loss: 0.702136\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [34560/60000 (58%)] Loss: 0.612457\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [28160/60000 (47%)] Loss: 0.639676\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [35200/60000 (59%)] Loss: 0.775702\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [28800/60000 (48%)] Loss: 0.618777\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [35840/60000 (60%)] Loss: 0.677434\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [29440/60000 (49%)] Loss: 0.600116\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [36480/60000 (61%)] Loss: 0.719881\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [30080/60000 (50%)] Loss: 0.755639\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [37120/60000 (62%)] Loss: 0.947347\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [30720/60000 (51%)] Loss: 0.974858\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [37760/60000 (63%)] Loss: 0.582867\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [31360/60000 (52%)] Loss: 0.594564\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [37760/60000 (63%)] Loss: 0.586345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [31360/60000 (52%)] Loss: 0.612834\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)] Loss: 0.636209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)] Loss: 0.602342\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [39040/60000 (65%)] Loss: 0.670400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32640/60000 (54%)] Loss: 0.617510\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [39680/60000 (66%)] Loss: 0.831303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [33280/60000 (55%)] Loss: 0.439259\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [40320/60000 (67%)] Loss: 0.694420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [33920/60000 (57%)] Loss: 0.545377\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [40960/60000 (68%)] Loss: 0.693431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [34560/60000 (58%)] Loss: 0.601812\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [41600/60000 (69%)] Loss: 0.583254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [35200/60000 (59%)] Loss: 0.790327\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)] Loss: 0.633877\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [32000/60000 (53%)] Loss: 0.585964\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [39040/60000 (65%)] Loss: 0.655600\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [32640/60000 (54%)] Loss: 0.585063\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [39680/60000 (66%)] Loss: 0.837099\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [33280/60000 (55%)] Loss: 0.461115\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [40320/60000 (67%)] Loss: 0.690240\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [33920/60000 (57%)] Loss: 0.537313\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [40960/60000 (68%)] Loss: 0.701980\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [34560/60000 (58%)] Loss: 0.612457\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [41600/60000 (69%)] Loss: 0.572289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [35200/60000 (59%)] Loss: 0.775702\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [42240/60000 (70%)] Loss: 0.577458\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [35840/60000 (60%)] Loss: 0.677434\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [42880/60000 (71%)] Loss: 0.647905\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [36480/60000 (61%)] Loss: 0.719881\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Train Epoch: 1 [42240/60000 (70%)] Loss: 0.578561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [35840/60000 (60%)] Loss: 0.681572\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [42880/60000 (71%)] Loss: 0.652027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [36480/60000 (61%)] Loss: 0.702572\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [43520/60000 (72%)] Loss: 0.670639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [37120/60000 (62%)] Loss: 0.947972\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44160/60000 (74%)] Loss: 0.734596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [37760/60000 (63%)] Loss: 0.586345\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)] Loss: 0.658686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.636209\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [45440/60000 (76%)] Loss: 0.435589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [39040/60000 (65%)] Loss: 0.670400\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [46080/60000 (77%)] Loss: 0.610422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [39680/60000 (66%)] Loss: 0.831303\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [46720/60000 (78%)] Loss: 0.688221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [40320/60000 (67%)] Loss: 0.694420\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [43520/60000 (72%)] Loss: 0.695250\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [37120/60000 (62%)] Loss: 0.947347\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [44160/60000 (74%)] Loss: 0.730602\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [37760/60000 (63%)] Loss: 0.582867\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)] Loss: 0.689333\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.633877\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [45440/60000 (76%)] Loss: 0.435044\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [39040/60000 (65%)] Loss: 0.655600\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [46080/60000 (77%)] Loss: 0.614693\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [39680/60000 (66%)] Loss: 0.837099\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [46720/60000 (78%)] Loss: 0.675602\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [40320/60000 (67%)] Loss: 0.690240\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [47360/60000 (79%)] Loss: 0.599071\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [40960/60000 (68%)] Loss: 0.701980\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [47360/60000 (79%)] Loss: 0.595887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [40960/60000 (68%)] Loss: 0.693431\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [48000/60000 (80%)] Loss: 0.438291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [41600/60000 (69%)] Loss: 0.583254\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [48640/60000 (81%)] Loss: 0.615952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [42240/60000 (70%)] Loss: 0.578561\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [49280/60000 (82%)] Loss: 0.493627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [42880/60000 (71%)] Loss: 0.652027\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [49920/60000 (83%)] Loss: 0.588574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [43520/60000 (72%)] Loss: 0.670639\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [50560/60000 (84%)] Loss: 0.560617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44160/60000 (74%)] Loss: 0.734596\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)] Loss: 0.697095\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)] Loss: 0.658686\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [48000/60000 (80%)] Loss: 0.447848\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [41600/60000 (69%)] Loss: 0.572289\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [48640/60000 (81%)] Loss: 0.596610\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [42240/60000 (70%)] Loss: 0.577458\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [49280/60000 (82%)] Loss: 0.490495\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [42880/60000 (71%)] Loss: 0.647905\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [49920/60000 (83%)] Loss: 0.593058\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [43520/60000 (72%)] Loss: 0.695250\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [50560/60000 (84%)] Loss: 0.578594\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [44160/60000 (74%)] Loss: 0.730602\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)] Loss: 0.664744\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [44800/60000 (75%)] Loss: 0.689333\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [51840/60000 (86%)] Loss: 0.365830\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [45440/60000 (76%)] Loss: 0.435044\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51840/60000 (86%)] Loss: 0.365182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [45440/60000 (76%)] Loss: 0.435589\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [52480/60000 (87%)] Loss: 0.357024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [46080/60000 (77%)] Loss: 0.610422\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [53120/60000 (88%)] Loss: 0.571931\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [46720/60000 (78%)] Loss: 0.688221\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [53760/60000 (90%)] Loss: 0.583758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [47360/60000 (79%)] Loss: 0.595887\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [54400/60000 (91%)] Loss: 0.401018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [48000/60000 (80%)] Loss: 0.438291\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [55040/60000 (92%)] Loss: 0.678986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [48640/60000 (81%)] Loss: 0.615952\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [55680/60000 (93%)] Loss: 0.464060\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [49280/60000 (82%)] Loss: 0.493627\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [52480/60000 (87%)] Loss: 0.352741\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [46080/60000 (77%)] Loss: 0.614693\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [53120/60000 (88%)] Loss: 0.587418\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [46720/60000 (78%)] Loss: 0.675602\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [53760/60000 (90%)] Loss: 0.590395\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [47360/60000 (79%)] Loss: 0.599071\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [54400/60000 (91%)] Loss: 0.399100\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [48000/60000 (80%)] Loss: 0.447848\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [55040/60000 (92%)] Loss: 0.680151\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [48640/60000 (81%)] Loss: 0.596610\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [55680/60000 (93%)] Loss: 0.474605\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [49280/60000 (82%)] Loss: 0.490495\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [56320/60000 (94%)] Loss: 0.323984\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [49920/60000 (83%)] Loss: 0.593058\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [56320/60000 (94%)] Loss: 0.317471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [49920/60000 (83%)] Loss: 0.588574\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [56960/60000 (95%)] Loss: 0.566832\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [50560/60000 (84%)] Loss: 0.560617\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)] Loss: 0.348050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.697095\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [58240/60000 (97%)] Loss: 0.307573\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51840/60000 (86%)] Loss: 0.365182\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [58880/60000 (98%)] Loss: 0.425404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [52480/60000 (87%)] Loss: 0.357024\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [59520/60000 (99%)] Loss: 0.469284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [53120/60000 (88%)] Loss: 0.571931\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [56960/60000 (95%)] Loss: 0.555805\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [50560/60000 (84%)] Loss: 0.578594\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)] Loss: 0.357811\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.664744\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [58240/60000 (97%)] Loss: 0.317995\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [51840/60000 (86%)] Loss: 0.365830\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [58880/60000 (98%)] Loss: 0.413098\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [52480/60000 (87%)] Loss: 0.352741\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [59520/60000 (99%)] Loss: 0.464581\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [53120/60000 (88%)] Loss: 0.587418\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [53760/60000 (90%)] Loss: 0.590395\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [54400/60000 (91%)] Loss: 0.399100\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [53760/60000 (90%)] Loss: 0.583758\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [54400/60000 (91%)] Loss: 0.401018\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 1#011Test set: Average loss: 0.2013, Accuracy: 93.88%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [55040/60000 (92%)] Loss: 0.678986\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [55680/60000 (93%)] Loss: 0.464060\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 1#011Test set: Average loss: 0.2009, Accuracy: 94.05%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [55040/60000 (92%)] Loss: 0.680151\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [55680/60000 (93%)] Loss: 0.474605\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [640/60000 (1%)] Loss: 0.700058\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [56320/60000 (94%)] Loss: 0.323984\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [1280/60000 (2%)] Loss: 0.487832\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [56960/60000 (95%)] Loss: 0.555805\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [640/60000 (1%)] Loss: 0.728956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [56320/60000 (94%)] Loss: 0.317471\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [1280/60000 (2%)] Loss: 0.496761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [56960/60000 (95%)] Loss: 0.566832\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [1920/60000 (3%)] Loss: 0.444945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)] Loss: 0.348050\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [2560/60000 (4%)] Loss: 0.306844\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [58240/60000 (97%)] Loss: 0.307573\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3200/60000 (5%)] Loss: 0.649742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [58880/60000 (98%)] Loss: 0.425404\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3840/60000 (6%)] Loss: 0.435952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [59520/60000 (99%)] Loss: 0.469284\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [4480/60000 (7%)] Loss: 0.355907\u001b[0m\n",
      "\u001b[34mEpoch: 1#011Test set: Average loss: 0.2013, Accuracy: 93.88%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [1920/60000 (3%)] Loss: 0.434912\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [57600/60000 (96%)] Loss: 0.357811\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [2560/60000 (4%)] Loss: 0.331555\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [58240/60000 (97%)] Loss: 0.317995\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [3200/60000 (5%)] Loss: 0.647907\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [58880/60000 (98%)] Loss: 0.413098\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [3840/60000 (6%)] Loss: 0.430586\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [59520/60000 (99%)] Loss: 0.464581\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [4480/60000 (7%)] Loss: 0.364085\u001b[0m\n",
      "\u001b[35mEpoch: 1#011Test set: Average loss: 0.2009, Accuracy: 94.05%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [5120/60000 (9%)] Loss: 0.518247\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [5760/60000 (10%)] Loss: 0.601655\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 1 with accuracy 0.941\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [5120/60000 (9%)] Loss: 0.534058\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [5760/60000 (10%)] Loss: 0.593422\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 1 with accuracy 0.939\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)] Loss: 0.412394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [640/60000 (1%)] Loss: 0.728956\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [7040/60000 (12%)] Loss: 0.440413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1280/60000 (2%)] Loss: 0.496761\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [7680/60000 (13%)] Loss: 0.464011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1920/60000 (3%)] Loss: 0.444945\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [8320/60000 (14%)] Loss: 0.263103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [2560/60000 (4%)] Loss: 0.306844\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [8960/60000 (15%)] Loss: 0.353541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3200/60000 (5%)] Loss: 0.649742\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)] Loss: 0.394229\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [640/60000 (1%)] Loss: 0.700058\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [7040/60000 (12%)] Loss: 0.461580\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [1280/60000 (2%)] Loss: 0.487832\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [7680/60000 (13%)] Loss: 0.461925\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [1920/60000 (3%)] Loss: 0.434912\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [8320/60000 (14%)] Loss: 0.255405\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [2560/60000 (4%)] Loss: 0.331555\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [8960/60000 (15%)] Loss: 0.340774\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [3200/60000 (5%)] Loss: 0.647907\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [9600/60000 (16%)] Loss: 0.482525\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [3840/60000 (6%)] Loss: 0.430586\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [10240/60000 (17%)] Loss: 0.584850\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [4480/60000 (7%)] Loss: 0.364085\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [9600/60000 (16%)] Loss: 0.488806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3840/60000 (6%)] Loss: 0.435952\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [10240/60000 (17%)] Loss: 0.572477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [4480/60000 (7%)] Loss: 0.355907\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [10880/60000 (18%)] Loss: 0.464004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [5120/60000 (9%)] Loss: 0.534058\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [11520/60000 (19%)] Loss: 0.464065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [5760/60000 (10%)] Loss: 0.593422\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12160/60000 (20%)] Loss: 0.401854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)] Loss: 0.412394\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)] Loss: 0.514975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [7040/60000 (12%)] Loss: 0.440413\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [13440/60000 (22%)] Loss: 0.494417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [7680/60000 (13%)] Loss: 0.464011\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [10880/60000 (18%)] Loss: 0.505457\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [5120/60000 (9%)] Loss: 0.518247\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [11520/60000 (19%)] Loss: 0.472357\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [5760/60000 (10%)] Loss: 0.601655\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [12160/60000 (20%)] Loss: 0.407176\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [6400/60000 (11%)] Loss: 0.394229\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)] Loss: 0.492951\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [7040/60000 (12%)] Loss: 0.461580\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [13440/60000 (22%)] Loss: 0.494078\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [7680/60000 (13%)] Loss: 0.461925\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [14080/60000 (23%)] Loss: 0.448517\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [8320/60000 (14%)] Loss: 0.255405\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [14720/60000 (25%)] Loss: 0.186437\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [8960/60000 (15%)] Loss: 0.340774\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [15360/60000 (26%)] Loss: 0.337660\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [9600/60000 (16%)] Loss: 0.482525\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [14080/60000 (23%)] Loss: 0.446381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [8320/60000 (14%)] Loss: 0.263103\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [14720/60000 (25%)] Loss: 0.173765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [8960/60000 (15%)] Loss: 0.353541\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [15360/60000 (26%)] Loss: 0.329959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [9600/60000 (16%)] Loss: 0.488806\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [16000/60000 (27%)] Loss: 0.550346\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10240/60000 (17%)] Loss: 0.572477\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [16640/60000 (28%)] Loss: 0.481258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10880/60000 (18%)] Loss: 0.464004\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [17280/60000 (29%)] Loss: 0.294725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [11520/60000 (19%)] Loss: 0.464065\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [17920/60000 (30%)] Loss: 0.908997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12160/60000 (20%)] Loss: 0.401854\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [18560/60000 (31%)] Loss: 0.609053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.514975\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [16000/60000 (27%)] Loss: 0.539467\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [10240/60000 (17%)] Loss: 0.584850\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [16640/60000 (28%)] Loss: 0.500526\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [10880/60000 (18%)] Loss: 0.505457\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [17280/60000 (29%)] Loss: 0.285901\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [11520/60000 (19%)] Loss: 0.472357\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [17920/60000 (30%)] Loss: 0.897939\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12160/60000 (20%)] Loss: 0.407176\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [18560/60000 (31%)] Loss: 0.618473\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.492951\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)] Loss: 0.381157\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [13440/60000 (22%)] Loss: 0.494078\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [19840/60000 (33%)] Loss: 0.432843\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [14080/60000 (23%)] Loss: 0.448517\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)] Loss: 0.365597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [13440/60000 (22%)] Loss: 0.494417\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19840/60000 (33%)] Loss: 0.436345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [14080/60000 (23%)] Loss: 0.446381\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [20480/60000 (34%)] Loss: 0.180099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [14720/60000 (25%)] Loss: 0.173765\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [21120/60000 (35%)] Loss: 0.326352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [15360/60000 (26%)] Loss: 0.329959\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [21760/60000 (36%)] Loss: 0.598429\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [16000/60000 (27%)] Loss: 0.550346\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [22400/60000 (37%)] Loss: 0.408572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [16640/60000 (28%)] Loss: 0.481258\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [23040/60000 (38%)] Loss: 0.445631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [17280/60000 (29%)] Loss: 0.294725\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [20480/60000 (34%)] Loss: 0.167429\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [14720/60000 (25%)] Loss: 0.186437\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [21120/60000 (35%)] Loss: 0.304395\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [15360/60000 (26%)] Loss: 0.337660\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [21760/60000 (36%)] Loss: 0.586674\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [16000/60000 (27%)] Loss: 0.539467\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [22400/60000 (37%)] Loss: 0.424896\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [16640/60000 (28%)] Loss: 0.500526\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [23040/60000 (38%)] Loss: 0.443679\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [17280/60000 (29%)] Loss: 0.285901\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [23680/60000 (39%)] Loss: 0.420080\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [17920/60000 (30%)] Loss: 0.897939\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [24320/60000 (41%)] Loss: 0.576859\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [18560/60000 (31%)] Loss: 0.618473\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [23680/60000 (39%)] Loss: 0.409249\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [17920/60000 (30%)] Loss: 0.908997\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [24320/60000 (41%)] Loss: 0.575094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [18560/60000 (31%)] Loss: 0.609053\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [24960/60000 (42%)] Loss: 0.283301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)] Loss: 0.365597\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)] Loss: 0.307152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19840/60000 (33%)] Loss: 0.436345\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [26240/60000 (44%)] Loss: 0.267358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [20480/60000 (34%)] Loss: 0.180099\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [26880/60000 (45%)] Loss: 0.279142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [21120/60000 (35%)] Loss: 0.326352\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [27520/60000 (46%)] Loss: 0.423186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [21760/60000 (36%)] Loss: 0.598429\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [24960/60000 (42%)] Loss: 0.302404\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19200/60000 (32%)] Loss: 0.381157\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)] Loss: 0.307918\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19840/60000 (33%)] Loss: 0.432843\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [26240/60000 (44%)] Loss: 0.278859\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [20480/60000 (34%)] Loss: 0.167429\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [26880/60000 (45%)] Loss: 0.293179\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [21120/60000 (35%)] Loss: 0.304395\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [27520/60000 (46%)] Loss: 0.424827\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [21760/60000 (36%)] Loss: 0.586674\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [28160/60000 (47%)] Loss: 0.428971\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [22400/60000 (37%)] Loss: 0.424896\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [28800/60000 (48%)] Loss: 0.337768\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [23040/60000 (38%)] Loss: 0.443679\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [28160/60000 (47%)] Loss: 0.416947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [22400/60000 (37%)] Loss: 0.408572\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [28800/60000 (48%)] Loss: 0.334616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [23040/60000 (38%)] Loss: 0.445631\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [29440/60000 (49%)] Loss: 0.316835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [23680/60000 (39%)] Loss: 0.409249\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [30080/60000 (50%)] Loss: 0.291248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [24320/60000 (41%)] Loss: 0.575094\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [30720/60000 (51%)] Loss: 0.572555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [24960/60000 (42%)] Loss: 0.283301\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [31360/60000 (52%)] Loss: 0.538087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.307152\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)] Loss: 0.575392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [26240/60000 (44%)] Loss: 0.267358\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [29440/60000 (49%)] Loss: 0.319107\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [23680/60000 (39%)] Loss: 0.420080\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [30080/60000 (50%)] Loss: 0.300779\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [24320/60000 (41%)] Loss: 0.576859\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [30720/60000 (51%)] Loss: 0.548153\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [24960/60000 (42%)] Loss: 0.302404\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [31360/60000 (52%)] Loss: 0.550801\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.307918\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)] Loss: 0.571701\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [26240/60000 (44%)] Loss: 0.278859\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [32640/60000 (54%)] Loss: 0.451862\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [26880/60000 (45%)] Loss: 0.293179\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [33280/60000 (55%)] Loss: 0.386338\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [27520/60000 (46%)] Loss: 0.424827\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32640/60000 (54%)] Loss: 0.429783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [26880/60000 (45%)] Loss: 0.279142\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [33280/60000 (55%)] Loss: 0.382640\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [27520/60000 (46%)] Loss: 0.423186\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [33920/60000 (57%)] Loss: 0.340792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [28160/60000 (47%)] Loss: 0.416947\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [34560/60000 (58%)] Loss: 0.355870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [28800/60000 (48%)] Loss: 0.334616\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [35200/60000 (59%)] Loss: 0.365755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [29440/60000 (49%)] Loss: 0.316835\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [35840/60000 (60%)] Loss: 0.435311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30080/60000 (50%)] Loss: 0.291248\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [36480/60000 (61%)] Loss: 0.414630\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30720/60000 (51%)] Loss: 0.572555\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [33920/60000 (57%)] Loss: 0.361100\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [28160/60000 (47%)] Loss: 0.428971\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [34560/60000 (58%)] Loss: 0.355616\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [28800/60000 (48%)] Loss: 0.337768\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [35200/60000 (59%)] Loss: 0.367827\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [29440/60000 (49%)] Loss: 0.319107\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [35840/60000 (60%)] Loss: 0.437870\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [30080/60000 (50%)] Loss: 0.300779\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [36480/60000 (61%)] Loss: 0.392356\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [30720/60000 (51%)] Loss: 0.548153\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [37120/60000 (62%)] Loss: 0.400960\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [31360/60000 (52%)] Loss: 0.550801\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [37760/60000 (63%)] Loss: 0.313635\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [32000/60000 (53%)] Loss: 0.571701\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [37120/60000 (62%)] Loss: 0.390312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [31360/60000 (52%)] Loss: 0.538087\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [37760/60000 (63%)] Loss: 0.300364\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)] Loss: 0.575392\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)] Loss: 0.429095\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32640/60000 (54%)] Loss: 0.429783\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [39040/60000 (65%)] Loss: 0.310158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [33280/60000 (55%)] Loss: 0.382640\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [39680/60000 (66%)] Loss: 0.376576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [33920/60000 (57%)] Loss: 0.340792\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [40320/60000 (67%)] Loss: 0.542084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [34560/60000 (58%)] Loss: 0.355870\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [40960/60000 (68%)] Loss: 0.455414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [35200/60000 (59%)] Loss: 0.365755\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)] Loss: 0.420374\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [32640/60000 (54%)] Loss: 0.451862\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [39040/60000 (65%)] Loss: 0.316503\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [33280/60000 (55%)] Loss: 0.386338\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [39680/60000 (66%)] Loss: 0.368304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [33920/60000 (57%)] Loss: 0.361100\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [40320/60000 (67%)] Loss: 0.546762\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [34560/60000 (58%)] Loss: 0.355616\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [40960/60000 (68%)] Loss: 0.457406\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [35200/60000 (59%)] Loss: 0.367827\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [41600/60000 (69%)] Loss: 0.316164\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [35840/60000 (60%)] Loss: 0.437870\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [42240/60000 (70%)] Loss: 0.393249\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [36480/60000 (61%)] Loss: 0.392356\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [41600/60000 (69%)] Loss: 0.309381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [35840/60000 (60%)] Loss: 0.435311\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [42240/60000 (70%)] Loss: 0.404683\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [36480/60000 (61%)] Loss: 0.414630\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [42880/60000 (71%)] Loss: 0.370423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [37120/60000 (62%)] Loss: 0.390312\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [43520/60000 (72%)] Loss: 0.441798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [37760/60000 (63%)] Loss: 0.300364\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44160/60000 (74%)] Loss: 0.413338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.429095\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)] Loss: 0.588516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [39040/60000 (65%)] Loss: 0.310158\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [45440/60000 (76%)] Loss: 0.246499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [39680/60000 (66%)] Loss: 0.376576\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [42880/60000 (71%)] Loss: 0.350286\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [37120/60000 (62%)] Loss: 0.400960\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [43520/60000 (72%)] Loss: 0.428007\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [37760/60000 (63%)] Loss: 0.313635\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [44160/60000 (74%)] Loss: 0.425507\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.420374\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)] Loss: 0.571483\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [39040/60000 (65%)] Loss: 0.316503\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [45440/60000 (76%)] Loss: 0.239797\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [39680/60000 (66%)] Loss: 0.368304\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [46080/60000 (77%)] Loss: 0.374376\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [40320/60000 (67%)] Loss: 0.546762\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [46720/60000 (78%)] Loss: 0.481953\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [40960/60000 (68%)] Loss: 0.457406\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [46080/60000 (77%)] Loss: 0.376954\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [40320/60000 (67%)] Loss: 0.542084\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [46720/60000 (78%)] Loss: 0.464050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [40960/60000 (68%)] Loss: 0.455414\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [47360/60000 (79%)] Loss: 0.470215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [41600/60000 (69%)] Loss: 0.309381\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [48000/60000 (80%)] Loss: 0.282651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [42240/60000 (70%)] Loss: 0.404683\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [48640/60000 (81%)] Loss: 0.429368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [42880/60000 (71%)] Loss: 0.370423\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [49280/60000 (82%)] Loss: 0.221565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [43520/60000 (72%)] Loss: 0.441798\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [49920/60000 (83%)] Loss: 0.513324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44160/60000 (74%)] Loss: 0.413338\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [50560/60000 (84%)] Loss: 0.332847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)] Loss: 0.588516\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [47360/60000 (79%)] Loss: 0.472687\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [41600/60000 (69%)] Loss: 0.316164\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [48000/60000 (80%)] Loss: 0.282187\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [42240/60000 (70%)] Loss: 0.393249\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [48640/60000 (81%)] Loss: 0.426036\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [42880/60000 (71%)] Loss: 0.350286\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [49280/60000 (82%)] Loss: 0.241616\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [43520/60000 (72%)] Loss: 0.428007\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [49920/60000 (83%)] Loss: 0.519187\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [44160/60000 (74%)] Loss: 0.425507\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [50560/60000 (84%)] Loss: 0.349205\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [44800/60000 (75%)] Loss: 0.571483\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)] Loss: 0.452711\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [45440/60000 (76%)] Loss: 0.239797\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)] Loss: 0.431604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [45440/60000 (76%)] Loss: 0.246499\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51840/60000 (86%)] Loss: 0.150641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [46080/60000 (77%)] Loss: 0.376954\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [52480/60000 (87%)] Loss: 0.305140\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [46720/60000 (78%)] Loss: 0.464050\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [53120/60000 (88%)] Loss: 0.655883\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [47360/60000 (79%)] Loss: 0.470215\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [53760/60000 (90%)] Loss: 0.349496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [48000/60000 (80%)] Loss: 0.282651\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [54400/60000 (91%)] Loss: 0.269703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [48640/60000 (81%)] Loss: 0.429368\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [55040/60000 (92%)] Loss: 0.278394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [49280/60000 (82%)] Loss: 0.221565\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [51840/60000 (86%)] Loss: 0.149139\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [46080/60000 (77%)] Loss: 0.374376\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [52480/60000 (87%)] Loss: 0.319986\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [46720/60000 (78%)] Loss: 0.481953\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [53120/60000 (88%)] Loss: 0.656865\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [47360/60000 (79%)] Loss: 0.472687\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [53760/60000 (90%)] Loss: 0.336648\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [48000/60000 (80%)] Loss: 0.282187\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [54400/60000 (91%)] Loss: 0.278081\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [48640/60000 (81%)] Loss: 0.426036\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [55040/60000 (92%)] Loss: 0.290665\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [49280/60000 (82%)] Loss: 0.241616\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [55680/60000 (93%)] Loss: 0.489817\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [49920/60000 (83%)] Loss: 0.519187\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [55680/60000 (93%)] Loss: 0.490098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [49920/60000 (83%)] Loss: 0.513324\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [56320/60000 (94%)] Loss: 0.282643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [50560/60000 (84%)] Loss: 0.332847\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [56960/60000 (95%)] Loss: 0.396513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.431604\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)] Loss: 0.264887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51840/60000 (86%)] Loss: 0.150641\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [58240/60000 (97%)] Loss: 0.339846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [52480/60000 (87%)] Loss: 0.305140\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [58880/60000 (98%)] Loss: 0.343000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [53120/60000 (88%)] Loss: 0.655883\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [59520/60000 (99%)] Loss: 0.509793\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [53760/60000 (90%)] Loss: 0.349496\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [56320/60000 (94%)] Loss: 0.275530\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [50560/60000 (84%)] Loss: 0.349205\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [56960/60000 (95%)] Loss: 0.384676\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.452711\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)] Loss: 0.262810\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [51840/60000 (86%)] Loss: 0.149139\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [58240/60000 (97%)] Loss: 0.335770\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [52480/60000 (87%)] Loss: 0.319986\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [58880/60000 (98%)] Loss: 0.330926\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [53120/60000 (88%)] Loss: 0.656865\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [59520/60000 (99%)] Loss: 0.529286\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [53760/60000 (90%)] Loss: 0.336648\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mINFO:__main__:Epoch: 2#011Test set: Average loss: 0.1267, Accuracy: 96.08%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [54400/60000 (91%)] Loss: 0.278081\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [55040/60000 (92%)] Loss: 0.290665\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 2#011Test set: Average loss: 0.1267, Accuracy: 96.09%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [54400/60000 (91%)] Loss: 0.269703\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [55040/60000 (92%)] Loss: 0.278394\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [640/60000 (1%)] Loss: 0.208702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [55680/60000 (93%)] Loss: 0.490098\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [1280/60000 (2%)] Loss: 0.323959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [56320/60000 (94%)] Loss: 0.282643\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [1920/60000 (3%)] Loss: 0.382809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [56960/60000 (95%)] Loss: 0.396513\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [2560/60000 (4%)] Loss: 0.262482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)] Loss: 0.264887\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [3200/60000 (5%)] Loss: 0.472320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [58240/60000 (97%)] Loss: 0.339846\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [3840/60000 (6%)] Loss: 0.316821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [58880/60000 (98%)] Loss: 0.343000\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [640/60000 (1%)] Loss: 0.179968\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [55680/60000 (93%)] Loss: 0.489817\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [1280/60000 (2%)] Loss: 0.307398\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [56320/60000 (94%)] Loss: 0.275530\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [1920/60000 (3%)] Loss: 0.354809\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [56960/60000 (95%)] Loss: 0.384676\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [2560/60000 (4%)] Loss: 0.263056\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [57600/60000 (96%)] Loss: 0.262810\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [3200/60000 (5%)] Loss: 0.490489\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [58240/60000 (97%)] Loss: 0.335770\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [3840/60000 (6%)] Loss: 0.316862\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [58880/60000 (98%)] Loss: 0.330926\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [4480/60000 (7%)] Loss: 0.432533\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [59520/60000 (99%)] Loss: 0.529286\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [4480/60000 (7%)] Loss: 0.434913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [59520/60000 (99%)] Loss: 0.509793\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [5120/60000 (9%)] Loss: 0.292060\u001b[0m\n",
      "\u001b[34mEpoch: 2#011Test set: Average loss: 0.1267, Accuracy: 96.09%\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [5760/60000 (10%)] Loss: 0.550409\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)] Loss: 0.410954\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 2 with accuracy 0.961\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [7040/60000 (12%)] Loss: 0.274244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [640/60000 (1%)] Loss: 0.208702\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [7680/60000 (13%)] Loss: 0.231766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1280/60000 (2%)] Loss: 0.323959\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [8320/60000 (14%)] Loss: 0.201929\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1920/60000 (3%)] Loss: 0.382809\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [5120/60000 (9%)] Loss: 0.288692\u001b[0m\n",
      "\u001b[35mEpoch: 2#011Test set: Average loss: 0.1267, Accuracy: 96.08%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [5760/60000 (10%)] Loss: 0.549983\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)] Loss: 0.403961\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 2 with accuracy 0.961\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [7040/60000 (12%)] Loss: 0.263112\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [640/60000 (1%)] Loss: 0.179968\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [7680/60000 (13%)] Loss: 0.228011\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [1280/60000 (2%)] Loss: 0.307398\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [8320/60000 (14%)] Loss: 0.208265\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [1920/60000 (3%)] Loss: 0.354809\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [8960/60000 (15%)] Loss: 0.180770\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [2560/60000 (4%)] Loss: 0.263056\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [8960/60000 (15%)] Loss: 0.191797\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [2560/60000 (4%)] Loss: 0.262482\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [9600/60000 (16%)] Loss: 0.392532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [3200/60000 (5%)] Loss: 0.472320\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [10240/60000 (17%)] Loss: 0.378692\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [3840/60000 (6%)] Loss: 0.316821\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [10880/60000 (18%)] Loss: 0.360802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [4480/60000 (7%)] Loss: 0.434913\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [11520/60000 (19%)] Loss: 0.256236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [5120/60000 (9%)] Loss: 0.292060\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12160/60000 (20%)] Loss: 0.371803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [5760/60000 (10%)] Loss: 0.550409\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)] Loss: 0.333903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)] Loss: 0.410954\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [13440/60000 (22%)] Loss: 0.341113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [7040/60000 (12%)] Loss: 0.274244\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [9600/60000 (16%)] Loss: 0.388107\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [3200/60000 (5%)] Loss: 0.490489\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [10240/60000 (17%)] Loss: 0.353251\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [3840/60000 (6%)] Loss: 0.316862\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [10880/60000 (18%)] Loss: 0.367991\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [4480/60000 (7%)] Loss: 0.432533\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [11520/60000 (19%)] Loss: 0.248203\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [5120/60000 (9%)] Loss: 0.288692\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [12160/60000 (20%)] Loss: 0.372963\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [5760/60000 (10%)] Loss: 0.549983\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)] Loss: 0.310674\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [6400/60000 (11%)] Loss: 0.403961\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [13440/60000 (22%)] Loss: 0.352651\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [7040/60000 (12%)] Loss: 0.263112\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [14080/60000 (23%)] Loss: 0.429629\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [7680/60000 (13%)] Loss: 0.228011\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [14080/60000 (23%)] Loss: 0.402991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [7680/60000 (13%)] Loss: 0.231766\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [14720/60000 (25%)] Loss: 0.142205\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [8320/60000 (14%)] Loss: 0.201929\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [15360/60000 (26%)] Loss: 0.157339\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [8960/60000 (15%)] Loss: 0.191797\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [16000/60000 (27%)] Loss: 0.284686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [9600/60000 (16%)] Loss: 0.392532\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [16640/60000 (28%)] Loss: 0.375190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [10240/60000 (17%)] Loss: 0.378692\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [17280/60000 (29%)] Loss: 0.160984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [10880/60000 (18%)] Loss: 0.360802\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [17920/60000 (30%)] Loss: 0.559584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [11520/60000 (19%)] Loss: 0.256236\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [14720/60000 (25%)] Loss: 0.148430\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [8320/60000 (14%)] Loss: 0.208265\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [15360/60000 (26%)] Loss: 0.154382\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [8960/60000 (15%)] Loss: 0.180770\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [16000/60000 (27%)] Loss: 0.285141\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [9600/60000 (16%)] Loss: 0.388107\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [16640/60000 (28%)] Loss: 0.375261\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [10240/60000 (17%)] Loss: 0.353251\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [17280/60000 (29%)] Loss: 0.156860\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [10880/60000 (18%)] Loss: 0.367991\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [17920/60000 (30%)] Loss: 0.545730\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [11520/60000 (19%)] Loss: 0.248203\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [18560/60000 (31%)] Loss: 0.297922\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12160/60000 (20%)] Loss: 0.372963\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [18560/60000 (31%)] Loss: 0.280126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12160/60000 (20%)] Loss: 0.371803\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)] Loss: 0.335983\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.333903\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19840/60000 (33%)] Loss: 0.377512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [13440/60000 (22%)] Loss: 0.341113\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [20480/60000 (34%)] Loss: 0.326733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [14080/60000 (23%)] Loss: 0.402991\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [21120/60000 (35%)] Loss: 0.262017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [14720/60000 (25%)] Loss: 0.142205\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [21760/60000 (36%)] Loss: 0.416920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [15360/60000 (26%)] Loss: 0.157339\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [22400/60000 (37%)] Loss: 0.495709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [16000/60000 (27%)] Loss: 0.284686\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)] Loss: 0.352183\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.310674\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [19840/60000 (33%)] Loss: 0.358580\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [13440/60000 (22%)] Loss: 0.352651\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [20480/60000 (34%)] Loss: 0.340399\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [14080/60000 (23%)] Loss: 0.429629\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [21120/60000 (35%)] Loss: 0.255632\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [14720/60000 (25%)] Loss: 0.148430\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [21760/60000 (36%)] Loss: 0.417509\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [15360/60000 (26%)] Loss: 0.154382\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [22400/60000 (37%)] Loss: 0.514460\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [16000/60000 (27%)] Loss: 0.285141\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [23040/60000 (38%)] Loss: 0.208745\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [16640/60000 (28%)] Loss: 0.375261\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [23040/60000 (38%)] Loss: 0.218625\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [16640/60000 (28%)] Loss: 0.375190\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [23680/60000 (39%)] Loss: 0.367069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [17280/60000 (29%)] Loss: 0.160984\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [24320/60000 (41%)] Loss: 0.325877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [17920/60000 (30%)] Loss: 0.559584\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [24960/60000 (42%)] Loss: 0.230315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [18560/60000 (31%)] Loss: 0.280126\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)] Loss: 0.202064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)] Loss: 0.335983\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [26240/60000 (44%)] Loss: 0.216958\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19840/60000 (33%)] Loss: 0.377512\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [26880/60000 (45%)] Loss: 0.224548\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [20480/60000 (34%)] Loss: 0.326733\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [23680/60000 (39%)] Loss: 0.352753\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [17280/60000 (29%)] Loss: 0.156860\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [24320/60000 (41%)] Loss: 0.291162\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [17920/60000 (30%)] Loss: 0.545730\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [24960/60000 (42%)] Loss: 0.235535\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [18560/60000 (31%)] Loss: 0.297922\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)] Loss: 0.204079\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19200/60000 (32%)] Loss: 0.352183\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [26240/60000 (44%)] Loss: 0.239750\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19840/60000 (33%)] Loss: 0.358580\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [26880/60000 (45%)] Loss: 0.216020\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [20480/60000 (34%)] Loss: 0.340399\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [27520/60000 (46%)] Loss: 0.358486\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [21120/60000 (35%)] Loss: 0.255632\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [27520/60000 (46%)] Loss: 0.372134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [21120/60000 (35%)] Loss: 0.262017\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [28160/60000 (47%)] Loss: 0.275559\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [21760/60000 (36%)] Loss: 0.416920\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [28800/60000 (48%)] Loss: 0.365268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [22400/60000 (37%)] Loss: 0.495709\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [29440/60000 (49%)] Loss: 0.280926\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [23040/60000 (38%)] Loss: 0.218625\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [30080/60000 (50%)] Loss: 0.472887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [23680/60000 (39%)] Loss: 0.367069\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [30720/60000 (51%)] Loss: 0.444078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [24320/60000 (41%)] Loss: 0.325877\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [31360/60000 (52%)] Loss: 0.404059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [24960/60000 (42%)] Loss: 0.230315\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [28160/60000 (47%)] Loss: 0.257305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [21760/60000 (36%)] Loss: 0.417509\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [28800/60000 (48%)] Loss: 0.366591\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [22400/60000 (37%)] Loss: 0.514460\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [29440/60000 (49%)] Loss: 0.280475\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [23040/60000 (38%)] Loss: 0.208745\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [30080/60000 (50%)] Loss: 0.483861\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [23680/60000 (39%)] Loss: 0.352753\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [30720/60000 (51%)] Loss: 0.443990\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [24320/60000 (41%)] Loss: 0.291162\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [31360/60000 (52%)] Loss: 0.394677\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [24960/60000 (42%)] Loss: 0.235535\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)] Loss: 0.219806\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.204079\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)] Loss: 0.222041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.202064\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32640/60000 (54%)] Loss: 0.239068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [26240/60000 (44%)] Loss: 0.216958\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [33280/60000 (55%)] Loss: 0.254853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [26880/60000 (45%)] Loss: 0.224548\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [33920/60000 (57%)] Loss: 0.191580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [27520/60000 (46%)] Loss: 0.372134\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [34560/60000 (58%)] Loss: 0.258593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [28160/60000 (47%)] Loss: 0.275559\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [35200/60000 (59%)] Loss: 0.265544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [28800/60000 (48%)] Loss: 0.365268\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [35840/60000 (60%)] Loss: 0.212762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [29440/60000 (49%)] Loss: 0.280926\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [32640/60000 (54%)] Loss: 0.260247\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [26240/60000 (44%)] Loss: 0.239750\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [33280/60000 (55%)] Loss: 0.243952\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [26880/60000 (45%)] Loss: 0.216020\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [33920/60000 (57%)] Loss: 0.184786\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [27520/60000 (46%)] Loss: 0.358486\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [34560/60000 (58%)] Loss: 0.249117\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [28160/60000 (47%)] Loss: 0.257305\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [35200/60000 (59%)] Loss: 0.270487\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [28800/60000 (48%)] Loss: 0.366591\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [35840/60000 (60%)] Loss: 0.236937\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [29440/60000 (49%)] Loss: 0.280475\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [36480/60000 (61%)] Loss: 0.262188\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [30080/60000 (50%)] Loss: 0.483861\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [36480/60000 (61%)] Loss: 0.250644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [30080/60000 (50%)] Loss: 0.472887\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [37120/60000 (62%)] Loss: 0.184729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [30720/60000 (51%)] Loss: 0.444078\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [37760/60000 (63%)] Loss: 0.255477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [31360/60000 (52%)] Loss: 0.404059\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)] Loss: 0.335785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)] Loss: 0.222041\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [39040/60000 (65%)] Loss: 0.179559\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32640/60000 (54%)] Loss: 0.239068\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [39680/60000 (66%)] Loss: 0.507476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [33280/60000 (55%)] Loss: 0.254853\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [40320/60000 (67%)] Loss: 0.275055\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [33920/60000 (57%)] Loss: 0.191580\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [37120/60000 (62%)] Loss: 0.189558\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [30720/60000 (51%)] Loss: 0.443990\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [37760/60000 (63%)] Loss: 0.254372\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [31360/60000 (52%)] Loss: 0.394677\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)] Loss: 0.313678\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [32000/60000 (53%)] Loss: 0.219806\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [39040/60000 (65%)] Loss: 0.170806\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [32640/60000 (54%)] Loss: 0.260247\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [39680/60000 (66%)] Loss: 0.513559\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [33280/60000 (55%)] Loss: 0.243952\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [40320/60000 (67%)] Loss: 0.293071\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [33920/60000 (57%)] Loss: 0.184786\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [40960/60000 (68%)] Loss: 0.304313\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [34560/60000 (58%)] Loss: 0.249117\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [41600/60000 (69%)] Loss: 0.337172\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [35200/60000 (59%)] Loss: 0.270487\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [40960/60000 (68%)] Loss: 0.320125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [34560/60000 (58%)] Loss: 0.258593\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [41600/60000 (69%)] Loss: 0.311032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [35200/60000 (59%)] Loss: 0.265544\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [42240/60000 (70%)] Loss: 0.209098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [35840/60000 (60%)] Loss: 0.212762\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [42880/60000 (71%)] Loss: 0.166681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [36480/60000 (61%)] Loss: 0.250644\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [43520/60000 (72%)] Loss: 0.451167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [37120/60000 (62%)] Loss: 0.184729\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44160/60000 (74%)] Loss: 0.519410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [37760/60000 (63%)] Loss: 0.255477\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)] Loss: 0.432712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.335785\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [45440/60000 (76%)] Loss: 0.344811\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [39040/60000 (65%)] Loss: 0.179559\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mINFO:__main__:Train Epoch: 3 [42240/60000 (70%)] Loss: 0.203819\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [35840/60000 (60%)] Loss: 0.236937\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [42880/60000 (71%)] Loss: 0.170989\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [36480/60000 (61%)] Loss: 0.262188\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [43520/60000 (72%)] Loss: 0.434066\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [37120/60000 (62%)] Loss: 0.189558\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [44160/60000 (74%)] Loss: 0.555165\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [37760/60000 (63%)] Loss: 0.254372\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)] Loss: 0.440353\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.313678\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [45440/60000 (76%)] Loss: 0.332281\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [39040/60000 (65%)] Loss: 0.170806\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [46080/60000 (77%)] Loss: 0.278145\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [39680/60000 (66%)] Loss: 0.513559\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [46080/60000 (77%)] Loss: 0.274961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [39680/60000 (66%)] Loss: 0.507476\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [46720/60000 (78%)] Loss: 0.316760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [40320/60000 (67%)] Loss: 0.275055\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [47360/60000 (79%)] Loss: 0.382955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [40960/60000 (68%)] Loss: 0.320125\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [48000/60000 (80%)] Loss: 0.288068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [41600/60000 (69%)] Loss: 0.311032\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [48640/60000 (81%)] Loss: 0.313125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [42240/60000 (70%)] Loss: 0.209098\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [49280/60000 (82%)] Loss: 0.164863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [42880/60000 (71%)] Loss: 0.166681\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [49920/60000 (83%)] Loss: 0.358666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [43520/60000 (72%)] Loss: 0.451167\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [46720/60000 (78%)] Loss: 0.319383\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [40320/60000 (67%)] Loss: 0.293071\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [47360/60000 (79%)] Loss: 0.384719\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [40960/60000 (68%)] Loss: 0.304313\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [48000/60000 (80%)] Loss: 0.279770\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [41600/60000 (69%)] Loss: 0.337172\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [48640/60000 (81%)] Loss: 0.328014\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [42240/60000 (70%)] Loss: 0.203819\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [49280/60000 (82%)] Loss: 0.170352\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [42880/60000 (71%)] Loss: 0.170989\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [49920/60000 (83%)] Loss: 0.373042\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [43520/60000 (72%)] Loss: 0.434066\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [50560/60000 (84%)] Loss: 0.288856\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [44160/60000 (74%)] Loss: 0.555165\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [50560/60000 (84%)] Loss: 0.290098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44160/60000 (74%)] Loss: 0.519410\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)] Loss: 0.423214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)] Loss: 0.432712\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51840/60000 (86%)] Loss: 0.250644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [45440/60000 (76%)] Loss: 0.344811\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [52480/60000 (87%)] Loss: 0.145036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [46080/60000 (77%)] Loss: 0.274961\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [53120/60000 (88%)] Loss: 0.480489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [46720/60000 (78%)] Loss: 0.316760\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [53760/60000 (90%)] Loss: 0.296086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [47360/60000 (79%)] Loss: 0.382955\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [54400/60000 (91%)] Loss: 0.139670\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [48000/60000 (80%)] Loss: 0.288068\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)] Loss: 0.458883\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [44800/60000 (75%)] Loss: 0.440353\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [51840/60000 (86%)] Loss: 0.262034\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [45440/60000 (76%)] Loss: 0.332281\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [52480/60000 (87%)] Loss: 0.162226\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [46080/60000 (77%)] Loss: 0.278145\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [53120/60000 (88%)] Loss: 0.476952\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [46720/60000 (78%)] Loss: 0.319383\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [53760/60000 (90%)] Loss: 0.329313\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [47360/60000 (79%)] Loss: 0.384719\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [54400/60000 (91%)] Loss: 0.145614\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [48000/60000 (80%)] Loss: 0.279770\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [55040/60000 (92%)] Loss: 0.425022\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [48640/60000 (81%)] Loss: 0.328014\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [55040/60000 (92%)] Loss: 0.432884\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [48640/60000 (81%)] Loss: 0.313125\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [55680/60000 (93%)] Loss: 0.439272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [49280/60000 (82%)] Loss: 0.164863\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [56320/60000 (94%)] Loss: 0.118011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [49920/60000 (83%)] Loss: 0.358666\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [56960/60000 (95%)] Loss: 0.270847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [50560/60000 (84%)] Loss: 0.290098\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)] Loss: 0.200261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.423214\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [58240/60000 (97%)] Loss: 0.116854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51840/60000 (86%)] Loss: 0.250644\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [58880/60000 (98%)] Loss: 0.282917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [52480/60000 (87%)] Loss: 0.145036\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [55680/60000 (93%)] Loss: 0.411323\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [49280/60000 (82%)] Loss: 0.170352\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [56320/60000 (94%)] Loss: 0.116743\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [49920/60000 (83%)] Loss: 0.373042\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [56960/60000 (95%)] Loss: 0.264090\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [50560/60000 (84%)] Loss: 0.288856\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)] Loss: 0.194616\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.458883\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [58240/60000 (97%)] Loss: 0.110658\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [51840/60000 (86%)] Loss: 0.262034\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [58880/60000 (98%)] Loss: 0.296979\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [52480/60000 (87%)] Loss: 0.162226\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [59520/60000 (99%)] Loss: 0.375185\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [53120/60000 (88%)] Loss: 0.476952\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [59520/60000 (99%)] Loss: 0.368713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [53120/60000 (88%)] Loss: 0.480489\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 3#011Test set: Average loss: 0.0991, Accuracy: 96.84%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [53760/60000 (90%)] Loss: 0.329313\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [54400/60000 (91%)] Loss: 0.145614\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [55040/60000 (92%)] Loss: 0.425022\n",
      "  File \"mnist_ckpoint.py\", line 194, in <module>\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [55680/60000 (93%)] Loss: 0.411323\n",
      "    train(parser.parse_args())\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [56320/60000 (94%)] Loss: 0.116743\n",
      "  File \"mnist_ckpoint.py\", line 104, in train\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [56960/60000 (95%)] Loss: 0.264090\n",
      "    assert (args.test_checkpoint != epoch), \"Interrupting the training for checkpoint testing\"\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [57600/60000 (96%)] Loss: 0.194616\u001b[0m\n",
      "\u001b[35mAssertionError: Interrupting the training for checkpoint testing\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [58240/60000 (97%)] Loss: 0.110658\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 3#011Test set: Average loss: 0.0988, Accuracy: 96.86%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [53760/60000 (90%)] Loss: 0.296086\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [54400/60000 (91%)] Loss: 0.139670\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [55040/60000 (92%)] Loss: 0.432884\n",
      "  File \"mnist_ckpoint.py\", line 194, in <module>\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [55680/60000 (93%)] Loss: 0.439272\n",
      "    train(parser.parse_args())\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [56320/60000 (94%)] Loss: 0.118011\n",
      "  File \"mnist_ckpoint.py\", line 104, in train\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [56960/60000 (95%)] Loss: 0.270847\n",
      "    assert (args.test_checkpoint != epoch), \"Interrupting the training for checkpoint testing\"\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)] Loss: 0.200261\u001b[0m\n",
      "\u001b[34mAssertionError: Interrupting the training for checkpoint testing\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [58240/60000 (97%)] Loss: 0.116854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [58880/60000 (98%)] Loss: 0.282917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [59520/60000 (99%)] Loss: 0.368713\u001b[0m\n",
      "\u001b[34mEpoch: 3#011Test set: Average loss: 0.0988, Accuracy: 96.86%\n",
      "\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 3 with accuracy 0.969\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:19:11.903 algo-1:46 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:19:12,274 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python mnist_ckpoint.py --backend nccl --epochs 5 --test-checkpoint 4\"\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [640/60000 (1%)] Loss: 2.304615\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.312131\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1920/60000 (3%)] Loss: 2.291755\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.294058\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/60000 (5%)] Loss: 2.291983\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 2.259225\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [4480/60000 (7%)] Loss: 2.270091\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 2.273199\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5760/60000 (10%)] Loss: 2.240260\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 2.219095\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7040/60000 (12%)] Loss: 2.242404\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 2.145658\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8320/60000 (14%)] Loss: 2.101541\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 2.152929\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [9600/60000 (16%)] Loss: 2.026896\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10240/60000 (17%)] Loss: 1.830962\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10880/60000 (18%)] Loss: 1.715244\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [11520/60000 (19%)] Loss: 1.782036\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12160/60000 (20%)] Loss: 1.597787\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)] Loss: 1.538395\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [13440/60000 (22%)] Loss: 1.445780\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14080/60000 (23%)] Loss: 1.310982\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14720/60000 (25%)] Loss: 0.964348\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [15360/60000 (26%)] Loss: 1.097690\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16000/60000 (27%)] Loss: 1.141092\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16640/60000 (28%)] Loss: 1.193740\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17280/60000 (29%)] Loss: 0.852487\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17920/60000 (30%)] Loss: 1.241178\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [18560/60000 (31%)] Loss: 1.007236\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)] Loss: 0.827731\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19840/60000 (33%)] Loss: 1.074296\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [20480/60000 (34%)] Loss: 0.738656\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [21120/60000 (35%)] Loss: 0.595940\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [21760/60000 (36%)] Loss: 0.913359\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [22400/60000 (37%)] Loss: 0.932164\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [23040/60000 (38%)] Loss: 0.958393\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [23680/60000 (39%)] Loss: 0.889941\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [24320/60000 (41%)] Loss: 0.931165\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [24960/60000 (42%)] Loss: 0.691616\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)] Loss: 0.781822\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [26240/60000 (44%)] Loss: 0.632963\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [26880/60000 (45%)] Loss: 0.524712\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [27520/60000 (46%)] Loss: 0.707324\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [28160/60000 (47%)] Loss: 0.641527\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [28800/60000 (48%)] Loss: 0.604308\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [29440/60000 (49%)] Loss: 0.602750\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [30080/60000 (50%)] Loss: 0.771171\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [30720/60000 (51%)] Loss: 0.965248\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [31360/60000 (52%)] Loss: 0.612834\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)] Loss: 0.602342\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32640/60000 (54%)] Loss: 0.617510\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [33280/60000 (55%)] Loss: 0.439259\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [33920/60000 (57%)] Loss: 0.545377\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [34560/60000 (58%)] Loss: 0.601812\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [35200/60000 (59%)] Loss: 0.790327\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [35840/60000 (60%)] Loss: 0.681572\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [36480/60000 (61%)] Loss: 0.702572\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [37120/60000 (62%)] Loss: 0.947972\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [37760/60000 (63%)] Loss: 0.586345\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)] Loss: 0.636209\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [39040/60000 (65%)] Loss: 0.670400\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [39680/60000 (66%)] Loss: 0.831303\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [40320/60000 (67%)] Loss: 0.694420\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [40960/60000 (68%)] Loss: 0.693431\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [41600/60000 (69%)] Loss: 0.583254\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [42240/60000 (70%)] Loss: 0.578561\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [42880/60000 (71%)] Loss: 0.652027\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [43520/60000 (72%)] Loss: 0.670639\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44160/60000 (74%)] Loss: 0.734596\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)] Loss: 0.658686\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [45440/60000 (76%)] Loss: 0.435589\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [46080/60000 (77%)] Loss: 0.610422\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [46720/60000 (78%)] Loss: 0.688221\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [47360/60000 (79%)] Loss: 0.595887\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [48000/60000 (80%)] Loss: 0.438291\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [48640/60000 (81%)] Loss: 0.615952\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [49280/60000 (82%)] Loss: 0.493627\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [49920/60000 (83%)] Loss: 0.588574\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [50560/60000 (84%)] Loss: 0.560617\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)] Loss: 0.697095\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51840/60000 (86%)] Loss: 0.365182\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [52480/60000 (87%)] Loss: 0.357024\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [53120/60000 (88%)] Loss: 0.571931\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [53760/60000 (90%)] Loss: 0.583758\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [54400/60000 (91%)] Loss: 0.401018\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [55040/60000 (92%)] Loss: 0.678986\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [55680/60000 (93%)] Loss: 0.464060\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [56320/60000 (94%)] Loss: 0.317471\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [56960/60000 (95%)] Loss: 0.566832\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)] Loss: 0.348050\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [58240/60000 (97%)] Loss: 0.307573\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [58880/60000 (98%)] Loss: 0.425404\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [59520/60000 (99%)] Loss: 0.469284\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 1#011Test set: Average loss: 0.2013, Accuracy: 93.88%\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [640/60000 (1%)] Loss: 0.728956\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [1280/60000 (2%)] Loss: 0.496761\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [1920/60000 (3%)] Loss: 0.444945\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [2560/60000 (4%)] Loss: 0.306844\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3200/60000 (5%)] Loss: 0.649742\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3840/60000 (6%)] Loss: 0.435952\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [4480/60000 (7%)] Loss: 0.355907\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [5120/60000 (9%)] Loss: 0.534058\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [5760/60000 (10%)] Loss: 0.593422\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)] Loss: 0.412394\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [7040/60000 (12%)] Loss: 0.440413\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [7680/60000 (13%)] Loss: 0.464011\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [8320/60000 (14%)] Loss: 0.263103\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [8960/60000 (15%)] Loss: 0.353541\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [9600/60000 (16%)] Loss: 0.488806\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [10240/60000 (17%)] Loss: 0.572477\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [10880/60000 (18%)] Loss: 0.464004\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [11520/60000 (19%)] Loss: 0.464065\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12160/60000 (20%)] Loss: 0.401854\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)] Loss: 0.514975\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [13440/60000 (22%)] Loss: 0.494417\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [14080/60000 (23%)] Loss: 0.446381\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [14720/60000 (25%)] Loss: 0.173765\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [15360/60000 (26%)] Loss: 0.329959\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [16000/60000 (27%)] Loss: 0.550346\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [16640/60000 (28%)] Loss: 0.481258\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [17280/60000 (29%)] Loss: 0.294725\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [17920/60000 (30%)] Loss: 0.908997\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [18560/60000 (31%)] Loss: 0.609053\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)] Loss: 0.365597\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19840/60000 (33%)] Loss: 0.436345\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [20480/60000 (34%)] Loss: 0.180099\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [21120/60000 (35%)] Loss: 0.326352\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [21760/60000 (36%)] Loss: 0.598429\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [22400/60000 (37%)] Loss: 0.408572\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [23040/60000 (38%)] Loss: 0.445631\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [23680/60000 (39%)] Loss: 0.409249\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [24320/60000 (41%)] Loss: 0.575094\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [24960/60000 (42%)] Loss: 0.283301\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)] Loss: 0.307152\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [26240/60000 (44%)] Loss: 0.267358\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [26880/60000 (45%)] Loss: 0.279142\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [27520/60000 (46%)] Loss: 0.423186\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [28160/60000 (47%)] Loss: 0.416947\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [28800/60000 (48%)] Loss: 0.334616\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [29440/60000 (49%)] Loss: 0.316835\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [30080/60000 (50%)] Loss: 0.291248\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [30720/60000 (51%)] Loss: 0.572555\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [31360/60000 (52%)] Loss: 0.538087\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)] Loss: 0.575392\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32640/60000 (54%)] Loss: 0.429783\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [33280/60000 (55%)] Loss: 0.382640\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [33920/60000 (57%)] Loss: 0.340792\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [34560/60000 (58%)] Loss: 0.355870\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [35200/60000 (59%)] Loss: 0.365755\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [35840/60000 (60%)] Loss: 0.435311\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [36480/60000 (61%)] Loss: 0.414630\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [37120/60000 (62%)] Loss: 0.390312\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [37760/60000 (63%)] Loss: 0.300364\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)] Loss: 0.429095\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [39040/60000 (65%)] Loss: 0.310158\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [39680/60000 (66%)] Loss: 0.376576\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [40320/60000 (67%)] Loss: 0.542084\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [40960/60000 (68%)] Loss: 0.455414\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [41600/60000 (69%)] Loss: 0.309381\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [42240/60000 (70%)] Loss: 0.404683\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [42880/60000 (71%)] Loss: 0.370423\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [43520/60000 (72%)] Loss: 0.441798\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44160/60000 (74%)] Loss: 0.413338\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)] Loss: 0.588516\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [45440/60000 (76%)] Loss: 0.246499\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [46080/60000 (77%)] Loss: 0.376954\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [46720/60000 (78%)] Loss: 0.464050\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [47360/60000 (79%)] Loss: 0.470215\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [48000/60000 (80%)] Loss: 0.282651\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [48640/60000 (81%)] Loss: 0.429368\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [49280/60000 (82%)] Loss: 0.221565\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [49920/60000 (83%)] Loss: 0.513324\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [50560/60000 (84%)] Loss: 0.332847\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)] Loss: 0.431604\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51840/60000 (86%)] Loss: 0.150641\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [52480/60000 (87%)] Loss: 0.305140\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [53120/60000 (88%)] Loss: 0.655883\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [53760/60000 (90%)] Loss: 0.349496\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [54400/60000 (91%)] Loss: 0.269703\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [55040/60000 (92%)] Loss: 0.278394\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [55680/60000 (93%)] Loss: 0.490098\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [56320/60000 (94%)] Loss: 0.282643\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [56960/60000 (95%)] Loss: 0.396513\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)] Loss: 0.264887\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [58240/60000 (97%)] Loss: 0.339846\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [58880/60000 (98%)] Loss: 0.343000\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [59520/60000 (99%)] Loss: 0.509793\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 2#011Test set: Average loss: 0.1267, Accuracy: 96.09%\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [640/60000 (1%)] Loss: 0.208702\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [1280/60000 (2%)] Loss: 0.323959\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [1920/60000 (3%)] Loss: 0.382809\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [2560/60000 (4%)] Loss: 0.262482\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [3200/60000 (5%)] Loss: 0.472320\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [3840/60000 (6%)] Loss: 0.316821\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [4480/60000 (7%)] Loss: 0.434913\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [5120/60000 (9%)] Loss: 0.292060\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [5760/60000 (10%)] Loss: 0.550409\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)] Loss: 0.410954\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [7040/60000 (12%)] Loss: 0.274244\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [7680/60000 (13%)] Loss: 0.231766\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [8320/60000 (14%)] Loss: 0.201929\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [8960/60000 (15%)] Loss: 0.191797\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [9600/60000 (16%)] Loss: 0.392532\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [10240/60000 (17%)] Loss: 0.378692\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [10880/60000 (18%)] Loss: 0.360802\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [11520/60000 (19%)] Loss: 0.256236\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12160/60000 (20%)] Loss: 0.371803\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)] Loss: 0.333903\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [13440/60000 (22%)] Loss: 0.341113\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [14080/60000 (23%)] Loss: 0.402991\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [14720/60000 (25%)] Loss: 0.142205\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [15360/60000 (26%)] Loss: 0.157339\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [16000/60000 (27%)] Loss: 0.284686\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [16640/60000 (28%)] Loss: 0.375190\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [17280/60000 (29%)] Loss: 0.160984\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [17920/60000 (30%)] Loss: 0.559584\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [18560/60000 (31%)] Loss: 0.280126\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)] Loss: 0.335983\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19840/60000 (33%)] Loss: 0.377512\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [20480/60000 (34%)] Loss: 0.326733\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [21120/60000 (35%)] Loss: 0.262017\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [21760/60000 (36%)] Loss: 0.416920\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [22400/60000 (37%)] Loss: 0.495709\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [23040/60000 (38%)] Loss: 0.218625\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [23680/60000 (39%)] Loss: 0.367069\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [24320/60000 (41%)] Loss: 0.325877\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [24960/60000 (42%)] Loss: 0.230315\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)] Loss: 0.202064\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [26240/60000 (44%)] Loss: 0.216958\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [26880/60000 (45%)] Loss: 0.224548\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [27520/60000 (46%)] Loss: 0.372134\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [28160/60000 (47%)] Loss: 0.275559\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [28800/60000 (48%)] Loss: 0.365268\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [29440/60000 (49%)] Loss: 0.280926\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [30080/60000 (50%)] Loss: 0.472887\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [30720/60000 (51%)] Loss: 0.444078\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [31360/60000 (52%)] Loss: 0.404059\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)] Loss: 0.222041\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32640/60000 (54%)] Loss: 0.239068\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [33280/60000 (55%)] Loss: 0.254853\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [33920/60000 (57%)] Loss: 0.191580\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [34560/60000 (58%)] Loss: 0.258593\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [35200/60000 (59%)] Loss: 0.265544\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [35840/60000 (60%)] Loss: 0.212762\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [36480/60000 (61%)] Loss: 0.250644\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [37120/60000 (62%)] Loss: 0.184729\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [37760/60000 (63%)] Loss: 0.255477\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)] Loss: 0.335785\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [39040/60000 (65%)] Loss: 0.179559\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [39680/60000 (66%)] Loss: 0.507476\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [40320/60000 (67%)] Loss: 0.275055\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [40960/60000 (68%)] Loss: 0.320125\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [41600/60000 (69%)] Loss: 0.311032\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [42240/60000 (70%)] Loss: 0.209098\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [42880/60000 (71%)] Loss: 0.166681\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [43520/60000 (72%)] Loss: 0.451167\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44160/60000 (74%)] Loss: 0.519410\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)] Loss: 0.432712\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [45440/60000 (76%)] Loss: 0.344811\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [46080/60000 (77%)] Loss: 0.274961\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [46720/60000 (78%)] Loss: 0.316760\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [47360/60000 (79%)] Loss: 0.382955\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [48000/60000 (80%)] Loss: 0.288068\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [48640/60000 (81%)] Loss: 0.313125\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [49280/60000 (82%)] Loss: 0.164863\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [49920/60000 (83%)] Loss: 0.358666\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [50560/60000 (84%)] Loss: 0.290098\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)] Loss: 0.423214\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51840/60000 (86%)] Loss: 0.250644\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [52480/60000 (87%)] Loss: 0.145036\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [53120/60000 (88%)] Loss: 0.480489\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [53760/60000 (90%)] Loss: 0.296086\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [54400/60000 (91%)] Loss: 0.139670\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [55040/60000 (92%)] Loss: 0.432884\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [55680/60000 (93%)] Loss: 0.439272\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [56320/60000 (94%)] Loss: 0.118011\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [56960/60000 (95%)] Loss: 0.270847\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)] Loss: 0.200261\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [58240/60000 (97%)] Loss: 0.116854\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [58880/60000 (98%)] Loss: 0.282917\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [59520/60000 (99%)] Loss: 0.368713\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 3#011Test set: Average loss: 0.0988, Accuracy: 96.86%\n",
      "\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"mnist_ckpoint.py\", line 194, in <module>\n",
      "    train(parser.parse_args())\n",
      "  File \"mnist_ckpoint.py\", line 104, in train\n",
      "    assert (args.test_checkpoint != epoch), \"Interrupting the training for checkpoint testing\"\u001b[0m\n",
      "\u001b[34mAssertionError: Interrupting the training for checkpoint testing\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [58880/60000 (98%)] Loss: 0.296979\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [59520/60000 (99%)] Loss: 0.375185\u001b[0m\n",
      "\u001b[35mEpoch: 3#011Test set: Average loss: 0.0991, Accuracy: 96.84%\n",
      "\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 3 with accuracy 0.968\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:19:11.806 algo-2:46 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[35m2020-02-17 11:19:12,170 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[35mCommand \"/opt/conda/bin/python mnist_ckpoint.py --backend nccl --epochs 5 --test-checkpoint 4\"\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [640/60000 (1%)] Loss: 2.304614\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.312132\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [1920/60000 (3%)] Loss: 2.291734\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.293818\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [3200/60000 (5%)] Loss: 2.291899\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 2.259277\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [4480/60000 (7%)] Loss: 2.269309\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 2.272827\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [5760/60000 (10%)] Loss: 2.240711\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 2.218936\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [7040/60000 (12%)] Loss: 2.241756\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 2.143724\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [8320/60000 (14%)] Loss: 2.101925\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 2.151304\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [9600/60000 (16%)] Loss: 2.023522\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [10240/60000 (17%)] Loss: 1.827971\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [10880/60000 (18%)] Loss: 1.706553\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [11520/60000 (19%)] Loss: 1.780761\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12160/60000 (20%)] Loss: 1.596794\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)] Loss: 1.530881\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [13440/60000 (22%)] Loss: 1.444683\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [14080/60000 (23%)] Loss: 1.316386\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [14720/60000 (25%)] Loss: 0.963505\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [15360/60000 (26%)] Loss: 1.094985\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [16000/60000 (27%)] Loss: 1.142972\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [16640/60000 (28%)] Loss: 1.189747\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [17280/60000 (29%)] Loss: 0.858050\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [17920/60000 (30%)] Loss: 1.255457\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [18560/60000 (31%)] Loss: 1.009188\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)] Loss: 0.837179\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19840/60000 (33%)] Loss: 1.074096\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [20480/60000 (34%)] Loss: 0.720260\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [21120/60000 (35%)] Loss: 0.594824\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [21760/60000 (36%)] Loss: 0.920185\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [22400/60000 (37%)] Loss: 0.945955\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [23040/60000 (38%)] Loss: 0.959031\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [23680/60000 (39%)] Loss: 0.872368\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [24320/60000 (41%)] Loss: 0.917179\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [24960/60000 (42%)] Loss: 0.691484\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)] Loss: 0.780470\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [26240/60000 (44%)] Loss: 0.626546\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [26880/60000 (45%)] Loss: 0.522827\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [27520/60000 (46%)] Loss: 0.702136\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [28160/60000 (47%)] Loss: 0.639676\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [28800/60000 (48%)] Loss: 0.618777\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [29440/60000 (49%)] Loss: 0.600116\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [30080/60000 (50%)] Loss: 0.755639\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [30720/60000 (51%)] Loss: 0.974858\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [31360/60000 (52%)] Loss: 0.594564\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)] Loss: 0.585964\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [32640/60000 (54%)] Loss: 0.585063\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [33280/60000 (55%)] Loss: 0.461115\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [33920/60000 (57%)] Loss: 0.537313\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [34560/60000 (58%)] Loss: 0.612457\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [35200/60000 (59%)] Loss: 0.775702\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [35840/60000 (60%)] Loss: 0.677434\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [36480/60000 (61%)] Loss: 0.719881\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [37120/60000 (62%)] Loss: 0.947347\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [37760/60000 (63%)] Loss: 0.582867\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)] Loss: 0.633877\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [39040/60000 (65%)] Loss: 0.655600\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [39680/60000 (66%)] Loss: 0.837099\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [40320/60000 (67%)] Loss: 0.690240\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [40960/60000 (68%)] Loss: 0.701980\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [41600/60000 (69%)] Loss: 0.572289\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [42240/60000 (70%)] Loss: 0.577458\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [42880/60000 (71%)] Loss: 0.647905\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [43520/60000 (72%)] Loss: 0.695250\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [44160/60000 (74%)] Loss: 0.730602\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)] Loss: 0.689333\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [45440/60000 (76%)] Loss: 0.435044\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [46080/60000 (77%)] Loss: 0.614693\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [46720/60000 (78%)] Loss: 0.675602\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [47360/60000 (79%)] Loss: 0.599071\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [48000/60000 (80%)] Loss: 0.447848\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [48640/60000 (81%)] Loss: 0.596610\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [49280/60000 (82%)] Loss: 0.490495\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [49920/60000 (83%)] Loss: 0.593058\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [50560/60000 (84%)] Loss: 0.578594\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)] Loss: 0.664744\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [51840/60000 (86%)] Loss: 0.365830\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [52480/60000 (87%)] Loss: 0.352741\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [53120/60000 (88%)] Loss: 0.587418\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [53760/60000 (90%)] Loss: 0.590395\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [54400/60000 (91%)] Loss: 0.399100\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [55040/60000 (92%)] Loss: 0.680151\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [55680/60000 (93%)] Loss: 0.474605\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [56320/60000 (94%)] Loss: 0.323984\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [56960/60000 (95%)] Loss: 0.555805\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)] Loss: 0.357811\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [58240/60000 (97%)] Loss: 0.317995\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [58880/60000 (98%)] Loss: 0.413098\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [59520/60000 (99%)] Loss: 0.464581\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 1#011Test set: Average loss: 0.2009, Accuracy: 94.05%\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [640/60000 (1%)] Loss: 0.700058\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [1280/60000 (2%)] Loss: 0.487832\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [1920/60000 (3%)] Loss: 0.434912\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [2560/60000 (4%)] Loss: 0.331555\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [3200/60000 (5%)] Loss: 0.647907\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [3840/60000 (6%)] Loss: 0.430586\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [4480/60000 (7%)] Loss: 0.364085\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [5120/60000 (9%)] Loss: 0.518247\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [5760/60000 (10%)] Loss: 0.601655\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)] Loss: 0.394229\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [7040/60000 (12%)] Loss: 0.461580\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [7680/60000 (13%)] Loss: 0.461925\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [8320/60000 (14%)] Loss: 0.255405\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [8960/60000 (15%)] Loss: 0.340774\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [9600/60000 (16%)] Loss: 0.482525\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [10240/60000 (17%)] Loss: 0.584850\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [10880/60000 (18%)] Loss: 0.505457\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [11520/60000 (19%)] Loss: 0.472357\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [12160/60000 (20%)] Loss: 0.407176\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)] Loss: 0.492951\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [13440/60000 (22%)] Loss: 0.494078\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [14080/60000 (23%)] Loss: 0.448517\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [14720/60000 (25%)] Loss: 0.186437\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [15360/60000 (26%)] Loss: 0.337660\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [16000/60000 (27%)] Loss: 0.539467\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [16640/60000 (28%)] Loss: 0.500526\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [17280/60000 (29%)] Loss: 0.285901\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [17920/60000 (30%)] Loss: 0.897939\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [18560/60000 (31%)] Loss: 0.618473\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)] Loss: 0.381157\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [19840/60000 (33%)] Loss: 0.432843\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [20480/60000 (34%)] Loss: 0.167429\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [21120/60000 (35%)] Loss: 0.304395\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [21760/60000 (36%)] Loss: 0.586674\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [22400/60000 (37%)] Loss: 0.424896\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [23040/60000 (38%)] Loss: 0.443679\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [23680/60000 (39%)] Loss: 0.420080\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [24320/60000 (41%)] Loss: 0.576859\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [24960/60000 (42%)] Loss: 0.302404\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)] Loss: 0.307918\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [26240/60000 (44%)] Loss: 0.278859\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [26880/60000 (45%)] Loss: 0.293179\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [27520/60000 (46%)] Loss: 0.424827\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [28160/60000 (47%)] Loss: 0.428971\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [28800/60000 (48%)] Loss: 0.337768\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [29440/60000 (49%)] Loss: 0.319107\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [30080/60000 (50%)] Loss: 0.300779\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [30720/60000 (51%)] Loss: 0.548153\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [31360/60000 (52%)] Loss: 0.550801\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)] Loss: 0.571701\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [32640/60000 (54%)] Loss: 0.451862\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [33280/60000 (55%)] Loss: 0.386338\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [33920/60000 (57%)] Loss: 0.361100\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [34560/60000 (58%)] Loss: 0.355616\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [35200/60000 (59%)] Loss: 0.367827\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [35840/60000 (60%)] Loss: 0.437870\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [36480/60000 (61%)] Loss: 0.392356\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [37120/60000 (62%)] Loss: 0.400960\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [37760/60000 (63%)] Loss: 0.313635\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)] Loss: 0.420374\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [39040/60000 (65%)] Loss: 0.316503\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [39680/60000 (66%)] Loss: 0.368304\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [40320/60000 (67%)] Loss: 0.546762\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [40960/60000 (68%)] Loss: 0.457406\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [41600/60000 (69%)] Loss: 0.316164\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [42240/60000 (70%)] Loss: 0.393249\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [42880/60000 (71%)] Loss: 0.350286\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [43520/60000 (72%)] Loss: 0.428007\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [44160/60000 (74%)] Loss: 0.425507\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)] Loss: 0.571483\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [45440/60000 (76%)] Loss: 0.239797\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [46080/60000 (77%)] Loss: 0.374376\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [46720/60000 (78%)] Loss: 0.481953\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [47360/60000 (79%)] Loss: 0.472687\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [48000/60000 (80%)] Loss: 0.282187\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [48640/60000 (81%)] Loss: 0.426036\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [49280/60000 (82%)] Loss: 0.241616\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [49920/60000 (83%)] Loss: 0.519187\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [50560/60000 (84%)] Loss: 0.349205\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)] Loss: 0.452711\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [51840/60000 (86%)] Loss: 0.149139\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [52480/60000 (87%)] Loss: 0.319986\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [53120/60000 (88%)] Loss: 0.656865\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [53760/60000 (90%)] Loss: 0.336648\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [54400/60000 (91%)] Loss: 0.278081\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [55040/60000 (92%)] Loss: 0.290665\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [55680/60000 (93%)] Loss: 0.489817\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [56320/60000 (94%)] Loss: 0.275530\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [56960/60000 (95%)] Loss: 0.384676\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)] Loss: 0.262810\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [58240/60000 (97%)] Loss: 0.335770\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [58880/60000 (98%)] Loss: 0.330926\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [59520/60000 (99%)] Loss: 0.529286\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 2#011Test set: Average loss: 0.1267, Accuracy: 96.08%\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [640/60000 (1%)] Loss: 0.179968\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [1280/60000 (2%)] Loss: 0.307398\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [1920/60000 (3%)] Loss: 0.354809\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [2560/60000 (4%)] Loss: 0.263056\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [3200/60000 (5%)] Loss: 0.490489\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [3840/60000 (6%)] Loss: 0.316862\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [4480/60000 (7%)] Loss: 0.432533\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [5120/60000 (9%)] Loss: 0.288692\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [5760/60000 (10%)] Loss: 0.549983\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)] Loss: 0.403961\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [7040/60000 (12%)] Loss: 0.263112\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [7680/60000 (13%)] Loss: 0.228011\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [8320/60000 (14%)] Loss: 0.208265\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [8960/60000 (15%)] Loss: 0.180770\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [9600/60000 (16%)] Loss: 0.388107\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [10240/60000 (17%)] Loss: 0.353251\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [10880/60000 (18%)] Loss: 0.367991\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [11520/60000 (19%)] Loss: 0.248203\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [12160/60000 (20%)] Loss: 0.372963\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)] Loss: 0.310674\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [13440/60000 (22%)] Loss: 0.352651\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [14080/60000 (23%)] Loss: 0.429629\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [14720/60000 (25%)] Loss: 0.148430\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [15360/60000 (26%)] Loss: 0.154382\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [16000/60000 (27%)] Loss: 0.285141\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [16640/60000 (28%)] Loss: 0.375261\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [17280/60000 (29%)] Loss: 0.156860\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [17920/60000 (30%)] Loss: 0.545730\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [18560/60000 (31%)] Loss: 0.297922\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)] Loss: 0.352183\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [19840/60000 (33%)] Loss: 0.358580\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [20480/60000 (34%)] Loss: 0.340399\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [21120/60000 (35%)] Loss: 0.255632\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [21760/60000 (36%)] Loss: 0.417509\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [22400/60000 (37%)] Loss: 0.514460\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [23040/60000 (38%)] Loss: 0.208745\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [23680/60000 (39%)] Loss: 0.352753\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [24320/60000 (41%)] Loss: 0.291162\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [24960/60000 (42%)] Loss: 0.235535\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)] Loss: 0.204079\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [26240/60000 (44%)] Loss: 0.239750\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [26880/60000 (45%)] Loss: 0.216020\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [27520/60000 (46%)] Loss: 0.358486\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [28160/60000 (47%)] Loss: 0.257305\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [28800/60000 (48%)] Loss: 0.366591\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [29440/60000 (49%)] Loss: 0.280475\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [30080/60000 (50%)] Loss: 0.483861\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [30720/60000 (51%)] Loss: 0.443990\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [31360/60000 (52%)] Loss: 0.394677\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)] Loss: 0.219806\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [32640/60000 (54%)] Loss: 0.260247\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [33280/60000 (55%)] Loss: 0.243952\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [33920/60000 (57%)] Loss: 0.184786\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [34560/60000 (58%)] Loss: 0.249117\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [35200/60000 (59%)] Loss: 0.270487\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [35840/60000 (60%)] Loss: 0.236937\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [36480/60000 (61%)] Loss: 0.262188\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [37120/60000 (62%)] Loss: 0.189558\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [37760/60000 (63%)] Loss: 0.254372\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)] Loss: 0.313678\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [39040/60000 (65%)] Loss: 0.170806\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [39680/60000 (66%)] Loss: 0.513559\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [40320/60000 (67%)] Loss: 0.293071\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [40960/60000 (68%)] Loss: 0.304313\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [41600/60000 (69%)] Loss: 0.337172\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [42240/60000 (70%)] Loss: 0.203819\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [42880/60000 (71%)] Loss: 0.170989\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [43520/60000 (72%)] Loss: 0.434066\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [44160/60000 (74%)] Loss: 0.555165\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)] Loss: 0.440353\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [45440/60000 (76%)] Loss: 0.332281\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [46080/60000 (77%)] Loss: 0.278145\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [46720/60000 (78%)] Loss: 0.319383\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [47360/60000 (79%)] Loss: 0.384719\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [48000/60000 (80%)] Loss: 0.279770\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [48640/60000 (81%)] Loss: 0.328014\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [49280/60000 (82%)] Loss: 0.170352\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [49920/60000 (83%)] Loss: 0.373042\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [50560/60000 (84%)] Loss: 0.288856\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)] Loss: 0.458883\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [51840/60000 (86%)] Loss: 0.262034\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [52480/60000 (87%)] Loss: 0.162226\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [53120/60000 (88%)] Loss: 0.476952\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [53760/60000 (90%)] Loss: 0.329313\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [54400/60000 (91%)] Loss: 0.145614\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [55040/60000 (92%)] Loss: 0.425022\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [55680/60000 (93%)] Loss: 0.411323\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [56320/60000 (94%)] Loss: 0.116743\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [56960/60000 (95%)] Loss: 0.264090\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)] Loss: 0.194616\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [58240/60000 (97%)] Loss: 0.110658\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [58880/60000 (98%)] Loss: 0.296979\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [59520/60000 (99%)] Loss: 0.375185\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 3#011Test set: Average loss: 0.0991, Accuracy: 96.84%\n",
      "\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"mnist_ckpoint.py\", line 194, in <module>\n",
      "    train(parser.parse_args())\n",
      "  File \"mnist_ckpoint.py\", line 104, in train\n",
      "    assert (args.test_checkpoint != epoch), \"Interrupting the training for checkpoint testing\"\u001b[0m\n",
      "\u001b[35mAssertionError: Interrupting the training for checkpoint testing\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-02-17 11:19:14 Uploading - Uploading generated training model\n",
      "2020-02-17 11:19:44 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2020-02-17-11-14-06-031: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python mnist_ckpoint.py --backend nccl --epochs 5 --test-checkpoint 4\"\nINFO:__main__:Train Epoch: 1 [640/60000 (1%)] Loss: 2.304615\nINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.312131\nINFO:__main__:Train Epoch: 1 [1920/60000 (3%)] Loss: 2.291755\nINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.294058\nINFO:__main__:Train Epoch: 1 [3200/60000 (5%)] Loss: 2.291983\nINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 2.259225\nINFO:__main__:Train Epoch: 1 [4480/60000 (7%)] Loss: 2.270091\nINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 2.273199\nINFO:__main__:Train Epoch: 1 [5760/60000 (10%)] Loss: 2.240260\nINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 2.219095\nINFO:__main__:Train Epoch: 1 [7040/60000 (12%)] Loss: 2.242404\nINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 2.145658\nINFO:__main__:Train Epoch: 1 [8320/60000 (14%)] Loss: 2.101541\nINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 2.152929\nINFO:__main__:Tr",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c4d9fe753c65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2613\u001b[0m                 ),\n\u001b[1;32m   2614\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2615\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2616\u001b[0m             )\n\u001b[1;32m   2617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2020-02-17-11-14-06-031: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python mnist_ckpoint.py --backend nccl --epochs 5 --test-checkpoint 4\"\nINFO:__main__:Train Epoch: 1 [640/60000 (1%)] Loss: 2.304615\nINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.312131\nINFO:__main__:Train Epoch: 1 [1920/60000 (3%)] Loss: 2.291755\nINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.294058\nINFO:__main__:Train Epoch: 1 [3200/60000 (5%)] Loss: 2.291983\nINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 2.259225\nINFO:__main__:Train Epoch: 1 [4480/60000 (7%)] Loss: 2.270091\nINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 2.273199\nINFO:__main__:Train Epoch: 1 [5760/60000 (10%)] Loss: 2.240260\nINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 2.219095\nINFO:__main__:Train Epoch: 1 [7040/60000 (12%)] Loss: 2.242404\nINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 2.145658\nINFO:__main__:Train Epoch: 1 [8320/60000 (14%)] Loss: 2.101541\nINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 2.152929\nINFO:__main__:Tr"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original training run failed, as requested. But we have the checkpoints stored in our bucket, under the `/checkpoints` directory. If you check the `mnist_ckpoint.py` script, you'll see that it has two functions that respectively save and restore the checkpoints:\n",
    "```python\n",
    "def save_checkpoint(state, filename='/opt/ml/checkpoints/checkpoint.pth.tar'):\n",
    "    print(f\"Saving checkpoint for epoch {state['epoch']} with accuracy {state['best_accuracy']:.3f}\")\n",
    "    torch.save(state, filename)  # save checkpoint\n",
    "\n",
    "    \n",
    "def load_checkpoint(filename='/opt/ml/checkpoints/checkpoint.pth.tar'):\n",
    "    return (torch.load(filename) if os.path.exists(filename) else None)\n",
    "```\n",
    "\n",
    "`save_checkpoint` is called inside the training loop, with the following code:\n",
    "```python\n",
    "        if (best_accuracy < accuracy) and (hvd.rank() == 0):\n",
    "            best_accuracy = accuracy\n",
    "            is_best = True\n",
    "            save_checkpoint(\n",
    "                {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_accuracy': best_accuracy\n",
    "                })\n",
    "```\n",
    "The `hvd.rank() == 0` is, according to [horovod's repo](https://github.com/horovod/horovod#usage) (see point 6), the right way to save checkpoints leveraging the framework (the example is for Tensorflow, we adapted it for PyTorch).\n",
    "\n",
    "`load_checkpoint` is called at the beginning of the `train` function, and if it finds a saved checkpoint training will resume from there:\n",
    "```python\n",
    "    start_epoch = 1\n",
    "    best_accuracy = 0\n",
    "    checkpoint = load_checkpoint()\n",
    "    if checkpoint:\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_accuracy = checkpoint['best_accuracy']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "```\n",
    "\n",
    "The next cell creates another PyTorch Estimator, without the request to interrupt training. From the logs, it can be seen that it started from the previous checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-17 11:21:11 Starting - Starting the training job...\n",
      "2020-02-17 11:21:13 Starting - Launching requested ML instances......\n",
      "2020-02-17 11:22:17 Starting - Preparing the instances for training......\n",
      "2020-02-17 11:23:37 Downloading - Downloading input data...\n",
      "2020-02-17 11:24:07 Training - Downloading the training image......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-02-17 11:24:58,460 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-02-17 11:24:58,485 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-02-17 11:25:14,877 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-02-17 11:25:14,902 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2020-02-17 11:25:17,917 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-02-17 11:25:18,173 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-02-17 11:25:18,173 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-02-17 11:25:18,173 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-02-17 11:25:18,173 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmpylqg_u1n/module_dir\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[35m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22887 sha256=6da95c595634c539c158099b905e72a16e4951536a611b4dcc92de5c653f3f6d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rnx724wl/wheels/9c/13/7a/a2bcc0f09e0b7accc907f705ee1ead017993cf0c0d5973ce2e\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[35mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[35mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2020-02-17 11:25:20,365 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"nccl\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2020-02-17-11-21-11-299\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-21-11-299/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_ckpoint\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_ckpoint.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"nccl\",\"epochs\":5}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist_ckpoint.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist_ckpoint\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-21-11-299/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"nccl\",\"epochs\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2020-02-17-11-21-11-299\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-21-11-299/source/sourcedir.tar.gz\",\"module_name\":\"mnist_ckpoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_ckpoint.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"nccl\",\"--epochs\",\"5\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python mnist_ckpoint.py --backend nccl --epochs 5\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34m2020-02-17 11:25:23,909 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:25:24,201 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-02-17 11:25:24,201 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-02-17 11:25:24,202 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-02-17 11:25:24,202 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpwfksiln6/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22889 sha256=6fb34113863fc110d482f683d3454db1e1bfc2d50c2f0805245758fe6f07cbd1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-q8xbakct/wheels/0a/42/db/51a2667899d1d8bd4c1d776d2fcc615d4239b53f4a35019cff\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:25:26,324 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"nccl\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-02-17-11-21-11-299\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-21-11-299/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_ckpoint\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_ckpoint.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"nccl\",\"epochs\":5}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist_ckpoint.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist_ckpoint\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-21-11-299/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"nccl\",\"epochs\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-02-17-11-21-11-299\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-21-11-299/source/sourcedir.tar.gz\",\"module_name\":\"mnist_ckpoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_ckpoint.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"nccl\",\"--epochs\",\"5\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python mnist_ckpoint.py --backend nccl --epochs 5\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-02-17 11:25:13 Training - Training image download completed. Training in progress.\u001b[35mINFO:__main__:Train Epoch: 1 [640/60000 (1%)] Loss: 2.304603\u001b[0m\n",
      "\u001b[35mStarting training from the beginning\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.312048\u001b[0m\n",
      "\u001b[35mTest checkpoint is 0\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [1920/60000 (3%)] Loss: 2.291684\u001b[0m\n",
      "\u001b[35mGet train data sampler and data loader\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.293913\u001b[0m\n",
      "\u001b[35mGet test data sampler and data loader\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [3200/60000 (5%)] Loss: 2.291727\u001b[0m\n",
      "\u001b[35mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 2.258277\u001b[0m\n",
      "\u001b[35mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [4480/60000 (7%)] Loss: 2.268969\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:25:25.250 algo-2:46 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 2.272753\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:25:25.251 algo-2:46 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [5760/60000 (10%)] Loss: 2.239941\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:25:25.251 algo-2:46 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 2.217517\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:25:25.253 algo-2:46 INFO hook.py:216] Initialized the hook with the last saved state: last_saved_step=5500 init_step = 5685, step = 5685 mode_steps = {<ModeKeys.GLOBAL: 4>: 5685}\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [7040/60000 (12%)] Loss: 2.242978\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:25:25.253 algo-2:46 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 2.142903\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [640/60000 (1%)] Loss: 2.304603\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [8320/60000 (14%)] Loss: 2.100338\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [1280/60000 (2%)] Loss: 2.312048\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 2.150455\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [1920/60000 (3%)] Loss: 2.291684\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [9600/60000 (16%)] Loss: 2.024704\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [2560/60000 (4%)] Loss: 2.293913\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [10240/60000 (17%)] Loss: 1.826627\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [3200/60000 (5%)] Loss: 2.291727\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [10880/60000 (18%)] Loss: 1.708502\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [3840/60000 (6%)] Loss: 2.258277\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [11520/60000 (19%)] Loss: 1.773089\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [4480/60000 (7%)] Loss: 2.268969\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12160/60000 (20%)] Loss: 1.590821\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [5120/60000 (9%)] Loss: 2.272753\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)] Loss: 1.524004\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [5760/60000 (10%)] Loss: 2.239941\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [13440/60000 (22%)] Loss: 1.443086\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [6400/60000 (11%)] Loss: 2.217517\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [14080/60000 (23%)] Loss: 1.311421\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [7040/60000 (12%)] Loss: 2.242978\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [14720/60000 (25%)] Loss: 0.964555\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [7680/60000 (13%)] Loss: 2.142903\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [15360/60000 (26%)] Loss: 1.096750\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [8320/60000 (14%)] Loss: 2.100338\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [16000/60000 (27%)] Loss: 1.156238\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [8960/60000 (15%)] Loss: 2.150455\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [16640/60000 (28%)] Loss: 1.184494\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [9600/60000 (16%)] Loss: 2.024704\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [17280/60000 (29%)] Loss: 0.855119\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [10240/60000 (17%)] Loss: 1.826627\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [17920/60000 (30%)] Loss: 1.245169\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [10880/60000 (18%)] Loss: 1.708502\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [18560/60000 (31%)] Loss: 1.008575\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [11520/60000 (19%)] Loss: 1.773089\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)] Loss: 0.847061\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12160/60000 (20%)] Loss: 1.590821\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19840/60000 (33%)] Loss: 1.065810\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/60000 (21%)] Loss: 1.524004\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [20480/60000 (34%)] Loss: 0.727140\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [13440/60000 (22%)] Loss: 1.443086\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [640/60000 (1%)] Loss: 2.304610\u001b[0m\n",
      "\u001b[34mStarting training from the beginning\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [21120/60000 (35%)] Loss: 0.597387\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [14080/60000 (23%)] Loss: 1.311421\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [21760/60000 (36%)] Loss: 0.914474\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [14720/60000 (25%)] Loss: 0.964555\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [22400/60000 (37%)] Loss: 0.951056\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [15360/60000 (26%)] Loss: 1.096750\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [23040/60000 (38%)] Loss: 0.954785\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [16000/60000 (27%)] Loss: 1.156238\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [23680/60000 (39%)] Loss: 0.879154\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [16640/60000 (28%)] Loss: 1.184494\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [24320/60000 (41%)] Loss: 0.927413\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [17280/60000 (29%)] Loss: 0.855119\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [24960/60000 (42%)] Loss: 0.685904\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [17920/60000 (30%)] Loss: 1.245169\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.312041\u001b[0m\n",
      "\u001b[34mTest checkpoint is 0\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1920/60000 (3%)] Loss: 2.291805\u001b[0m\n",
      "\u001b[34mGet train data sampler and data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.294055\u001b[0m\n",
      "\u001b[34mGet test data sampler and data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/60000 (5%)] Loss: 2.291652\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 2.259515\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [4480/60000 (7%)] Loss: 2.268900\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:25:30.741 algo-1:46 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 2.272656\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:25:30.741 algo-1:46 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5760/60000 (10%)] Loss: 2.239258\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:25:30.742 algo-1:46 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)] Loss: 0.775834\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [18560/60000 (31%)] Loss: 1.008575\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [26240/60000 (44%)] Loss: 0.625897\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19200/60000 (32%)] Loss: 0.847061\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [26880/60000 (45%)] Loss: 0.533781\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19840/60000 (33%)] Loss: 1.065810\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [27520/60000 (46%)] Loss: 0.699869\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [20480/60000 (34%)] Loss: 0.727140\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [28160/60000 (47%)] Loss: 0.641019\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [21120/60000 (35%)] Loss: 0.597387\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [28800/60000 (48%)] Loss: 0.609706\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [21760/60000 (36%)] Loss: 0.914474\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [29440/60000 (49%)] Loss: 0.588114\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [22400/60000 (37%)] Loss: 0.951056\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 2.217926\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:25:30.743 algo-1:46 INFO hook.py:216] Initialized the hook with the last saved state: last_saved_step=5500 init_step = 5685, step = 5685 mode_steps = {<ModeKeys.GLOBAL: 4>: 5685}\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7040/60000 (12%)] Loss: 2.242130\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:25:30.744 algo-1:46 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 2.143054\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [640/60000 (1%)] Loss: 2.304610\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8320/60000 (14%)] Loss: 2.099188\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1280/60000 (2%)] Loss: 2.312041\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 2.148371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1920/60000 (3%)] Loss: 2.291805\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [9600/60000 (16%)] Loss: 2.023465\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2560/60000 (4%)] Loss: 2.294055\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [30080/60000 (50%)] Loss: 0.776426\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [23040/60000 (38%)] Loss: 0.954785\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [30720/60000 (51%)] Loss: 0.967916\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [23680/60000 (39%)] Loss: 0.879154\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [31360/60000 (52%)] Loss: 0.597704\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [24320/60000 (41%)] Loss: 0.927413\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)] Loss: 0.591015\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [24960/60000 (42%)] Loss: 0.685904\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [32640/60000 (54%)] Loss: 0.586873\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.775834\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [33280/60000 (55%)] Loss: 0.454291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [26240/60000 (44%)] Loss: 0.625897\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [33920/60000 (57%)] Loss: 0.551242\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [26880/60000 (45%)] Loss: 0.533781\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10240/60000 (17%)] Loss: 1.825982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/60000 (5%)] Loss: 2.291652\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10880/60000 (18%)] Loss: 1.709779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3840/60000 (6%)] Loss: 2.259515\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [11520/60000 (19%)] Loss: 1.772861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [4480/60000 (7%)] Loss: 2.268900\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12160/60000 (20%)] Loss: 1.590020\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5120/60000 (9%)] Loss: 2.272656\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)] Loss: 1.525059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5760/60000 (10%)] Loss: 2.239258\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [13440/60000 (22%)] Loss: 1.449087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)] Loss: 2.217926\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [34560/60000 (58%)] Loss: 0.605918\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [27520/60000 (46%)] Loss: 0.699869\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [35200/60000 (59%)] Loss: 0.773452\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [28160/60000 (47%)] Loss: 0.641019\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [35840/60000 (60%)] Loss: 0.678096\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [28800/60000 (48%)] Loss: 0.609706\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [36480/60000 (61%)] Loss: 0.693580\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [29440/60000 (49%)] Loss: 0.588114\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [37120/60000 (62%)] Loss: 0.937972\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [30080/60000 (50%)] Loss: 0.776426\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [37760/60000 (63%)] Loss: 0.585394\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [30720/60000 (51%)] Loss: 0.967916\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)] Loss: 0.628902\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [31360/60000 (52%)] Loss: 0.597704\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [39040/60000 (65%)] Loss: 0.657173\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [32000/60000 (53%)] Loss: 0.591015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14080/60000 (23%)] Loss: 1.307881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7040/60000 (12%)] Loss: 2.242130\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14720/60000 (25%)] Loss: 0.953352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7680/60000 (13%)] Loss: 2.143054\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [15360/60000 (26%)] Loss: 1.087243\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8320/60000 (14%)] Loss: 2.099188\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16000/60000 (27%)] Loss: 1.146010\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8960/60000 (15%)] Loss: 2.148371\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16640/60000 (28%)] Loss: 1.196020\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [9600/60000 (16%)] Loss: 2.023465\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17280/60000 (29%)] Loss: 0.857527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10240/60000 (17%)] Loss: 1.825982\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17920/60000 (30%)] Loss: 1.254826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10880/60000 (18%)] Loss: 1.709779\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [39680/60000 (66%)] Loss: 0.837893\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [32640/60000 (54%)] Loss: 0.586873\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [40320/60000 (67%)] Loss: 0.695209\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [33280/60000 (55%)] Loss: 0.454291\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [40960/60000 (68%)] Loss: 0.679713\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [33920/60000 (57%)] Loss: 0.551242\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [41600/60000 (69%)] Loss: 0.564797\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [34560/60000 (58%)] Loss: 0.605918\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [42240/60000 (70%)] Loss: 0.586834\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [35200/60000 (59%)] Loss: 0.773452\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [42880/60000 (71%)] Loss: 0.655498\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [35840/60000 (60%)] Loss: 0.678096\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [43520/60000 (72%)] Loss: 0.697791\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [36480/60000 (61%)] Loss: 0.693580\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Train Epoch: 1 [18560/60000 (31%)] Loss: 1.008385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [11520/60000 (19%)] Loss: 1.772861\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)] Loss: 0.814286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12160/60000 (20%)] Loss: 1.590020\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19840/60000 (33%)] Loss: 1.060687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)] Loss: 1.525059\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [20480/60000 (34%)] Loss: 0.714971\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [13440/60000 (22%)] Loss: 1.449087\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [21120/60000 (35%)] Loss: 0.588677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14080/60000 (23%)] Loss: 1.307881\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [21760/60000 (36%)] Loss: 0.904387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14720/60000 (25%)] Loss: 0.953352\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [22400/60000 (37%)] Loss: 0.930393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [15360/60000 (26%)] Loss: 1.087243\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [23040/60000 (38%)] Loss: 0.949185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16000/60000 (27%)] Loss: 1.146010\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [44160/60000 (74%)] Loss: 0.720959\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [37120/60000 (62%)] Loss: 0.937972\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)] Loss: 0.688124\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [37760/60000 (63%)] Loss: 0.585394\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [45440/60000 (76%)] Loss: 0.441936\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.628902\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [46080/60000 (77%)] Loss: 0.616563\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [39040/60000 (65%)] Loss: 0.657173\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [46720/60000 (78%)] Loss: 0.685495\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [39680/60000 (66%)] Loss: 0.837893\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [47360/60000 (79%)] Loss: 0.604365\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [40320/60000 (67%)] Loss: 0.695209\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [48000/60000 (80%)] Loss: 0.447064\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [40960/60000 (68%)] Loss: 0.679713\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [23680/60000 (39%)] Loss: 0.874019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16640/60000 (28%)] Loss: 1.196020\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [24320/60000 (41%)] Loss: 0.923699\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [17280/60000 (29%)] Loss: 0.857527\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [24960/60000 (42%)] Loss: 0.683865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [17920/60000 (30%)] Loss: 1.254826\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)] Loss: 0.774141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [18560/60000 (31%)] Loss: 1.008385\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [26240/60000 (44%)] Loss: 0.620621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)] Loss: 0.814286\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [26880/60000 (45%)] Loss: 0.527532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19840/60000 (33%)] Loss: 1.060687\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [27520/60000 (46%)] Loss: 0.717649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [20480/60000 (34%)] Loss: 0.714971\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [48640/60000 (81%)] Loss: 0.585426\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [41600/60000 (69%)] Loss: 0.564797\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [49280/60000 (82%)] Loss: 0.485148\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [42240/60000 (70%)] Loss: 0.586834\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [49920/60000 (83%)] Loss: 0.586103\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [42880/60000 (71%)] Loss: 0.655498\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [50560/60000 (84%)] Loss: 0.561907\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [43520/60000 (72%)] Loss: 0.697791\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)] Loss: 0.668834\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [44160/60000 (74%)] Loss: 0.720959\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [51840/60000 (86%)] Loss: 0.363585\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [44800/60000 (75%)] Loss: 0.688124\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [52480/60000 (87%)] Loss: 0.369236\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [45440/60000 (76%)] Loss: 0.441936\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [28160/60000 (47%)] Loss: 0.636877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [21120/60000 (35%)] Loss: 0.588677\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [28800/60000 (48%)] Loss: 0.596776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [21760/60000 (36%)] Loss: 0.904387\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [29440/60000 (49%)] Loss: 0.591662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [22400/60000 (37%)] Loss: 0.930393\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [30080/60000 (50%)] Loss: 0.754830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [23040/60000 (38%)] Loss: 0.949185\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [30720/60000 (51%)] Loss: 0.973500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [23680/60000 (39%)] Loss: 0.874019\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [31360/60000 (52%)] Loss: 0.587222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [24320/60000 (41%)] Loss: 0.923699\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)] Loss: 0.599548\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [24960/60000 (42%)] Loss: 0.683865\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32640/60000 (54%)] Loss: 0.602859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.774141\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [53120/60000 (88%)] Loss: 0.573505\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [46080/60000 (77%)] Loss: 0.616563\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [53760/60000 (90%)] Loss: 0.588075\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [46720/60000 (78%)] Loss: 0.685495\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [54400/60000 (91%)] Loss: 0.398631\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [47360/60000 (79%)] Loss: 0.604365\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [55040/60000 (92%)] Loss: 0.698388\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [48000/60000 (80%)] Loss: 0.447064\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [55680/60000 (93%)] Loss: 0.463765\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [48640/60000 (81%)] Loss: 0.585426\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [56320/60000 (94%)] Loss: 0.329087\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [49280/60000 (82%)] Loss: 0.485148\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [56960/60000 (95%)] Loss: 0.552424\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [49920/60000 (83%)] Loss: 0.586103\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [33280/60000 (55%)] Loss: 0.440616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [26240/60000 (44%)] Loss: 0.620621\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [33920/60000 (57%)] Loss: 0.521199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [26880/60000 (45%)] Loss: 0.527532\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [34560/60000 (58%)] Loss: 0.604348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [27520/60000 (46%)] Loss: 0.717649\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [35200/60000 (59%)] Loss: 0.771448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [28160/60000 (47%)] Loss: 0.636877\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [35840/60000 (60%)] Loss: 0.669767\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [28800/60000 (48%)] Loss: 0.596776\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [36480/60000 (61%)] Loss: 0.702634\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [29440/60000 (49%)] Loss: 0.591662\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [37120/60000 (62%)] Loss: 0.928318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [30080/60000 (50%)] Loss: 0.754830\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)] Loss: 0.337455\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [50560/60000 (84%)] Loss: 0.561907\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [58240/60000 (97%)] Loss: 0.314765\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.668834\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [58880/60000 (98%)] Loss: 0.424755\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [51840/60000 (86%)] Loss: 0.363585\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [59520/60000 (99%)] Loss: 0.462849\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [52480/60000 (87%)] Loss: 0.369236\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [53120/60000 (88%)] Loss: 0.573505\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [53760/60000 (90%)] Loss: 0.588075\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [37760/60000 (63%)] Loss: 0.582365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [30720/60000 (51%)] Loss: 0.973500\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)] Loss: 0.614130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [31360/60000 (52%)] Loss: 0.587222\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [39040/60000 (65%)] Loss: 0.677869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)] Loss: 0.599548\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [39680/60000 (66%)] Loss: 0.839576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32640/60000 (54%)] Loss: 0.602859\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [40320/60000 (67%)] Loss: 0.692011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [33280/60000 (55%)] Loss: 0.440616\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [40960/60000 (68%)] Loss: 0.676431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [33920/60000 (57%)] Loss: 0.521199\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [41600/60000 (69%)] Loss: 0.578787\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [34560/60000 (58%)] Loss: 0.604348\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [42240/60000 (70%)] Loss: 0.585320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [35200/60000 (59%)] Loss: 0.771448\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [42880/60000 (71%)] Loss: 0.663543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [35840/60000 (60%)] Loss: 0.669767\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [43520/60000 (72%)] Loss: 0.678644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [36480/60000 (61%)] Loss: 0.702634\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44160/60000 (74%)] Loss: 0.721903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [37120/60000 (62%)] Loss: 0.928318\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)] Loss: 0.689233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [37760/60000 (63%)] Loss: 0.582365\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [45440/60000 (76%)] Loss: 0.434660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.614130\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [46080/60000 (77%)] Loss: 0.595386\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [39040/60000 (65%)] Loss: 0.677869\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [46720/60000 (78%)] Loss: 0.672320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [39680/60000 (66%)] Loss: 0.839576\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 1#011Test set: Average loss: 0.2000, Accuracy: 93.99%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [54400/60000 (91%)] Loss: 0.398631\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [55040/60000 (92%)] Loss: 0.698388\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [640/60000 (1%)] Loss: 0.685729\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [55680/60000 (93%)] Loss: 0.463765\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [47360/60000 (79%)] Loss: 0.603049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [40320/60000 (67%)] Loss: 0.692011\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [48000/60000 (80%)] Loss: 0.439495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [40960/60000 (68%)] Loss: 0.676431\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [48640/60000 (81%)] Loss: 0.587446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [41600/60000 (69%)] Loss: 0.578787\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [49280/60000 (82%)] Loss: 0.475984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [42240/60000 (70%)] Loss: 0.585320\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [49920/60000 (83%)] Loss: 0.593183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [42880/60000 (71%)] Loss: 0.663543\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [50560/60000 (84%)] Loss: 0.548429\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [43520/60000 (72%)] Loss: 0.678644\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)] Loss: 0.672025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44160/60000 (74%)] Loss: 0.721903\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [1280/60000 (2%)] Loss: 0.511623\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [56320/60000 (94%)] Loss: 0.329087\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [1920/60000 (3%)] Loss: 0.446834\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [56960/60000 (95%)] Loss: 0.552424\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [2560/60000 (4%)] Loss: 0.297016\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [57600/60000 (96%)] Loss: 0.337455\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [3200/60000 (5%)] Loss: 0.654509\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [58240/60000 (97%)] Loss: 0.314765\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [3840/60000 (6%)] Loss: 0.411781\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [58880/60000 (98%)] Loss: 0.424755\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [4480/60000 (7%)] Loss: 0.355394\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [59520/60000 (99%)] Loss: 0.462849\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [5120/60000 (9%)] Loss: 0.514998\u001b[0m\n",
      "\u001b[35mEpoch: 1#011Test set: Average loss: 0.2000, Accuracy: 93.99%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [5760/60000 (10%)] Loss: 0.590997\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51840/60000 (86%)] Loss: 0.355403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)] Loss: 0.689233\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [52480/60000 (87%)] Loss: 0.355388\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [45440/60000 (76%)] Loss: 0.434660\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [53120/60000 (88%)] Loss: 0.593336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [46080/60000 (77%)] Loss: 0.595386\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [53760/60000 (90%)] Loss: 0.586190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [46720/60000 (78%)] Loss: 0.672320\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [54400/60000 (91%)] Loss: 0.391875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [47360/60000 (79%)] Loss: 0.603049\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [55040/60000 (92%)] Loss: 0.704062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [48000/60000 (80%)] Loss: 0.439495\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [55680/60000 (93%)] Loss: 0.466761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [48640/60000 (81%)] Loss: 0.587446\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [56320/60000 (94%)] Loss: 0.315531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [49280/60000 (82%)] Loss: 0.475984\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)] Loss: 0.408467\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 1 with accuracy 0.940\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [7040/60000 (12%)] Loss: 0.456512\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [640/60000 (1%)] Loss: 0.685729\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [7680/60000 (13%)] Loss: 0.458401\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [1280/60000 (2%)] Loss: 0.511623\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [8320/60000 (14%)] Loss: 0.250484\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [1920/60000 (3%)] Loss: 0.446834\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [8960/60000 (15%)] Loss: 0.340570\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [2560/60000 (4%)] Loss: 0.297016\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [9600/60000 (16%)] Loss: 0.489326\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [3200/60000 (5%)] Loss: 0.654509\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [10240/60000 (17%)] Loss: 0.585057\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [3840/60000 (6%)] Loss: 0.411781\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [56960/60000 (95%)] Loss: 0.562253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [49920/60000 (83%)] Loss: 0.593183\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)] Loss: 0.349102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [50560/60000 (84%)] Loss: 0.548429\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [58240/60000 (97%)] Loss: 0.310217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.672025\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [58880/60000 (98%)] Loss: 0.442504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51840/60000 (86%)] Loss: 0.355403\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [59520/60000 (99%)] Loss: 0.476230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [52480/60000 (87%)] Loss: 0.355388\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [53120/60000 (88%)] Loss: 0.593336\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [53760/60000 (90%)] Loss: 0.586190\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [10880/60000 (18%)] Loss: 0.486138\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [4480/60000 (7%)] Loss: 0.355394\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [11520/60000 (19%)] Loss: 0.470106\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [5120/60000 (9%)] Loss: 0.514998\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [12160/60000 (20%)] Loss: 0.412872\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [5760/60000 (10%)] Loss: 0.590997\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)] Loss: 0.490327\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [6400/60000 (11%)] Loss: 0.408467\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [13440/60000 (22%)] Loss: 0.503925\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [7040/60000 (12%)] Loss: 0.456512\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [14080/60000 (23%)] Loss: 0.431281\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [7680/60000 (13%)] Loss: 0.458401\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [14720/60000 (25%)] Loss: 0.174918\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [8320/60000 (14%)] Loss: 0.250484\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [15360/60000 (26%)] Loss: 0.334293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [8960/60000 (15%)] Loss: 0.340570\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [16000/60000 (27%)] Loss: 0.521367\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [9600/60000 (16%)] Loss: 0.489326\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [16640/60000 (28%)] Loss: 0.491021\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [10240/60000 (17%)] Loss: 0.585057\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [17280/60000 (29%)] Loss: 0.289758\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [10880/60000 (18%)] Loss: 0.486138\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [17920/60000 (30%)] Loss: 0.895181\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [11520/60000 (19%)] Loss: 0.470106\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [18560/60000 (31%)] Loss: 0.606898\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12160/60000 (20%)] Loss: 0.412872\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)] Loss: 0.374929\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.490327\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Epoch: 1#011Test set: Average loss: 0.2008, Accuracy: 94.02%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [54400/60000 (91%)] Loss: 0.391875\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [55040/60000 (92%)] Loss: 0.704062\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [640/60000 (1%)] Loss: 0.685819\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [55680/60000 (93%)] Loss: 0.466761\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [19840/60000 (33%)] Loss: 0.426466\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [13440/60000 (22%)] Loss: 0.503925\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [20480/60000 (34%)] Loss: 0.175942\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [14080/60000 (23%)] Loss: 0.431281\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [21120/60000 (35%)] Loss: 0.312501\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [14720/60000 (25%)] Loss: 0.174918\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [21760/60000 (36%)] Loss: 0.598874\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [15360/60000 (26%)] Loss: 0.334293\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [22400/60000 (37%)] Loss: 0.416368\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [16000/60000 (27%)] Loss: 0.521367\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [23040/60000 (38%)] Loss: 0.443982\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [16640/60000 (28%)] Loss: 0.491021\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [23680/60000 (39%)] Loss: 0.406396\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [17280/60000 (29%)] Loss: 0.289758\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [1280/60000 (2%)] Loss: 0.506606\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [56320/60000 (94%)] Loss: 0.315531\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [1920/60000 (3%)] Loss: 0.441287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [56960/60000 (95%)] Loss: 0.562253\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [2560/60000 (4%)] Loss: 0.304162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)] Loss: 0.349102\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3200/60000 (5%)] Loss: 0.635830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [58240/60000 (97%)] Loss: 0.310217\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3840/60000 (6%)] Loss: 0.426132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [58880/60000 (98%)] Loss: 0.442504\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [4480/60000 (7%)] Loss: 0.369644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [59520/60000 (99%)] Loss: 0.476230\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [5120/60000 (9%)] Loss: 0.510136\u001b[0m\n",
      "\u001b[34mEpoch: 1#011Test set: Average loss: 0.2008, Accuracy: 94.02%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [24320/60000 (41%)] Loss: 0.575505\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [17920/60000 (30%)] Loss: 0.895181\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [24960/60000 (42%)] Loss: 0.287866\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [18560/60000 (31%)] Loss: 0.606898\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)] Loss: 0.318580\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19200/60000 (32%)] Loss: 0.374929\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [26240/60000 (44%)] Loss: 0.273800\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19840/60000 (33%)] Loss: 0.426466\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [26880/60000 (45%)] Loss: 0.288933\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [20480/60000 (34%)] Loss: 0.175942\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [27520/60000 (46%)] Loss: 0.425012\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [21120/60000 (35%)] Loss: 0.312501\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [28160/60000 (47%)] Loss: 0.421317\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [21760/60000 (36%)] Loss: 0.598874\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [28800/60000 (48%)] Loss: 0.330514\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [22400/60000 (37%)] Loss: 0.416368\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [5760/60000 (10%)] Loss: 0.579577\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)] Loss: 0.395365\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 1 with accuracy 0.940\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [7040/60000 (12%)] Loss: 0.457036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [640/60000 (1%)] Loss: 0.685819\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [7680/60000 (13%)] Loss: 0.460619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1280/60000 (2%)] Loss: 0.506606\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [8320/60000 (14%)] Loss: 0.255924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1920/60000 (3%)] Loss: 0.441287\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [8960/60000 (15%)] Loss: 0.333168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [2560/60000 (4%)] Loss: 0.304162\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [9600/60000 (16%)] Loss: 0.483299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3200/60000 (5%)] Loss: 0.635830\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [29440/60000 (49%)] Loss: 0.317247\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [23040/60000 (38%)] Loss: 0.443982\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [30080/60000 (50%)] Loss: 0.293147\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [23680/60000 (39%)] Loss: 0.406396\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [30720/60000 (51%)] Loss: 0.537435\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [24320/60000 (41%)] Loss: 0.575505\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [31360/60000 (52%)] Loss: 0.527554\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [24960/60000 (42%)] Loss: 0.287866\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)] Loss: 0.585332\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.318580\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [32640/60000 (54%)] Loss: 0.444773\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [26240/60000 (44%)] Loss: 0.273800\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [33280/60000 (55%)] Loss: 0.392805\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [26880/60000 (45%)] Loss: 0.288933\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [10240/60000 (17%)] Loss: 0.581276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3840/60000 (6%)] Loss: 0.426132\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [10880/60000 (18%)] Loss: 0.466773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [4480/60000 (7%)] Loss: 0.369644\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [11520/60000 (19%)] Loss: 0.469236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [5120/60000 (9%)] Loss: 0.510136\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12160/60000 (20%)] Loss: 0.405032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [5760/60000 (10%)] Loss: 0.579577\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)] Loss: 0.480381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)] Loss: 0.395365\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [13440/60000 (22%)] Loss: 0.470270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [7040/60000 (12%)] Loss: 0.457036\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [14080/60000 (23%)] Loss: 0.450604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [7680/60000 (13%)] Loss: 0.460619\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [14720/60000 (25%)] Loss: 0.183430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [8320/60000 (14%)] Loss: 0.255924\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [33920/60000 (57%)] Loss: 0.337790\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [27520/60000 (46%)] Loss: 0.425012\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [34560/60000 (58%)] Loss: 0.365110\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [28160/60000 (47%)] Loss: 0.421317\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [35200/60000 (59%)] Loss: 0.373095\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [28800/60000 (48%)] Loss: 0.330514\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [35840/60000 (60%)] Loss: 0.433996\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [29440/60000 (49%)] Loss: 0.317247\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [36480/60000 (61%)] Loss: 0.393475\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [30080/60000 (50%)] Loss: 0.293147\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [37120/60000 (62%)] Loss: 0.405717\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [30720/60000 (51%)] Loss: 0.537435\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [37760/60000 (63%)] Loss: 0.309226\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [31360/60000 (52%)] Loss: 0.527554\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [15360/60000 (26%)] Loss: 0.333690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [8960/60000 (15%)] Loss: 0.333168\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [16000/60000 (27%)] Loss: 0.517207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [9600/60000 (16%)] Loss: 0.483299\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [16640/60000 (28%)] Loss: 0.490113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10240/60000 (17%)] Loss: 0.581276\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [17280/60000 (29%)] Loss: 0.304873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10880/60000 (18%)] Loss: 0.466773\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [17920/60000 (30%)] Loss: 0.883129\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [11520/60000 (19%)] Loss: 0.469236\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [18560/60000 (31%)] Loss: 0.644266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12160/60000 (20%)] Loss: 0.405032\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)] Loss: 0.368490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.480381\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)] Loss: 0.418000\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [32000/60000 (53%)] Loss: 0.585332\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [39040/60000 (65%)] Loss: 0.318198\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [32640/60000 (54%)] Loss: 0.444773\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [39680/60000 (66%)] Loss: 0.371492\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [33280/60000 (55%)] Loss: 0.392805\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [40320/60000 (67%)] Loss: 0.557672\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [33920/60000 (57%)] Loss: 0.337790\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [40960/60000 (68%)] Loss: 0.446905\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [34560/60000 (58%)] Loss: 0.365110\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [41600/60000 (69%)] Loss: 0.312543\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [35200/60000 (59%)] Loss: 0.373095\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [42240/60000 (70%)] Loss: 0.410468\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [35840/60000 (60%)] Loss: 0.433996\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19840/60000 (33%)] Loss: 0.436458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [13440/60000 (22%)] Loss: 0.470270\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [20480/60000 (34%)] Loss: 0.175977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [14080/60000 (23%)] Loss: 0.450604\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [21120/60000 (35%)] Loss: 0.305217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [14720/60000 (25%)] Loss: 0.183430\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [21760/60000 (36%)] Loss: 0.616544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [15360/60000 (26%)] Loss: 0.333690\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [22400/60000 (37%)] Loss: 0.415140\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [16000/60000 (27%)] Loss: 0.517207\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [23040/60000 (38%)] Loss: 0.440677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [16640/60000 (28%)] Loss: 0.490113\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [23680/60000 (39%)] Loss: 0.405125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [17280/60000 (29%)] Loss: 0.304873\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [42880/60000 (71%)] Loss: 0.364003\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [36480/60000 (61%)] Loss: 0.393475\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [43520/60000 (72%)] Loss: 0.439616\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [37120/60000 (62%)] Loss: 0.405717\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [44160/60000 (74%)] Loss: 0.433930\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [37760/60000 (63%)] Loss: 0.309226\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)] Loss: 0.576663\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.418000\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [45440/60000 (76%)] Loss: 0.236858\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [39040/60000 (65%)] Loss: 0.318198\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [46080/60000 (77%)] Loss: 0.400114\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [39680/60000 (66%)] Loss: 0.371492\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [46720/60000 (78%)] Loss: 0.450690\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [40320/60000 (67%)] Loss: 0.557672\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [24320/60000 (41%)] Loss: 0.603815\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [17920/60000 (30%)] Loss: 0.883129\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [24960/60000 (42%)] Loss: 0.270180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [18560/60000 (31%)] Loss: 0.644266\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)] Loss: 0.299216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)] Loss: 0.368490\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [26240/60000 (44%)] Loss: 0.275795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19840/60000 (33%)] Loss: 0.436458\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [26880/60000 (45%)] Loss: 0.287907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [20480/60000 (34%)] Loss: 0.175977\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [27520/60000 (46%)] Loss: 0.421833\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [21120/60000 (35%)] Loss: 0.305217\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [28160/60000 (47%)] Loss: 0.412769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [21760/60000 (36%)] Loss: 0.616544\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [47360/60000 (79%)] Loss: 0.468017\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [40960/60000 (68%)] Loss: 0.446905\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [48000/60000 (80%)] Loss: 0.278412\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [41600/60000 (69%)] Loss: 0.312543\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [48640/60000 (81%)] Loss: 0.431284\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [42240/60000 (70%)] Loss: 0.410468\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [49280/60000 (82%)] Loss: 0.220378\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [42880/60000 (71%)] Loss: 0.364003\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [49920/60000 (83%)] Loss: 0.526374\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [43520/60000 (72%)] Loss: 0.439616\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [50560/60000 (84%)] Loss: 0.337380\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [44160/60000 (74%)] Loss: 0.433930\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)] Loss: 0.459765\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [44800/60000 (75%)] Loss: 0.576663\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [28800/60000 (48%)] Loss: 0.325635\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [22400/60000 (37%)] Loss: 0.415140\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [29440/60000 (49%)] Loss: 0.320863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [23040/60000 (38%)] Loss: 0.440677\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [30080/60000 (50%)] Loss: 0.293968\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [23680/60000 (39%)] Loss: 0.405125\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [30720/60000 (51%)] Loss: 0.564494\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [24320/60000 (41%)] Loss: 0.603815\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [31360/60000 (52%)] Loss: 0.553157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [24960/60000 (42%)] Loss: 0.270180\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)] Loss: 0.573893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.299216\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32640/60000 (54%)] Loss: 0.422172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [26240/60000 (44%)] Loss: 0.275795\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [33280/60000 (55%)] Loss: 0.398617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [26880/60000 (45%)] Loss: 0.287907\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [51840/60000 (86%)] Loss: 0.146162\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [45440/60000 (76%)] Loss: 0.236858\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [52480/60000 (87%)] Loss: 0.313268\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [46080/60000 (77%)] Loss: 0.400114\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [53120/60000 (88%)] Loss: 0.667109\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [46720/60000 (78%)] Loss: 0.450690\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [53760/60000 (90%)] Loss: 0.342094\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [47360/60000 (79%)] Loss: 0.468017\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [54400/60000 (91%)] Loss: 0.290982\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [48000/60000 (80%)] Loss: 0.278412\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [55040/60000 (92%)] Loss: 0.283051\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [48640/60000 (81%)] Loss: 0.431284\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [55680/60000 (93%)] Loss: 0.487282\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [49280/60000 (82%)] Loss: 0.220378\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [33920/60000 (57%)] Loss: 0.328286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [27520/60000 (46%)] Loss: 0.421833\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [34560/60000 (58%)] Loss: 0.345065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [28160/60000 (47%)] Loss: 0.412769\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [35200/60000 (59%)] Loss: 0.364438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [28800/60000 (48%)] Loss: 0.325635\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [35840/60000 (60%)] Loss: 0.388725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [29440/60000 (49%)] Loss: 0.320863\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [36480/60000 (61%)] Loss: 0.419038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30080/60000 (50%)] Loss: 0.293968\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [37120/60000 (62%)] Loss: 0.403179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30720/60000 (51%)] Loss: 0.564494\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [37760/60000 (63%)] Loss: 0.301032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [31360/60000 (52%)] Loss: 0.553157\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [56320/60000 (94%)] Loss: 0.276657\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [49920/60000 (83%)] Loss: 0.526374\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [56960/60000 (95%)] Loss: 0.386344\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [50560/60000 (84%)] Loss: 0.337380\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)] Loss: 0.274206\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.459765\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [58240/60000 (97%)] Loss: 0.340081\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [51840/60000 (86%)] Loss: 0.146162\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [58880/60000 (98%)] Loss: 0.345973\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [52480/60000 (87%)] Loss: 0.313268\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [59520/60000 (99%)] Loss: 0.508906\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [53120/60000 (88%)] Loss: 0.667109\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)] Loss: 0.433173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)] Loss: 0.573893\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [39040/60000 (65%)] Loss: 0.327437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32640/60000 (54%)] Loss: 0.422172\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [39680/60000 (66%)] Loss: 0.384601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [33280/60000 (55%)] Loss: 0.398617\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [40320/60000 (67%)] Loss: 0.561495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [33920/60000 (57%)] Loss: 0.328286\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [40960/60000 (68%)] Loss: 0.443896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [34560/60000 (58%)] Loss: 0.345065\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [41600/60000 (69%)] Loss: 0.331663\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [35200/60000 (59%)] Loss: 0.364438\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [42240/60000 (70%)] Loss: 0.417134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [35840/60000 (60%)] Loss: 0.388725\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Train Epoch: 2 [42880/60000 (71%)] Loss: 0.372698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [36480/60000 (61%)] Loss: 0.419038\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [43520/60000 (72%)] Loss: 0.449386\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [37120/60000 (62%)] Loss: 0.403179\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44160/60000 (74%)] Loss: 0.433247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [37760/60000 (63%)] Loss: 0.301032\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)] Loss: 0.593096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.433173\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [45440/60000 (76%)] Loss: 0.244111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [39040/60000 (65%)] Loss: 0.327437\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [46080/60000 (77%)] Loss: 0.375437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [39680/60000 (66%)] Loss: 0.384601\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [46720/60000 (78%)] Loss: 0.463178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [40320/60000 (67%)] Loss: 0.561495\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [47360/60000 (79%)] Loss: 0.486274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [40960/60000 (68%)] Loss: 0.443896\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 2#011Test set: Average loss: 0.1271, Accuracy: 96.11%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [53760/60000 (90%)] Loss: 0.342094\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [54400/60000 (91%)] Loss: 0.290982\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [48000/60000 (80%)] Loss: 0.283225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [41600/60000 (69%)] Loss: 0.331663\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [48640/60000 (81%)] Loss: 0.423679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [42240/60000 (70%)] Loss: 0.417134\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [49280/60000 (82%)] Loss: 0.242853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [42880/60000 (71%)] Loss: 0.372698\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [49920/60000 (83%)] Loss: 0.552044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [43520/60000 (72%)] Loss: 0.449386\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [50560/60000 (84%)] Loss: 0.355976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44160/60000 (74%)] Loss: 0.433247\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)] Loss: 0.436330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)] Loss: 0.593096\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51840/60000 (86%)] Loss: 0.151432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [45440/60000 (76%)] Loss: 0.244111\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [640/60000 (1%)] Loss: 0.188191\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [55040/60000 (92%)] Loss: 0.283051\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [1280/60000 (2%)] Loss: 0.307816\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [55680/60000 (93%)] Loss: 0.487282\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [1920/60000 (3%)] Loss: 0.371385\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [56320/60000 (94%)] Loss: 0.276657\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [2560/60000 (4%)] Loss: 0.270645\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [56960/60000 (95%)] Loss: 0.386344\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [3200/60000 (5%)] Loss: 0.482244\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [57600/60000 (96%)] Loss: 0.274206\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [3840/60000 (6%)] Loss: 0.318805\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [58240/60000 (97%)] Loss: 0.340081\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [4480/60000 (7%)] Loss: 0.450219\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [58880/60000 (98%)] Loss: 0.345973\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [52480/60000 (87%)] Loss: 0.295480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [46080/60000 (77%)] Loss: 0.375437\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [53120/60000 (88%)] Loss: 0.690675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [46720/60000 (78%)] Loss: 0.463178\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [53760/60000 (90%)] Loss: 0.328112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [47360/60000 (79%)] Loss: 0.486274\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [54400/60000 (91%)] Loss: 0.270983\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [48000/60000 (80%)] Loss: 0.283225\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [55040/60000 (92%)] Loss: 0.292782\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [48640/60000 (81%)] Loss: 0.423679\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [55680/60000 (93%)] Loss: 0.470539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [49280/60000 (82%)] Loss: 0.242853\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [56320/60000 (94%)] Loss: 0.282818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [49920/60000 (83%)] Loss: 0.552044\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [5120/60000 (9%)] Loss: 0.290557\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [59520/60000 (99%)] Loss: 0.508906\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [5760/60000 (10%)] Loss: 0.521361\u001b[0m\n",
      "\u001b[35mEpoch: 2#011Test set: Average loss: 0.1271, Accuracy: 96.11%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)] Loss: 0.419975\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [7040/60000 (12%)] Loss: 0.253756\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 2 with accuracy 0.961\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [7680/60000 (13%)] Loss: 0.215740\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [640/60000 (1%)] Loss: 0.188191\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [8320/60000 (14%)] Loss: 0.217284\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [1280/60000 (2%)] Loss: 0.307816\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [8960/60000 (15%)] Loss: 0.182617\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [1920/60000 (3%)] Loss: 0.371385\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [56960/60000 (95%)] Loss: 0.394969\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [50560/60000 (84%)] Loss: 0.355976\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)] Loss: 0.272814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.436330\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [58240/60000 (97%)] Loss: 0.345529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51840/60000 (86%)] Loss: 0.151432\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [58880/60000 (98%)] Loss: 0.328816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [52480/60000 (87%)] Loss: 0.295480\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [59520/60000 (99%)] Loss: 0.545936\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [53120/60000 (88%)] Loss: 0.690675\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [9600/60000 (16%)] Loss: 0.392773\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [2560/60000 (4%)] Loss: 0.270645\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [10240/60000 (17%)] Loss: 0.356171\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [3200/60000 (5%)] Loss: 0.482244\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [10880/60000 (18%)] Loss: 0.351745\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [3840/60000 (6%)] Loss: 0.318805\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [11520/60000 (19%)] Loss: 0.244153\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [4480/60000 (7%)] Loss: 0.450219\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [12160/60000 (20%)] Loss: 0.391616\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [5120/60000 (9%)] Loss: 0.290557\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)] Loss: 0.331444\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [5760/60000 (10%)] Loss: 0.521361\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [13440/60000 (22%)] Loss: 0.341538\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [6400/60000 (11%)] Loss: 0.419975\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [14080/60000 (23%)] Loss: 0.419192\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [7040/60000 (12%)] Loss: 0.253756\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [14720/60000 (25%)] Loss: 0.154529\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [7680/60000 (13%)] Loss: 0.215740\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [15360/60000 (26%)] Loss: 0.161173\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [8320/60000 (14%)] Loss: 0.217284\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [16000/60000 (27%)] Loss: 0.282931\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [8960/60000 (15%)] Loss: 0.182617\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [16640/60000 (28%)] Loss: 0.402594\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [9600/60000 (16%)] Loss: 0.392773\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [17280/60000 (29%)] Loss: 0.153693\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [10240/60000 (17%)] Loss: 0.356171\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [17920/60000 (30%)] Loss: 0.551427\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [10880/60000 (18%)] Loss: 0.351745\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 2#011Test set: Average loss: 0.1276, Accuracy: 96.04%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [53760/60000 (90%)] Loss: 0.328112\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [54400/60000 (91%)] Loss: 0.270983\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [640/60000 (1%)] Loss: 0.182246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [55040/60000 (92%)] Loss: 0.292782\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [18560/60000 (31%)] Loss: 0.294602\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [11520/60000 (19%)] Loss: 0.244153\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)] Loss: 0.355268\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12160/60000 (20%)] Loss: 0.391616\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [19840/60000 (33%)] Loss: 0.365113\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.331444\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [20480/60000 (34%)] Loss: 0.329172\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [13440/60000 (22%)] Loss: 0.341538\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [21120/60000 (35%)] Loss: 0.277494\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [14080/60000 (23%)] Loss: 0.419192\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [21760/60000 (36%)] Loss: 0.427563\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [14720/60000 (25%)] Loss: 0.154529\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [22400/60000 (37%)] Loss: 0.501674\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [15360/60000 (26%)] Loss: 0.161173\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [1280/60000 (2%)] Loss: 0.300210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [55680/60000 (93%)] Loss: 0.470539\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [1920/60000 (3%)] Loss: 0.343686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [56320/60000 (94%)] Loss: 0.282818\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [2560/60000 (4%)] Loss: 0.284022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [56960/60000 (95%)] Loss: 0.394969\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [3200/60000 (5%)] Loss: 0.472242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)] Loss: 0.272814\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [3840/60000 (6%)] Loss: 0.314999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [58240/60000 (97%)] Loss: 0.345529\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [4480/60000 (7%)] Loss: 0.441781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [58880/60000 (98%)] Loss: 0.328816\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [5120/60000 (9%)] Loss: 0.310087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [59520/60000 (99%)] Loss: 0.545936\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [5760/60000 (10%)] Loss: 0.530352\u001b[0m\n",
      "\u001b[34mEpoch: 2#011Test set: Average loss: 0.1276, Accuracy: 96.04%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [23040/60000 (38%)] Loss: 0.225276\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [16000/60000 (27%)] Loss: 0.282931\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [23680/60000 (39%)] Loss: 0.347606\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [16640/60000 (28%)] Loss: 0.402594\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [24320/60000 (41%)] Loss: 0.314479\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [17280/60000 (29%)] Loss: 0.153693\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [24960/60000 (42%)] Loss: 0.225984\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [17920/60000 (30%)] Loss: 0.551427\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)] Loss: 0.210111\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [18560/60000 (31%)] Loss: 0.294602\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [26240/60000 (44%)] Loss: 0.224254\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19200/60000 (32%)] Loss: 0.355268\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [26880/60000 (45%)] Loss: 0.224423\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19840/60000 (33%)] Loss: 0.365113\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)] Loss: 0.410359\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [7040/60000 (12%)] Loss: 0.243009\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 2 with accuracy 0.960\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [7680/60000 (13%)] Loss: 0.221147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [640/60000 (1%)] Loss: 0.182246\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [8320/60000 (14%)] Loss: 0.209475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1280/60000 (2%)] Loss: 0.300210\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [8960/60000 (15%)] Loss: 0.168862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1920/60000 (3%)] Loss: 0.343686\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [9600/60000 (16%)] Loss: 0.399190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [2560/60000 (4%)] Loss: 0.284022\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [10240/60000 (17%)] Loss: 0.340694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [3200/60000 (5%)] Loss: 0.472242\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [27520/60000 (46%)] Loss: 0.341994\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [20480/60000 (34%)] Loss: 0.329172\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [28160/60000 (47%)] Loss: 0.260877\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [21120/60000 (35%)] Loss: 0.277494\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [28800/60000 (48%)] Loss: 0.367683\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [21760/60000 (36%)] Loss: 0.427563\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [29440/60000 (49%)] Loss: 0.295395\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [22400/60000 (37%)] Loss: 0.501674\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [30080/60000 (50%)] Loss: 0.491016\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [23040/60000 (38%)] Loss: 0.225276\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [30720/60000 (51%)] Loss: 0.469296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [23680/60000 (39%)] Loss: 0.347606\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [31360/60000 (52%)] Loss: 0.375431\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [24320/60000 (41%)] Loss: 0.314479\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [10880/60000 (18%)] Loss: 0.382319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [3840/60000 (6%)] Loss: 0.314999\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [11520/60000 (19%)] Loss: 0.251432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [4480/60000 (7%)] Loss: 0.441781\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12160/60000 (20%)] Loss: 0.402135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [5120/60000 (9%)] Loss: 0.310087\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)] Loss: 0.325823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [5760/60000 (10%)] Loss: 0.530352\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [13440/60000 (22%)] Loss: 0.356703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)] Loss: 0.410359\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [14080/60000 (23%)] Loss: 0.398225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [7040/60000 (12%)] Loss: 0.243009\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [14720/60000 (25%)] Loss: 0.152495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [7680/60000 (13%)] Loss: 0.221147\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)] Loss: 0.222094\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [24960/60000 (42%)] Loss: 0.225984\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [32640/60000 (54%)] Loss: 0.249692\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.210111\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [33280/60000 (55%)] Loss: 0.269090\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [26240/60000 (44%)] Loss: 0.224254\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [33920/60000 (57%)] Loss: 0.183516\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [26880/60000 (45%)] Loss: 0.224423\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [34560/60000 (58%)] Loss: 0.248295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [27520/60000 (46%)] Loss: 0.341994\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [35200/60000 (59%)] Loss: 0.286480\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [28160/60000 (47%)] Loss: 0.260877\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [35840/60000 (60%)] Loss: 0.235519\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [28800/60000 (48%)] Loss: 0.367683\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [15360/60000 (26%)] Loss: 0.167592\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [8320/60000 (14%)] Loss: 0.209475\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [16000/60000 (27%)] Loss: 0.273813\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [8960/60000 (15%)] Loss: 0.168862\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [16640/60000 (28%)] Loss: 0.394751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [9600/60000 (16%)] Loss: 0.399190\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [17280/60000 (29%)] Loss: 0.159762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [10240/60000 (17%)] Loss: 0.340694\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [17920/60000 (30%)] Loss: 0.573488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [10880/60000 (18%)] Loss: 0.382319\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [18560/60000 (31%)] Loss: 0.300296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [11520/60000 (19%)] Loss: 0.251432\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)] Loss: 0.351069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12160/60000 (20%)] Loss: 0.402135\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19840/60000 (33%)] Loss: 0.378202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.325823\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [36480/60000 (61%)] Loss: 0.243498\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [29440/60000 (49%)] Loss: 0.295395\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [37120/60000 (62%)] Loss: 0.188892\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [30080/60000 (50%)] Loss: 0.491016\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [37760/60000 (63%)] Loss: 0.253286\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [30720/60000 (51%)] Loss: 0.469296\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)] Loss: 0.321469\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [31360/60000 (52%)] Loss: 0.375431\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [39040/60000 (65%)] Loss: 0.174761\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [32000/60000 (53%)] Loss: 0.222094\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [39680/60000 (66%)] Loss: 0.495154\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [32640/60000 (54%)] Loss: 0.249692\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [40320/60000 (67%)] Loss: 0.267102\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [33280/60000 (55%)] Loss: 0.269090\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Train Epoch: 3 [20480/60000 (34%)] Loss: 0.327778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [13440/60000 (22%)] Loss: 0.356703\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [21120/60000 (35%)] Loss: 0.256373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [14080/60000 (23%)] Loss: 0.398225\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [21760/60000 (36%)] Loss: 0.449268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [14720/60000 (25%)] Loss: 0.152495\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [22400/60000 (37%)] Loss: 0.459165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [15360/60000 (26%)] Loss: 0.167592\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [23040/60000 (38%)] Loss: 0.231470\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [16000/60000 (27%)] Loss: 0.273813\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [23680/60000 (39%)] Loss: 0.353869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [16640/60000 (28%)] Loss: 0.394751\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [24320/60000 (41%)] Loss: 0.305309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [17280/60000 (29%)] Loss: 0.159762\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [40960/60000 (68%)] Loss: 0.291024\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [33920/60000 (57%)] Loss: 0.183516\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [41600/60000 (69%)] Loss: 0.297025\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [34560/60000 (58%)] Loss: 0.248295\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [42240/60000 (70%)] Loss: 0.208850\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [35200/60000 (59%)] Loss: 0.286480\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [42880/60000 (71%)] Loss: 0.176966\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [35840/60000 (60%)] Loss: 0.235519\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [43520/60000 (72%)] Loss: 0.456641\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [36480/60000 (61%)] Loss: 0.243498\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [44160/60000 (74%)] Loss: 0.555128\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [37120/60000 (62%)] Loss: 0.188892\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)] Loss: 0.431796\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [37760/60000 (63%)] Loss: 0.253286\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [24960/60000 (42%)] Loss: 0.237622\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [17920/60000 (30%)] Loss: 0.573488\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)] Loss: 0.208946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [18560/60000 (31%)] Loss: 0.300296\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [26240/60000 (44%)] Loss: 0.219306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)] Loss: 0.351069\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [26880/60000 (45%)] Loss: 0.224084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19840/60000 (33%)] Loss: 0.378202\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [27520/60000 (46%)] Loss: 0.344611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [20480/60000 (34%)] Loss: 0.327778\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [28160/60000 (47%)] Loss: 0.275144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [21120/60000 (35%)] Loss: 0.256373\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [28800/60000 (48%)] Loss: 0.361700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [21760/60000 (36%)] Loss: 0.449268\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [45440/60000 (76%)] Loss: 0.311188\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.321469\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [46080/60000 (77%)] Loss: 0.277556\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [39040/60000 (65%)] Loss: 0.174761\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [46720/60000 (78%)] Loss: 0.310013\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [39680/60000 (66%)] Loss: 0.495154\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [47360/60000 (79%)] Loss: 0.388766\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [40320/60000 (67%)] Loss: 0.267102\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [48000/60000 (80%)] Loss: 0.260211\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [40960/60000 (68%)] Loss: 0.291024\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [48640/60000 (81%)] Loss: 0.329900\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [41600/60000 (69%)] Loss: 0.297025\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [49280/60000 (82%)] Loss: 0.179960\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [42240/60000 (70%)] Loss: 0.208850\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [29440/60000 (49%)] Loss: 0.297811\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [22400/60000 (37%)] Loss: 0.459165\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [30080/60000 (50%)] Loss: 0.488520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [23040/60000 (38%)] Loss: 0.231470\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [30720/60000 (51%)] Loss: 0.485148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [23680/60000 (39%)] Loss: 0.353869\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [31360/60000 (52%)] Loss: 0.396321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [24320/60000 (41%)] Loss: 0.305309\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)] Loss: 0.226314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [24960/60000 (42%)] Loss: 0.237622\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32640/60000 (54%)] Loss: 0.251923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.208946\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [33280/60000 (55%)] Loss: 0.253399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [26240/60000 (44%)] Loss: 0.219306\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [33920/60000 (57%)] Loss: 0.187379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [26880/60000 (45%)] Loss: 0.224084\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [49920/60000 (83%)] Loss: 0.386525\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [42880/60000 (71%)] Loss: 0.176966\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [50560/60000 (84%)] Loss: 0.306625\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [43520/60000 (72%)] Loss: 0.456641\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)] Loss: 0.449745\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [44160/60000 (74%)] Loss: 0.555128\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [51840/60000 (86%)] Loss: 0.255968\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [44800/60000 (75%)] Loss: 0.431796\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [52480/60000 (87%)] Loss: 0.163671\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [45440/60000 (76%)] Loss: 0.311188\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [53120/60000 (88%)] Loss: 0.489387\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [46080/60000 (77%)] Loss: 0.277556\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [53760/60000 (90%)] Loss: 0.323017\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [46720/60000 (78%)] Loss: 0.310013\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [34560/60000 (58%)] Loss: 0.269207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [27520/60000 (46%)] Loss: 0.344611\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [35200/60000 (59%)] Loss: 0.311749\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [28160/60000 (47%)] Loss: 0.275144\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [35840/60000 (60%)] Loss: 0.226626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [28800/60000 (48%)] Loss: 0.361700\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [36480/60000 (61%)] Loss: 0.256693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [29440/60000 (49%)] Loss: 0.297811\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [37120/60000 (62%)] Loss: 0.181268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [30080/60000 (50%)] Loss: 0.488520\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [37760/60000 (63%)] Loss: 0.264153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [30720/60000 (51%)] Loss: 0.485148\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)] Loss: 0.330228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [31360/60000 (52%)] Loss: 0.396321\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [54400/60000 (91%)] Loss: 0.140819\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [47360/60000 (79%)] Loss: 0.388766\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [55040/60000 (92%)] Loss: 0.409544\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [48000/60000 (80%)] Loss: 0.260211\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [55680/60000 (93%)] Loss: 0.369429\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [48640/60000 (81%)] Loss: 0.329900\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [56320/60000 (94%)] Loss: 0.115198\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [49280/60000 (82%)] Loss: 0.179960\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [56960/60000 (95%)] Loss: 0.260239\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [49920/60000 (83%)] Loss: 0.386525\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)] Loss: 0.197130\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [50560/60000 (84%)] Loss: 0.306625\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [58240/60000 (97%)] Loss: 0.112925\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.449745\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [39040/60000 (65%)] Loss: 0.176470\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)] Loss: 0.226314\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [39680/60000 (66%)] Loss: 0.522949\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32640/60000 (54%)] Loss: 0.251923\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [40320/60000 (67%)] Loss: 0.285445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [33280/60000 (55%)] Loss: 0.253399\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [40960/60000 (68%)] Loss: 0.289937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [33920/60000 (57%)] Loss: 0.187379\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [41600/60000 (69%)] Loss: 0.314269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [34560/60000 (58%)] Loss: 0.269207\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [42240/60000 (70%)] Loss: 0.213329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [35200/60000 (59%)] Loss: 0.311749\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [42880/60000 (71%)] Loss: 0.183367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [35840/60000 (60%)] Loss: 0.226626\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [43520/60000 (72%)] Loss: 0.448157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [36480/60000 (61%)] Loss: 0.256693\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [58880/60000 (98%)] Loss: 0.258117\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [51840/60000 (86%)] Loss: 0.255968\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [59520/60000 (99%)] Loss: 0.386674\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [52480/60000 (87%)] Loss: 0.163671\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44160/60000 (74%)] Loss: 0.566748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [37120/60000 (62%)] Loss: 0.181268\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)] Loss: 0.430715\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [37760/60000 (63%)] Loss: 0.264153\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [45440/60000 (76%)] Loss: 0.339052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.330228\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [46080/60000 (77%)] Loss: 0.281341\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [39040/60000 (65%)] Loss: 0.176470\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [46720/60000 (78%)] Loss: 0.330494\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [39680/60000 (66%)] Loss: 0.522949\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [47360/60000 (79%)] Loss: 0.398212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [40320/60000 (67%)] Loss: 0.285445\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [48000/60000 (80%)] Loss: 0.253095\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [40960/60000 (68%)] Loss: 0.289937\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [48640/60000 (81%)] Loss: 0.317855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [41600/60000 (69%)] Loss: 0.314269\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [49280/60000 (82%)] Loss: 0.184017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [42240/60000 (70%)] Loss: 0.213329\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [49920/60000 (83%)] Loss: 0.369957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [42880/60000 (71%)] Loss: 0.183367\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [50560/60000 (84%)] Loss: 0.320062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [43520/60000 (72%)] Loss: 0.448157\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)] Loss: 0.464202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44160/60000 (74%)] Loss: 0.566748\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51840/60000 (86%)] Loss: 0.247806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)] Loss: 0.430715\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [52480/60000 (87%)] Loss: 0.171320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [45440/60000 (76%)] Loss: 0.339052\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 3#011Test set: Average loss: 0.0984, Accuracy: 96.79%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [53120/60000 (88%)] Loss: 0.489387\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [53760/60000 (90%)] Loss: 0.323017\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [640/60000 (1%)] Loss: 0.373113\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [54400/60000 (91%)] Loss: 0.140819\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [1280/60000 (2%)] Loss: 0.280556\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [55040/60000 (92%)] Loss: 0.409544\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [1920/60000 (3%)] Loss: 0.184633\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [55680/60000 (93%)] Loss: 0.369429\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [2560/60000 (4%)] Loss: 0.309870\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [56320/60000 (94%)] Loss: 0.115198\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [53120/60000 (88%)] Loss: 0.492994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [46080/60000 (77%)] Loss: 0.281341\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [53760/60000 (90%)] Loss: 0.331074\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [46720/60000 (78%)] Loss: 0.330494\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [54400/60000 (91%)] Loss: 0.143555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [47360/60000 (79%)] Loss: 0.398212\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [55040/60000 (92%)] Loss: 0.413738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [48000/60000 (80%)] Loss: 0.253095\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [55680/60000 (93%)] Loss: 0.374394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [48640/60000 (81%)] Loss: 0.317855\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [56320/60000 (94%)] Loss: 0.105711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [49280/60000 (82%)] Loss: 0.184017\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [56960/60000 (95%)] Loss: 0.261124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [49920/60000 (83%)] Loss: 0.369957\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)] Loss: 0.184127\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [50560/60000 (84%)] Loss: 0.320062\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [3200/60000 (5%)] Loss: 0.311225\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [56960/60000 (95%)] Loss: 0.260239\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [3840/60000 (6%)] Loss: 0.343502\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [57600/60000 (96%)] Loss: 0.197130\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [4480/60000 (7%)] Loss: 0.241728\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [58240/60000 (97%)] Loss: 0.112925\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [5120/60000 (9%)] Loss: 0.289741\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [58880/60000 (98%)] Loss: 0.258117\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [5760/60000 (10%)] Loss: 0.435970\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [59520/60000 (99%)] Loss: 0.386674\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)] Loss: 0.227905\u001b[0m\n",
      "\u001b[35mEpoch: 3#011Test set: Average loss: 0.0984, Accuracy: 96.79%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [7040/60000 (12%)] Loss: 0.233405\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [58240/60000 (97%)] Loss: 0.094730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.464202\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [58880/60000 (98%)] Loss: 0.261620\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51840/60000 (86%)] Loss: 0.247806\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [59520/60000 (99%)] Loss: 0.384137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [52480/60000 (87%)] Loss: 0.171320\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [7680/60000 (13%)] Loss: 0.167123\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 3 with accuracy 0.968\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [8320/60000 (14%)] Loss: 0.205867\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [640/60000 (1%)] Loss: 0.373113\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [8960/60000 (15%)] Loss: 0.330423\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [1280/60000 (2%)] Loss: 0.280556\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [9600/60000 (16%)] Loss: 0.172425\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [1920/60000 (3%)] Loss: 0.184633\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [10240/60000 (17%)] Loss: 0.419445\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [2560/60000 (4%)] Loss: 0.309870\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [10880/60000 (18%)] Loss: 0.313692\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [3200/60000 (5%)] Loss: 0.311225\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [11520/60000 (19%)] Loss: 0.129704\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [3840/60000 (6%)] Loss: 0.343502\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [12160/60000 (20%)] Loss: 0.231296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [4480/60000 (7%)] Loss: 0.241728\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)] Loss: 0.422094\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [5120/60000 (9%)] Loss: 0.289741\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [13440/60000 (22%)] Loss: 0.251616\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [5760/60000 (10%)] Loss: 0.435970\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [14080/60000 (23%)] Loss: 0.213122\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [6400/60000 (11%)] Loss: 0.227905\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [14720/60000 (25%)] Loss: 0.194985\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [7040/60000 (12%)] Loss: 0.233405\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [15360/60000 (26%)] Loss: 0.134083\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [7680/60000 (13%)] Loss: 0.167123\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [16000/60000 (27%)] Loss: 0.287389\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [8320/60000 (14%)] Loss: 0.205867\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [16640/60000 (28%)] Loss: 0.138685\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [8960/60000 (15%)] Loss: 0.330423\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [17280/60000 (29%)] Loss: 0.208941\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [9600/60000 (16%)] Loss: 0.172425\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [17920/60000 (30%)] Loss: 0.365488\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [10240/60000 (17%)] Loss: 0.419445\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [18560/60000 (31%)] Loss: 0.296961\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [10880/60000 (18%)] Loss: 0.313692\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)] Loss: 0.294917\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [11520/60000 (19%)] Loss: 0.129704\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [19840/60000 (33%)] Loss: 0.183932\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12160/60000 (20%)] Loss: 0.231296\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [20480/60000 (34%)] Loss: 0.169715\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12800/60000 (21%)] Loss: 0.422094\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Epoch: 3#011Test set: Average loss: 0.1001, Accuracy: 96.79%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [53120/60000 (88%)] Loss: 0.492994\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [53760/60000 (90%)] Loss: 0.331074\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [640/60000 (1%)] Loss: 0.390152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [54400/60000 (91%)] Loss: 0.143555\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [1280/60000 (2%)] Loss: 0.274002\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [55040/60000 (92%)] Loss: 0.413738\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [1920/60000 (3%)] Loss: 0.184563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [55680/60000 (93%)] Loss: 0.374394\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [2560/60000 (4%)] Loss: 0.316577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [56320/60000 (94%)] Loss: 0.105711\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [3200/60000 (5%)] Loss: 0.321438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [56960/60000 (95%)] Loss: 0.261124\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [3840/60000 (6%)] Loss: 0.336956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)] Loss: 0.184127\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [4480/60000 (7%)] Loss: 0.236679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [58240/60000 (97%)] Loss: 0.094730\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [5120/60000 (9%)] Loss: 0.308483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [58880/60000 (98%)] Loss: 0.261620\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [5760/60000 (10%)] Loss: 0.442454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [59520/60000 (99%)] Loss: 0.384137\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)] Loss: 0.251460\u001b[0m\n",
      "\u001b[34mEpoch: 3#011Test set: Average loss: 0.1001, Accuracy: 96.79%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [21120/60000 (35%)] Loss: 0.169781\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [13440/60000 (22%)] Loss: 0.251616\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [21760/60000 (36%)] Loss: 0.707615\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [14080/60000 (23%)] Loss: 0.213122\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [22400/60000 (37%)] Loss: 0.359918\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [14720/60000 (25%)] Loss: 0.194985\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [23040/60000 (38%)] Loss: 0.225943\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [15360/60000 (26%)] Loss: 0.134083\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [23680/60000 (39%)] Loss: 0.209084\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [16000/60000 (27%)] Loss: 0.287389\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [24320/60000 (41%)] Loss: 0.241593\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [16640/60000 (28%)] Loss: 0.138685\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [24960/60000 (42%)] Loss: 0.130299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [17280/60000 (29%)] Loss: 0.208941\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [7040/60000 (12%)] Loss: 0.234001\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [7680/60000 (13%)] Loss: 0.161243\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 3 with accuracy 0.968\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [8320/60000 (14%)] Loss: 0.218990\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [640/60000 (1%)] Loss: 0.390152\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [8960/60000 (15%)] Loss: 0.342730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1280/60000 (2%)] Loss: 0.274002\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [9600/60000 (16%)] Loss: 0.179071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1920/60000 (3%)] Loss: 0.184563\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [10240/60000 (17%)] Loss: 0.409086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [2560/60000 (4%)] Loss: 0.316577\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [10880/60000 (18%)] Loss: 0.303497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [3200/60000 (5%)] Loss: 0.321438\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [11520/60000 (19%)] Loss: 0.126406\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [3840/60000 (6%)] Loss: 0.336956\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)] Loss: 0.152690\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [17920/60000 (30%)] Loss: 0.365488\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [26240/60000 (44%)] Loss: 0.185463\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [18560/60000 (31%)] Loss: 0.296961\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [26880/60000 (45%)] Loss: 0.188641\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [19200/60000 (32%)] Loss: 0.294917\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [27520/60000 (46%)] Loss: 0.361345\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [19840/60000 (33%)] Loss: 0.183932\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [28160/60000 (47%)] Loss: 0.242181\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [20480/60000 (34%)] Loss: 0.169715\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [28800/60000 (48%)] Loss: 0.226317\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [21120/60000 (35%)] Loss: 0.169781\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [29440/60000 (49%)] Loss: 0.263864\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [21760/60000 (36%)] Loss: 0.707615\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12160/60000 (20%)] Loss: 0.210741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [4480/60000 (7%)] Loss: 0.236679\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)] Loss: 0.433896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [5120/60000 (9%)] Loss: 0.308483\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [13440/60000 (22%)] Loss: 0.230393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [5760/60000 (10%)] Loss: 0.442454\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [14080/60000 (23%)] Loss: 0.203935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)] Loss: 0.251460\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [14720/60000 (25%)] Loss: 0.197764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [7040/60000 (12%)] Loss: 0.234001\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [15360/60000 (26%)] Loss: 0.133936\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [7680/60000 (13%)] Loss: 0.161243\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [16000/60000 (27%)] Loss: 0.248361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [8320/60000 (14%)] Loss: 0.218990\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [30080/60000 (50%)] Loss: 0.175941\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [22400/60000 (37%)] Loss: 0.359918\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [30720/60000 (51%)] Loss: 0.334557\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [23040/60000 (38%)] Loss: 0.225943\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [31360/60000 (52%)] Loss: 0.282194\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [23680/60000 (39%)] Loss: 0.209084\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)] Loss: 0.359634\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [24320/60000 (41%)] Loss: 0.241593\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [32640/60000 (54%)] Loss: 0.349145\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [24960/60000 (42%)] Loss: 0.130299\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [33280/60000 (55%)] Loss: 0.221529\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [25600/60000 (43%)] Loss: 0.152690\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [33920/60000 (57%)] Loss: 0.161350\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [26240/60000 (44%)] Loss: 0.185463\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [16640/60000 (28%)] Loss: 0.130945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [8960/60000 (15%)] Loss: 0.342730\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [17280/60000 (29%)] Loss: 0.223977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [9600/60000 (16%)] Loss: 0.179071\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [17920/60000 (30%)] Loss: 0.401952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [10240/60000 (17%)] Loss: 0.409086\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [18560/60000 (31%)] Loss: 0.324941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [10880/60000 (18%)] Loss: 0.303497\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)] Loss: 0.286163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [11520/60000 (19%)] Loss: 0.126406\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19840/60000 (33%)] Loss: 0.170935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12160/60000 (20%)] Loss: 0.210741\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [20480/60000 (34%)] Loss: 0.161109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)] Loss: 0.433896\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [34560/60000 (58%)] Loss: 0.247382\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [26880/60000 (45%)] Loss: 0.188641\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [35200/60000 (59%)] Loss: 0.292795\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [27520/60000 (46%)] Loss: 0.361345\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [35840/60000 (60%)] Loss: 0.370253\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [28160/60000 (47%)] Loss: 0.242181\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [36480/60000 (61%)] Loss: 0.310459\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [28800/60000 (48%)] Loss: 0.226317\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [37120/60000 (62%)] Loss: 0.178092\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [29440/60000 (49%)] Loss: 0.263864\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [37760/60000 (63%)] Loss: 0.349026\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [30080/60000 (50%)] Loss: 0.175941\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)] Loss: 0.361567\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [30720/60000 (51%)] Loss: 0.334557\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [21120/60000 (35%)] Loss: 0.197077\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [13440/60000 (22%)] Loss: 0.230393\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [21760/60000 (36%)] Loss: 0.714219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [14080/60000 (23%)] Loss: 0.203935\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [22400/60000 (37%)] Loss: 0.374578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [14720/60000 (25%)] Loss: 0.197764\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [23040/60000 (38%)] Loss: 0.212109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [15360/60000 (26%)] Loss: 0.133936\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [23680/60000 (39%)] Loss: 0.211061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [16000/60000 (27%)] Loss: 0.248361\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [24320/60000 (41%)] Loss: 0.240611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [16640/60000 (28%)] Loss: 0.130945\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [24960/60000 (42%)] Loss: 0.139345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [17280/60000 (29%)] Loss: 0.223977\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)] Loss: 0.150706\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [17920/60000 (30%)] Loss: 0.401952\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [39040/60000 (65%)] Loss: 0.409996\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [31360/60000 (52%)] Loss: 0.282194\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [39680/60000 (66%)] Loss: 0.294282\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [32000/60000 (53%)] Loss: 0.359634\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [40320/60000 (67%)] Loss: 0.170419\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [32640/60000 (54%)] Loss: 0.349145\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [40960/60000 (68%)] Loss: 0.213312\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [33280/60000 (55%)] Loss: 0.221529\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [41600/60000 (69%)] Loss: 0.349455\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [33920/60000 (57%)] Loss: 0.161350\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [42240/60000 (70%)] Loss: 0.240347\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [34560/60000 (58%)] Loss: 0.247382\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [42880/60000 (71%)] Loss: 0.199385\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [35200/60000 (59%)] Loss: 0.292795\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [26240/60000 (44%)] Loss: 0.185044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [18560/60000 (31%)] Loss: 0.324941\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [26880/60000 (45%)] Loss: 0.178271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)] Loss: 0.286163\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [27520/60000 (46%)] Loss: 0.365207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19840/60000 (33%)] Loss: 0.170935\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [28160/60000 (47%)] Loss: 0.235655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [20480/60000 (34%)] Loss: 0.161109\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [28800/60000 (48%)] Loss: 0.229707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [21120/60000 (35%)] Loss: 0.197077\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [29440/60000 (49%)] Loss: 0.297347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [21760/60000 (36%)] Loss: 0.714219\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [30080/60000 (50%)] Loss: 0.186152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [22400/60000 (37%)] Loss: 0.374578\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [43520/60000 (72%)] Loss: 0.393463\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [35840/60000 (60%)] Loss: 0.370253\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [44160/60000 (74%)] Loss: 0.409760\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [36480/60000 (61%)] Loss: 0.310459\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)] Loss: 0.428043\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [37120/60000 (62%)] Loss: 0.178092\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [45440/60000 (76%)] Loss: 0.228350\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [37760/60000 (63%)] Loss: 0.349026\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [46080/60000 (77%)] Loss: 0.136083\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [38400/60000 (64%)] Loss: 0.361567\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [46720/60000 (78%)] Loss: 0.235351\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [39040/60000 (65%)] Loss: 0.409996\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [47360/60000 (79%)] Loss: 0.252907\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [39680/60000 (66%)] Loss: 0.294282\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [48000/60000 (80%)] Loss: 0.257572\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [40320/60000 (67%)] Loss: 0.170419\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [30720/60000 (51%)] Loss: 0.328344\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [23040/60000 (38%)] Loss: 0.212109\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [31360/60000 (52%)] Loss: 0.249462\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [23680/60000 (39%)] Loss: 0.211061\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)] Loss: 0.337498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [24320/60000 (41%)] Loss: 0.240611\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32640/60000 (54%)] Loss: 0.347365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [24960/60000 (42%)] Loss: 0.139345\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [33280/60000 (55%)] Loss: 0.226927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)] Loss: 0.150706\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [33920/60000 (57%)] Loss: 0.158179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [26240/60000 (44%)] Loss: 0.185044\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [34560/60000 (58%)] Loss: 0.258435\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [26880/60000 (45%)] Loss: 0.178271\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [48640/60000 (81%)] Loss: 0.144166\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [40960/60000 (68%)] Loss: 0.213312\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [49280/60000 (82%)] Loss: 0.208069\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [41600/60000 (69%)] Loss: 0.349455\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [49920/60000 (83%)] Loss: 0.672803\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [42240/60000 (70%)] Loss: 0.240347\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [50560/60000 (84%)] Loss: 0.210969\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [42880/60000 (71%)] Loss: 0.199385\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)] Loss: 0.256401\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [43520/60000 (72%)] Loss: 0.393463\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [51840/60000 (86%)] Loss: 0.140458\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [44160/60000 (74%)] Loss: 0.409760\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [52480/60000 (87%)] Loss: 0.172894\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [44800/60000 (75%)] Loss: 0.428043\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [35200/60000 (59%)] Loss: 0.325668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [27520/60000 (46%)] Loss: 0.365207\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [35840/60000 (60%)] Loss: 0.388755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [28160/60000 (47%)] Loss: 0.235655\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [36480/60000 (61%)] Loss: 0.292564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [28800/60000 (48%)] Loss: 0.229707\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [37120/60000 (62%)] Loss: 0.196371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [29440/60000 (49%)] Loss: 0.297347\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [37760/60000 (63%)] Loss: 0.345195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [30080/60000 (50%)] Loss: 0.186152\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)] Loss: 0.368374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [30720/60000 (51%)] Loss: 0.328344\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [39040/60000 (65%)] Loss: 0.394422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [31360/60000 (52%)] Loss: 0.249462\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [53120/60000 (88%)] Loss: 0.236886\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [45440/60000 (76%)] Loss: 0.228350\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [53760/60000 (90%)] Loss: 0.143807\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [46080/60000 (77%)] Loss: 0.136083\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [54400/60000 (91%)] Loss: 0.125940\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [46720/60000 (78%)] Loss: 0.235351\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [55040/60000 (92%)] Loss: 0.337985\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [47360/60000 (79%)] Loss: 0.252907\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [55680/60000 (93%)] Loss: 0.224241\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [48000/60000 (80%)] Loss: 0.257572\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [56320/60000 (94%)] Loss: 0.194472\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [48640/60000 (81%)] Loss: 0.144166\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [56960/60000 (95%)] Loss: 0.243681\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [49280/60000 (82%)] Loss: 0.208069\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [39680/60000 (66%)] Loss: 0.303954\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)] Loss: 0.337498\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [40320/60000 (67%)] Loss: 0.174165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32640/60000 (54%)] Loss: 0.347365\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [40960/60000 (68%)] Loss: 0.226078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [33280/60000 (55%)] Loss: 0.226927\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [41600/60000 (69%)] Loss: 0.350925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [33920/60000 (57%)] Loss: 0.158179\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [42240/60000 (70%)] Loss: 0.223804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [34560/60000 (58%)] Loss: 0.258435\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [42880/60000 (71%)] Loss: 0.202556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [35200/60000 (59%)] Loss: 0.325668\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [43520/60000 (72%)] Loss: 0.406482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [35840/60000 (60%)] Loss: 0.388755\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44160/60000 (74%)] Loss: 0.442430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [36480/60000 (61%)] Loss: 0.292564\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)] Loss: 0.205564\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [49920/60000 (83%)] Loss: 0.672803\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [58240/60000 (97%)] Loss: 0.281686\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [50560/60000 (84%)] Loss: 0.210969\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [58880/60000 (98%)] Loss: 0.243636\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [51200/60000 (85%)] Loss: 0.256401\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [59520/60000 (99%)] Loss: 0.341282\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [51840/60000 (86%)] Loss: 0.140458\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)] Loss: 0.424614\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [37120/60000 (62%)] Loss: 0.196371\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [45440/60000 (76%)] Loss: 0.194445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [37760/60000 (63%)] Loss: 0.345195\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [46080/60000 (77%)] Loss: 0.142868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)] Loss: 0.368374\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [46720/60000 (78%)] Loss: 0.253209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [39040/60000 (65%)] Loss: 0.394422\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [47360/60000 (79%)] Loss: 0.268019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [39680/60000 (66%)] Loss: 0.303954\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [48000/60000 (80%)] Loss: 0.272374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [40320/60000 (67%)] Loss: 0.174165\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [48640/60000 (81%)] Loss: 0.141553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [40960/60000 (68%)] Loss: 0.226078\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [49280/60000 (82%)] Loss: 0.209236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [41600/60000 (69%)] Loss: 0.350925\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [49920/60000 (83%)] Loss: 0.682645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [42240/60000 (70%)] Loss: 0.223804\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [50560/60000 (84%)] Loss: 0.224259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [42880/60000 (71%)] Loss: 0.202556\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)] Loss: 0.242186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [43520/60000 (72%)] Loss: 0.406482\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51840/60000 (86%)] Loss: 0.122007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44160/60000 (74%)] Loss: 0.442430\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [52480/60000 (87%)] Loss: 0.184574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)] Loss: 0.424614\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [53120/60000 (88%)] Loss: 0.242496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [45440/60000 (76%)] Loss: 0.194445\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 4#011Test set: Average loss: 0.0840, Accuracy: 97.31%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [52480/60000 (87%)] Loss: 0.172894\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [53120/60000 (88%)] Loss: 0.236886\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [640/60000 (1%)] Loss: 0.199380\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [53760/60000 (90%)] Loss: 0.143807\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [53760/60000 (90%)] Loss: 0.131364\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [46080/60000 (77%)] Loss: 0.142868\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [54400/60000 (91%)] Loss: 0.132252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [46720/60000 (78%)] Loss: 0.253209\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [55040/60000 (92%)] Loss: 0.336301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [47360/60000 (79%)] Loss: 0.268019\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [55680/60000 (93%)] Loss: 0.215824\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [48000/60000 (80%)] Loss: 0.272374\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [56320/60000 (94%)] Loss: 0.174604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [48640/60000 (81%)] Loss: 0.141553\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [56960/60000 (95%)] Loss: 0.249637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [49280/60000 (82%)] Loss: 0.209236\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)] Loss: 0.190419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [49920/60000 (83%)] Loss: 0.682645\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [1280/60000 (2%)] Loss: 0.206524\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [54400/60000 (91%)] Loss: 0.125940\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [1920/60000 (3%)] Loss: 0.128889\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [55040/60000 (92%)] Loss: 0.337985\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [2560/60000 (4%)] Loss: 0.193269\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [55680/60000 (93%)] Loss: 0.224241\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [3200/60000 (5%)] Loss: 0.269071\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [56320/60000 (94%)] Loss: 0.194472\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [3840/60000 (6%)] Loss: 0.134182\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [56960/60000 (95%)] Loss: 0.243681\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [4480/60000 (7%)] Loss: 0.175419\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [57600/60000 (96%)] Loss: 0.205564\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [5120/60000 (9%)] Loss: 0.282501\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [58240/60000 (97%)] Loss: 0.281686\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [58240/60000 (97%)] Loss: 0.260852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [50560/60000 (84%)] Loss: 0.224259\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [58880/60000 (98%)] Loss: 0.233371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)] Loss: 0.242186\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [59520/60000 (99%)] Loss: 0.321203\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51840/60000 (86%)] Loss: 0.122007\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [5760/60000 (10%)] Loss: 0.372603\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [58880/60000 (98%)] Loss: 0.243636\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)] Loss: 0.206372\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [59520/60000 (99%)] Loss: 0.341282\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [7040/60000 (12%)] Loss: 0.262054\u001b[0m\n",
      "\u001b[35mEpoch: 4#011Test set: Average loss: 0.0840, Accuracy: 97.31%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [7680/60000 (13%)] Loss: 0.187857\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [8320/60000 (14%)] Loss: 0.218296\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 4 with accuracy 0.973\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [8960/60000 (15%)] Loss: 0.153583\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [640/60000 (1%)] Loss: 0.199380\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [9600/60000 (16%)] Loss: 0.310232\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [1280/60000 (2%)] Loss: 0.206524\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [10240/60000 (17%)] Loss: 0.308171\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [1920/60000 (3%)] Loss: 0.128889\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [10880/60000 (18%)] Loss: 0.218034\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [2560/60000 (4%)] Loss: 0.193269\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [11520/60000 (19%)] Loss: 0.207137\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [3200/60000 (5%)] Loss: 0.269071\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [12160/60000 (20%)] Loss: 0.286927\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [3840/60000 (6%)] Loss: 0.134182\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)] Loss: 0.252096\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [4480/60000 (7%)] Loss: 0.175419\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [13440/60000 (22%)] Loss: 0.258997\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [5120/60000 (9%)] Loss: 0.282501\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [14080/60000 (23%)] Loss: 0.253746\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [5760/60000 (10%)] Loss: 0.372603\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 4#011Test set: Average loss: 0.0841, Accuracy: 97.27%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [52480/60000 (87%)] Loss: 0.184574\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [53120/60000 (88%)] Loss: 0.242496\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [640/60000 (1%)] Loss: 0.171839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [53760/60000 (90%)] Loss: 0.131364\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [1280/60000 (2%)] Loss: 0.211159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [54400/60000 (91%)] Loss: 0.132252\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [1920/60000 (3%)] Loss: 0.144663\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [55040/60000 (92%)] Loss: 0.336301\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [2560/60000 (4%)] Loss: 0.184145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [55680/60000 (93%)] Loss: 0.215824\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [14720/60000 (25%)] Loss: 0.136261\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [6400/60000 (11%)] Loss: 0.206372\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [15360/60000 (26%)] Loss: 0.167448\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [7040/60000 (12%)] Loss: 0.262054\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [16000/60000 (27%)] Loss: 0.329925\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [7680/60000 (13%)] Loss: 0.187857\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [16640/60000 (28%)] Loss: 0.217428\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [8320/60000 (14%)] Loss: 0.218296\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [17280/60000 (29%)] Loss: 0.193269\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [8960/60000 (15%)] Loss: 0.153583\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [17920/60000 (30%)] Loss: 0.612403\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [9600/60000 (16%)] Loss: 0.310232\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [18560/60000 (31%)] Loss: 0.147641\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [10240/60000 (17%)] Loss: 0.308171\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [3200/60000 (5%)] Loss: 0.261491\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [56320/60000 (94%)] Loss: 0.174604\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [3840/60000 (6%)] Loss: 0.130561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [56960/60000 (95%)] Loss: 0.249637\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [4480/60000 (7%)] Loss: 0.175854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)] Loss: 0.190419\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [5120/60000 (9%)] Loss: 0.292927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [58240/60000 (97%)] Loss: 0.260852\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [5760/60000 (10%)] Loss: 0.393212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [58880/60000 (98%)] Loss: 0.233371\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)] Loss: 0.202272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [59520/60000 (99%)] Loss: 0.321203\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [7040/60000 (12%)] Loss: 0.248592\u001b[0m\n",
      "\u001b[34mEpoch: 4#011Test set: Average loss: 0.0841, Accuracy: 97.27%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)] Loss: 0.182226\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [10880/60000 (18%)] Loss: 0.218034\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [19840/60000 (33%)] Loss: 0.264663\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [11520/60000 (19%)] Loss: 0.207137\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [20480/60000 (34%)] Loss: 0.150951\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12160/60000 (20%)] Loss: 0.286927\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [21120/60000 (35%)] Loss: 0.164648\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12800/60000 (21%)] Loss: 0.252096\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [21760/60000 (36%)] Loss: 0.567720\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [13440/60000 (22%)] Loss: 0.258997\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [22400/60000 (37%)] Loss: 0.242524\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [14080/60000 (23%)] Loss: 0.253746\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [23040/60000 (38%)] Loss: 0.330521\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [14720/60000 (25%)] Loss: 0.136261\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [7680/60000 (13%)] Loss: 0.193089\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [8320/60000 (14%)] Loss: 0.199753\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 4 with accuracy 0.973\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [8960/60000 (15%)] Loss: 0.148805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [640/60000 (1%)] Loss: 0.171839\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [9600/60000 (16%)] Loss: 0.317136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [1280/60000 (2%)] Loss: 0.211159\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [10240/60000 (17%)] Loss: 0.321737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [1920/60000 (3%)] Loss: 0.144663\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [10880/60000 (18%)] Loss: 0.194125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [2560/60000 (4%)] Loss: 0.184145\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [11520/60000 (19%)] Loss: 0.180461\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [3200/60000 (5%)] Loss: 0.261491\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [23680/60000 (39%)] Loss: 0.223566\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [15360/60000 (26%)] Loss: 0.167448\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [24320/60000 (41%)] Loss: 0.260099\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [16000/60000 (27%)] Loss: 0.329925\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [24960/60000 (42%)] Loss: 0.191499\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [16640/60000 (28%)] Loss: 0.217428\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)] Loss: 0.189520\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [17280/60000 (29%)] Loss: 0.193269\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [26240/60000 (44%)] Loss: 0.170401\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [17920/60000 (30%)] Loss: 0.612403\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [26880/60000 (45%)] Loss: 0.221804\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [18560/60000 (31%)] Loss: 0.147641\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [27520/60000 (46%)] Loss: 0.301175\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [19200/60000 (32%)] Loss: 0.182226\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12160/60000 (20%)] Loss: 0.272553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [3840/60000 (6%)] Loss: 0.130561\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)] Loss: 0.271309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [4480/60000 (7%)] Loss: 0.175854\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [13440/60000 (22%)] Loss: 0.236044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [5120/60000 (9%)] Loss: 0.292927\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [14080/60000 (23%)] Loss: 0.261704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [5760/60000 (10%)] Loss: 0.393212\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [14720/60000 (25%)] Loss: 0.147323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)] Loss: 0.202272\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [15360/60000 (26%)] Loss: 0.172694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [7040/60000 (12%)] Loss: 0.248592\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [16000/60000 (27%)] Loss: 0.289326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [7680/60000 (13%)] Loss: 0.193089\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [28160/60000 (47%)] Loss: 0.277938\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [19840/60000 (33%)] Loss: 0.264663\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [28800/60000 (48%)] Loss: 0.192345\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [20480/60000 (34%)] Loss: 0.150951\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [29440/60000 (49%)] Loss: 0.163941\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [21120/60000 (35%)] Loss: 0.164648\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [30080/60000 (50%)] Loss: 0.136900\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [21760/60000 (36%)] Loss: 0.567720\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [30720/60000 (51%)] Loss: 0.338917\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [22400/60000 (37%)] Loss: 0.242524\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [31360/60000 (52%)] Loss: 0.230936\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [23040/60000 (38%)] Loss: 0.330521\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)] Loss: 0.293617\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [23680/60000 (39%)] Loss: 0.223566\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [16640/60000 (28%)] Loss: 0.220537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [8320/60000 (14%)] Loss: 0.199753\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [17280/60000 (29%)] Loss: 0.180635\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [8960/60000 (15%)] Loss: 0.148805\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [17920/60000 (30%)] Loss: 0.647646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [9600/60000 (16%)] Loss: 0.317136\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [18560/60000 (31%)] Loss: 0.160873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [10240/60000 (17%)] Loss: 0.321737\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)] Loss: 0.162572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [10880/60000 (18%)] Loss: 0.194125\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19840/60000 (33%)] Loss: 0.252349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [11520/60000 (19%)] Loss: 0.180461\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [20480/60000 (34%)] Loss: 0.150062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12160/60000 (20%)] Loss: 0.272553\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [32640/60000 (54%)] Loss: 0.372854\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [24320/60000 (41%)] Loss: 0.260099\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [33280/60000 (55%)] Loss: 0.272421\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [24960/60000 (42%)] Loss: 0.191499\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [33920/60000 (57%)] Loss: 0.191886\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [25600/60000 (43%)] Loss: 0.189520\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [34560/60000 (58%)] Loss: 0.251277\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [26240/60000 (44%)] Loss: 0.170401\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [35200/60000 (59%)] Loss: 0.351280\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [26880/60000 (45%)] Loss: 0.221804\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [35840/60000 (60%)] Loss: 0.222553\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [27520/60000 (46%)] Loss: 0.301175\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [36480/60000 (61%)] Loss: 0.253655\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [28160/60000 (47%)] Loss: 0.277938\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [37120/60000 (62%)] Loss: 0.254907\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [28800/60000 (48%)] Loss: 0.192345\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [37760/60000 (63%)] Loss: 0.243179\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [29440/60000 (49%)] Loss: 0.163941\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)] Loss: 0.275111\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [30080/60000 (50%)] Loss: 0.136900\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [39040/60000 (65%)] Loss: 0.164858\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [30720/60000 (51%)] Loss: 0.338917\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [39680/60000 (66%)] Loss: 0.474335\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [31360/60000 (52%)] Loss: 0.230936\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [40320/60000 (67%)] Loss: 0.308492\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [32000/60000 (53%)] Loss: 0.293617\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [40960/60000 (68%)] Loss: 0.412307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [32640/60000 (54%)] Loss: 0.372854\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [41600/60000 (69%)] Loss: 0.278303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [33280/60000 (55%)] Loss: 0.272421\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Train Epoch: 5 [21120/60000 (35%)] Loss: 0.170458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)] Loss: 0.271309\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [21760/60000 (36%)] Loss: 0.579850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [13440/60000 (22%)] Loss: 0.236044\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [22400/60000 (37%)] Loss: 0.243769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [14080/60000 (23%)] Loss: 0.261704\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [23040/60000 (38%)] Loss: 0.338645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [14720/60000 (25%)] Loss: 0.147323\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [23680/60000 (39%)] Loss: 0.222933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [15360/60000 (26%)] Loss: 0.172694\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [24320/60000 (41%)] Loss: 0.267527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [16000/60000 (27%)] Loss: 0.289326\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [24960/60000 (42%)] Loss: 0.188442\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [16640/60000 (28%)] Loss: 0.220537\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)] Loss: 0.219004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [17280/60000 (29%)] Loss: 0.180635\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [26240/60000 (44%)] Loss: 0.167003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [17920/60000 (30%)] Loss: 0.647646\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [26880/60000 (45%)] Loss: 0.238864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [18560/60000 (31%)] Loss: 0.160873\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [27520/60000 (46%)] Loss: 0.302509\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)] Loss: 0.162572\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [28160/60000 (47%)] Loss: 0.275839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19840/60000 (33%)] Loss: 0.252349\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [28800/60000 (48%)] Loss: 0.168299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [20480/60000 (34%)] Loss: 0.150062\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [29440/60000 (49%)] Loss: 0.168655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [21120/60000 (35%)] Loss: 0.170458\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [30080/60000 (50%)] Loss: 0.127251\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [21760/60000 (36%)] Loss: 0.579850\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [42240/60000 (70%)] Loss: 0.207171\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [33920/60000 (57%)] Loss: 0.191886\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [42880/60000 (71%)] Loss: 0.259641\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [34560/60000 (58%)] Loss: 0.251277\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [43520/60000 (72%)] Loss: 0.236912\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [35200/60000 (59%)] Loss: 0.351280\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [44160/60000 (74%)] Loss: 0.314293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [35840/60000 (60%)] Loss: 0.222553\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)] Loss: 0.299121\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [36480/60000 (61%)] Loss: 0.253655\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [45440/60000 (76%)] Loss: 0.181858\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [37120/60000 (62%)] Loss: 0.254907\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [46080/60000 (77%)] Loss: 0.195808\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [37760/60000 (63%)] Loss: 0.243179\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [30720/60000 (51%)] Loss: 0.353153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [22400/60000 (37%)] Loss: 0.243769\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [31360/60000 (52%)] Loss: 0.222594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [23040/60000 (38%)] Loss: 0.338645\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)] Loss: 0.297456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [23680/60000 (39%)] Loss: 0.222933\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32640/60000 (54%)] Loss: 0.332424\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [24320/60000 (41%)] Loss: 0.267527\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [33280/60000 (55%)] Loss: 0.277326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [24960/60000 (42%)] Loss: 0.188442\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [33920/60000 (57%)] Loss: 0.183336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)] Loss: 0.219004\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [34560/60000 (58%)] Loss: 0.237080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [26240/60000 (44%)] Loss: 0.167003\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [46720/60000 (78%)] Loss: 0.321051\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [38400/60000 (64%)] Loss: 0.275111\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [47360/60000 (79%)] Loss: 0.281878\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [39040/60000 (65%)] Loss: 0.164858\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [48000/60000 (80%)] Loss: 0.100791\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [39680/60000 (66%)] Loss: 0.474335\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [48640/60000 (81%)] Loss: 0.404650\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [40320/60000 (67%)] Loss: 0.308492\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [49280/60000 (82%)] Loss: 0.168071\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [40960/60000 (68%)] Loss: 0.412307\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [49920/60000 (83%)] Loss: 0.454050\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [41600/60000 (69%)] Loss: 0.278303\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [35200/60000 (59%)] Loss: 0.308691\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [26880/60000 (45%)] Loss: 0.238864\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [35840/60000 (60%)] Loss: 0.207768\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [27520/60000 (46%)] Loss: 0.302509\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [36480/60000 (61%)] Loss: 0.239503\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [28160/60000 (47%)] Loss: 0.275839\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [37120/60000 (62%)] Loss: 0.262834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [28800/60000 (48%)] Loss: 0.168299\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [37760/60000 (63%)] Loss: 0.260001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [29440/60000 (49%)] Loss: 0.168655\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)] Loss: 0.322419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [30080/60000 (50%)] Loss: 0.127251\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [39040/60000 (65%)] Loss: 0.203287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [30720/60000 (51%)] Loss: 0.353153\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [50560/60000 (84%)] Loss: 0.184468\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [42240/60000 (70%)] Loss: 0.207171\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)] Loss: 0.204126\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [42880/60000 (71%)] Loss: 0.259641\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [51840/60000 (86%)] Loss: 0.249332\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [43520/60000 (72%)] Loss: 0.236912\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [52480/60000 (87%)] Loss: 0.218672\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [44160/60000 (74%)] Loss: 0.314293\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [53120/60000 (88%)] Loss: 0.523450\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [44800/60000 (75%)] Loss: 0.299121\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [53760/60000 (90%)] Loss: 0.277780\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [45440/60000 (76%)] Loss: 0.181858\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [54400/60000 (91%)] Loss: 0.131471\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [46080/60000 (77%)] Loss: 0.195808\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [55040/60000 (92%)] Loss: 0.362582\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [46720/60000 (78%)] Loss: 0.321051\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [39680/60000 (66%)] Loss: 0.477827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [31360/60000 (52%)] Loss: 0.222594\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [40320/60000 (67%)] Loss: 0.290265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)] Loss: 0.297456\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [40960/60000 (68%)] Loss: 0.413347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32640/60000 (54%)] Loss: 0.332424\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [41600/60000 (69%)] Loss: 0.277700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [33280/60000 (55%)] Loss: 0.277326\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [42240/60000 (70%)] Loss: 0.183723\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [33920/60000 (57%)] Loss: 0.183336\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [42880/60000 (71%)] Loss: 0.256620\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [34560/60000 (58%)] Loss: 0.237080\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [43520/60000 (72%)] Loss: 0.228947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [35200/60000 (59%)] Loss: 0.308691\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [55680/60000 (93%)] Loss: 0.329692\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [47360/60000 (79%)] Loss: 0.281878\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [56320/60000 (94%)] Loss: 0.108889\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [48000/60000 (80%)] Loss: 0.100791\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [56960/60000 (95%)] Loss: 0.282477\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [48640/60000 (81%)] Loss: 0.404650\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)] Loss: 0.208052\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [49280/60000 (82%)] Loss: 0.168071\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [58240/60000 (97%)] Loss: 0.228773\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [49920/60000 (83%)] Loss: 0.454050\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [58880/60000 (98%)] Loss: 0.295286\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [50560/60000 (84%)] Loss: 0.184468\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [59520/60000 (99%)] Loss: 0.268597\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [51200/60000 (85%)] Loss: 0.204126\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44160/60000 (74%)] Loss: 0.302315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [35840/60000 (60%)] Loss: 0.207768\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)] Loss: 0.305232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [36480/60000 (61%)] Loss: 0.239503\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [45440/60000 (76%)] Loss: 0.190244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [37120/60000 (62%)] Loss: 0.262834\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [46080/60000 (77%)] Loss: 0.207361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [37760/60000 (63%)] Loss: 0.260001\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [46720/60000 (78%)] Loss: 0.344687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)] Loss: 0.322419\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [47360/60000 (79%)] Loss: 0.271702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [39040/60000 (65%)] Loss: 0.203287\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [48000/60000 (80%)] Loss: 0.104662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [39680/60000 (66%)] Loss: 0.477827\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [48640/60000 (81%)] Loss: 0.382654\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [40320/60000 (67%)] Loss: 0.290265\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [49280/60000 (82%)] Loss: 0.161315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [40960/60000 (68%)] Loss: 0.413347\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [49920/60000 (83%)] Loss: 0.448338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [41600/60000 (69%)] Loss: 0.277700\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [50560/60000 (84%)] Loss: 0.185104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [42240/60000 (70%)] Loss: 0.183723\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)] Loss: 0.186320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [42880/60000 (71%)] Loss: 0.256620\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51840/60000 (86%)] Loss: 0.245618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [43520/60000 (72%)] Loss: 0.228947\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [52480/60000 (87%)] Loss: 0.185763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44160/60000 (74%)] Loss: 0.302315\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [53120/60000 (88%)] Loss: 0.543138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)] Loss: 0.305232\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [53760/60000 (90%)] Loss: 0.254244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [45440/60000 (76%)] Loss: 0.190244\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [54400/60000 (91%)] Loss: 0.130588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [46080/60000 (77%)] Loss: 0.207361\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [55040/60000 (92%)] Loss: 0.340800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [46720/60000 (78%)] Loss: 0.344687\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [55680/60000 (93%)] Loss: 0.325343\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [47360/60000 (79%)] Loss: 0.271702\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [56320/60000 (94%)] Loss: 0.103922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [48000/60000 (80%)] Loss: 0.104662\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [56960/60000 (95%)] Loss: 0.303123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [48640/60000 (81%)] Loss: 0.382654\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)] Loss: 0.194148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [49280/60000 (82%)] Loss: 0.161315\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 5#011Test set: Average loss: 0.0743, Accuracy: 97.53%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [51840/60000 (86%)] Loss: 0.249332\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [52480/60000 (87%)] Loss: 0.218672\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [53120/60000 (88%)] Loss: 0.523450\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [53760/60000 (90%)] Loss: 0.277780\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [54400/60000 (91%)] Loss: 0.131471\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [55040/60000 (92%)] Loss: 0.362582\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [55680/60000 (93%)] Loss: 0.329692\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [56320/60000 (94%)] Loss: 0.108889\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [56960/60000 (95%)] Loss: 0.282477\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [57600/60000 (96%)] Loss: 0.208052\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [58240/60000 (97%)] Loss: 0.228773\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [58880/60000 (98%)] Loss: 0.295286\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [59520/60000 (99%)] Loss: 0.268597\u001b[0m\n",
      "\u001b[35mEpoch: 5#011Test set: Average loss: 0.0743, Accuracy: 97.53%\n",
      "\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 5 with accuracy 0.975\u001b[0m\n",
      "\u001b[35m[2020-02-17 11:26:45.388 algo-2:46 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[35m2020-02-17 11:26:45,816 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [58240/60000 (97%)] Loss: 0.253291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [49920/60000 (83%)] Loss: 0.448338\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [58880/60000 (98%)] Loss: 0.275624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [50560/60000 (84%)] Loss: 0.185104\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [59520/60000 (99%)] Loss: 0.271151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)] Loss: 0.186320\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:__main__:Epoch: 5#011Test set: Average loss: 0.0749, Accuracy: 97.58%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51840/60000 (86%)] Loss: 0.245618\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [52480/60000 (87%)] Loss: 0.185763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [53120/60000 (88%)] Loss: 0.543138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [53760/60000 (90%)] Loss: 0.254244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [54400/60000 (91%)] Loss: 0.130588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [55040/60000 (92%)] Loss: 0.340800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [55680/60000 (93%)] Loss: 0.325343\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [56320/60000 (94%)] Loss: 0.103922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [56960/60000 (95%)] Loss: 0.303123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)] Loss: 0.194148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [58240/60000 (97%)] Loss: 0.253291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [58880/60000 (98%)] Loss: 0.275624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [59520/60000 (99%)] Loss: 0.271151\u001b[0m\n",
      "\u001b[34mEpoch: 5#011Test set: Average loss: 0.0749, Accuracy: 97.58%\n",
      "\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 5 with accuracy 0.976\u001b[0m\n",
      "\u001b[34m[2020-02-17 11:26:48.638 algo-1:46 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:26:49,012 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-02-17 11:26:50 Uploading - Uploading generated training model\n",
      "2020-02-17 11:27:48 Completed - Training job completed\n",
      "Training seconds: 502\n",
      "Billable seconds: 152\n",
      "Managed Spot Training savings: 69.7%\n"
     ]
    }
   ],
   "source": [
    "estimator2 = PyTorch(entry_point='mnist_ckpoint.py',\n",
    "                    source_dir='code',\n",
    "                    role=role,\n",
    "                    framework_version='1.3.1',\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    metric_definitions=metrics,\n",
    "                    train_use_spot_instances=True,\n",
    "                    train_max_wait=25*60*60,\n",
    "                    train_instance_count=2,\n",
    "                    checkpoint_s3_uri='s3://sagemaker-us-east-1-113147044314/checkpoints',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 5,\n",
    "                        'backend': 'nccl',\n",
    "                    })\n",
    "estimator2.fit({'training': inputs}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.58000183105469\n",
      "Test Loss: 0.07490000128746033\n",
      "Epoch: 5.0\n",
      "Train loss: 0.27115100622177124\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([f\"{m['MetricName']}: {m['Value']}\"for m in estimator2.latest_training_job.describe()['FinalMetricDataList']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a fully trained estimator, we could save it as a deployable model. But before we do so, let's see how we can fine-tune the hyperparameters for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for hyperparameter optimization is to define which parameters should be tried and their respective ranges or categories. We'll showcase how it's done with the learning rate and the batch size, but any parameter can be tried. Actually, by adapting the training script and passing it some parameters, even the network architecture itself could be tried dynamically.\n",
    "\n",
    "We'll also define which metric we want to optimize for, in this case average test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'lr': ContinuousParameter(0.001, 0.1),\n",
    "    'batch-size': CategoricalParameter([32,64,128])\n",
    "}\n",
    "objective_metric_name = 'average test loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'average test loss',\n",
    "                       'Regex': 'Test set: Average loss: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameter ranges and target metric ready, we create a tuner from the estimator we trained before. We are limiting the search to 9 attempts and telling it to try up to 3 parallel training optimizations. Note that your account has to have quotas for the combination of optimization runs **times** the degree of parallelism of the estimator, and that the Bayesian optimization will have a better chance to reduce the total cost when trials are run sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator2,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=9,\n",
    "                            max_parallel_jobs=3,\n",
    "                            early_stopping_type=\"Off\",\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tuner.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once tuning is finished, we can request the best estimator and check its hyperparameters and achieved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-17 11:40:37 Starting - Preparing the instances for training\n",
      "2020-02-17 11:40:37 Downloading - Downloading input data\n",
      "2020-02-17 11:40:37 Training - Training image download completed. Training in progress.\n",
      "2020-02-17 11:40:37 Uploading - Uploading generated training model\n",
      "2020-02-17 11:40:37 Completed - Training job completed\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-02-17 11:38:56,381 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-02-17 11:38:56,384 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average test loss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-02-17 11:38:56,409 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:38:57,825 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:38:58,210 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-02-17 11:38:58,210 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-02-17 11:38:58,210 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-02-17 11:38:58,211 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35m2020-02-17 11:38:58,212 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-02-17 11:38:58,215 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average test loss to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-02-17 11:38:58,239 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2020-02-17 11:38:58,240 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-02-17 11:38:58,533 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-02-17 11:38:58,533 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-02-17 11:38:58,534 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-02-17 11:38:58,534 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmporeixjxn/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22888 sha256=a6c169e72fca9d4dac271fd3ad876dc40ed657741187523fe6979a3def82a74b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-14oziaud/wheels/25/3c/8c/01932a8ba9f6521888a85d919e234b6aae0ebc61271a37b975\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmpuvo_iy4z/module_dir\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22888 sha256=723281ed0ed12b49a0a7f2cff63ea41f49914d02fc6bfeeeaeed440c1125260b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zpg5jc0e/wheels/d0/52/9f/d7744c2b822269c664fcdce74ece051818fc1e5c1d51a7ea6c\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-02-17 11:39:00,448 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average test loss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-02-17 11:39:00,463 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average test loss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-02-17 11:39:00,489 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average test loss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-02-17 11:39:00,516 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator_module\": \"sagemaker.pytorch.estimator\",\n",
      "        \"sagemaker_estimator_class_name\": \"PyTorch\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"lr\": 0.07896950695095702,\n",
      "        \"batch-size\": \"128\",\n",
      "        \"backend\": \"nccl\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-200217-1128-004-d84ef244\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-28-26-417/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_ckpoint\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_ckpoint.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"nccl\",\"batch-size\":\"128\",\"epochs\":5,\"lr\":0.07896950695095702}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist_ckpoint.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator_class_name\":\"PyTorch\",\"sagemaker_estimator_module\":\"sagemaker.pytorch.estimator\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist_ckpoint\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-28-26-417/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator_class_name\":\"PyTorch\",\"sagemaker_estimator_module\":\"sagemaker.pytorch.estimator\"},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"nccl\",\"batch-size\":\"128\",\"epochs\":5,\"lr\":0.07896950695095702},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-200217-1128-004-d84ef244\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-28-26-417/source/sourcedir.tar.gz\",\"module_name\":\"mnist_ckpoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_ckpoint.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"nccl\",\"--batch-size\",\"128\",\"--epochs\",\"5\",\"--lr\",\"0.07896950695095702\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.07896950695095702\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python mnist_ckpoint.py --backend nccl --batch-size 128 --epochs 5 --lr 0.07896950695095702\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[35mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2020-02-17 11:39:00,975 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average test loss to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-02-17 11:39:00,988 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average test loss to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-02-17 11:39:01,014 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average test loss to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-02-17 11:39:01,041 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator_module\": \"sagemaker.pytorch.estimator\",\n",
      "        \"sagemaker_estimator_class_name\": \"PyTorch\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"lr\": 0.07896950695095702,\n",
      "        \"batch-size\": \"128\",\n",
      "        \"backend\": \"nccl\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-200217-1128-004-d84ef244\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-28-26-417/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_ckpoint\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_ckpoint.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"nccl\",\"batch-size\":\"128\",\"epochs\":5,\"lr\":0.07896950695095702}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist_ckpoint.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator_class_name\":\"PyTorch\",\"sagemaker_estimator_module\":\"sagemaker.pytorch.estimator\"}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist_ckpoint\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-28-26-417/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator_class_name\":\"PyTorch\",\"sagemaker_estimator_module\":\"sagemaker.pytorch.estimator\"},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"nccl\",\"batch-size\":\"128\",\"epochs\":5,\"lr\":0.07896950695095702},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-200217-1128-004-d84ef244\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-28-26-417/source/sourcedir.tar.gz\",\"module_name\":\"mnist_ckpoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_ckpoint.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"nccl\",\"--batch-size\",\"128\",\"--epochs\",\"5\",\"--lr\",\"0.07896950695095702\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_LR=0.07896950695095702\u001b[0m\n",
      "\u001b[35mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python mnist_ckpoint.py --backend nccl --batch-size 128 --epochs 5 --lr 0.07896950695095702\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.288304\u001b[0m\n",
      "\u001b[34mStarting training from the beginning\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.205799\u001b[0m\n",
      "\u001b[34mTest checkpoint is 0\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 1.652436\u001b[0m\n",
      "\u001b[34mGet train data sampler and data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 1.396118\u001b[0m\n",
      "\u001b[34mGet test data sampler and data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 0.999448\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [1280/60000 (2%)] Loss: 2.288331\u001b[0m\n",
      "\u001b[35mStarting training from the beginning\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [2560/60000 (4%)] Loss: 2.205932\u001b[0m\n",
      "\u001b[35mTest checkpoint is 0\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [3840/60000 (6%)] Loss: 1.656372\u001b[0m\n",
      "\u001b[35mGet train data sampler and data loader\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [5120/60000 (9%)] Loss: 1.380357\u001b[0m\n",
      "\u001b[35mGet test data sampler and data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 0.787745\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 0.695125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1280/60000 (2%)] Loss: 2.288304\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10240/60000 (17%)] Loss: 0.692007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2560/60000 (4%)] Loss: 2.205799\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [11520/60000 (19%)] Loss: 0.827753\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3840/60000 (6%)] Loss: 1.652436\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 1.004416\u001b[0m\n",
      "\u001b[35mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [7680/60000 (13%)] Loss: 0.770446\u001b[0m\n",
      "\u001b[35mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [8960/60000 (15%)] Loss: 0.686315\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [1280/60000 (2%)] Loss: 2.288331\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)] Loss: 0.661831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5120/60000 (9%)] Loss: 1.396118\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14080/60000 (23%)] Loss: 0.593779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)] Loss: 0.999448\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [15360/60000 (26%)] Loss: 0.568669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7680/60000 (13%)] Loss: 0.787745\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16640/60000 (28%)] Loss: 0.531109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8960/60000 (15%)] Loss: 0.695125\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [10240/60000 (17%)] Loss: 0.723218\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [2560/60000 (4%)] Loss: 2.205932\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [11520/60000 (19%)] Loss: 0.845284\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [3840/60000 (6%)] Loss: 1.656372\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)] Loss: 0.699707\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [5120/60000 (9%)] Loss: 1.380357\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [14080/60000 (23%)] Loss: 0.647216\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [6400/60000 (11%)] Loss: 1.004416\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17920/60000 (30%)] Loss: 0.819001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10240/60000 (17%)] Loss: 0.692007\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)] Loss: 0.546945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [11520/60000 (19%)] Loss: 0.827753\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [20480/60000 (34%)] Loss: 0.377854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)] Loss: 0.661831\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [21760/60000 (36%)] Loss: 0.595245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14080/60000 (23%)] Loss: 0.593779\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [15360/60000 (26%)] Loss: 0.589110\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [7680/60000 (13%)] Loss: 0.770446\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [16640/60000 (28%)] Loss: 0.523857\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [8960/60000 (15%)] Loss: 0.686315\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [17920/60000 (30%)] Loss: 0.803714\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [10240/60000 (17%)] Loss: 0.723218\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)] Loss: 0.552952\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [11520/60000 (19%)] Loss: 0.845284\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [23040/60000 (38%)] Loss: 0.390206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [15360/60000 (26%)] Loss: 0.568669\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [24320/60000 (41%)] Loss: 0.513422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16640/60000 (28%)] Loss: 0.531109\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)] Loss: 0.347733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [17920/60000 (30%)] Loss: 0.819001\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [20480/60000 (34%)] Loss: 0.410409\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/60000 (21%)] Loss: 0.699707\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [21760/60000 (36%)] Loss: 0.597606\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [14080/60000 (23%)] Loss: 0.647216\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [23040/60000 (38%)] Loss: 0.420703\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [15360/60000 (26%)] Loss: 0.589110\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [24320/60000 (41%)] Loss: 0.542406\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [16640/60000 (28%)] Loss: 0.523857\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [26880/60000 (45%)] Loss: 0.434210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)] Loss: 0.546945\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [28160/60000 (47%)] Loss: 0.501766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [20480/60000 (34%)] Loss: 0.377854\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [29440/60000 (49%)] Loss: 0.421869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [21760/60000 (36%)] Loss: 0.595245\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [30720/60000 (51%)] Loss: 0.490808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [23040/60000 (38%)] Loss: 0.390206\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)] Loss: 0.344522\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [17920/60000 (30%)] Loss: 0.803714\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [26880/60000 (45%)] Loss: 0.406683\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19200/60000 (32%)] Loss: 0.552952\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [28160/60000 (47%)] Loss: 0.514783\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [20480/60000 (34%)] Loss: 0.410409\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)] Loss: 0.395994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [24320/60000 (41%)] Loss: 0.513422\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [33280/60000 (55%)] Loss: 0.392624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.347733\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [34560/60000 (58%)] Loss: 0.444606\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [26880/60000 (45%)] Loss: 0.434210\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [35840/60000 (60%)] Loss: 0.386512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [28160/60000 (47%)] Loss: 0.501766\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [29440/60000 (49%)] Loss: 0.467830\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [21760/60000 (36%)] Loss: 0.597606\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [30720/60000 (51%)] Loss: 0.466592\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [23040/60000 (38%)] Loss: 0.420703\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)] Loss: 0.408685\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [24320/60000 (41%)] Loss: 0.542406\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [33280/60000 (55%)] Loss: 0.366086\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.344522\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [37120/60000 (62%)] Loss: 0.296042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [29440/60000 (49%)] Loss: 0.421869\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)] Loss: 0.396202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [30720/60000 (51%)] Loss: 0.490808\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [39680/60000 (66%)] Loss: 0.427056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)] Loss: 0.395994\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [40960/60000 (68%)] Loss: 0.483235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [33280/60000 (55%)] Loss: 0.392624\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [34560/60000 (58%)] Loss: 0.439678\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [26880/60000 (45%)] Loss: 0.406683\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [35840/60000 (60%)] Loss: 0.420112\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [28160/60000 (47%)] Loss: 0.514783\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [37120/60000 (62%)] Loss: 0.314173\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [29440/60000 (49%)] Loss: 0.467830\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [42240/60000 (70%)] Loss: 0.323691\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [34560/60000 (58%)] Loss: 0.444606\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [43520/60000 (72%)] Loss: 0.339118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [35840/60000 (60%)] Loss: 0.386512\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)] Loss: 0.385045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [37120/60000 (62%)] Loss: 0.296042\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [46080/60000 (77%)] Loss: 0.234478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.396202\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)] Loss: 0.371632\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [30720/60000 (51%)] Loss: 0.466592\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [39680/60000 (66%)] Loss: 0.480046\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [32000/60000 (53%)] Loss: 0.408685\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [40960/60000 (68%)] Loss: 0.520895\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [33280/60000 (55%)] Loss: 0.366086\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [42240/60000 (70%)] Loss: 0.366064\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [34560/60000 (58%)] Loss: 0.439678\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [47360/60000 (79%)] Loss: 0.355731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [39680/60000 (66%)] Loss: 0.427056\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [48640/60000 (81%)] Loss: 0.318281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [40960/60000 (68%)] Loss: 0.483235\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [49920/60000 (83%)] Loss: 0.486769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [42240/60000 (70%)] Loss: 0.323691\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [43520/60000 (72%)] Loss: 0.371251\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [35840/60000 (60%)] Loss: 0.420112\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)] Loss: 0.390540\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [37120/60000 (62%)] Loss: 0.314173\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [46080/60000 (77%)] Loss: 0.265893\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.371632\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [47360/60000 (79%)] Loss: 0.372668\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [39680/60000 (66%)] Loss: 0.480046\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)] Loss: 0.283496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [43520/60000 (72%)] Loss: 0.339118\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [52480/60000 (87%)] Loss: 0.225519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)] Loss: 0.385045\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [53760/60000 (90%)] Loss: 0.352145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [46080/60000 (77%)] Loss: 0.234478\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [55040/60000 (92%)] Loss: 0.303695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [47360/60000 (79%)] Loss: 0.355731\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [48640/60000 (81%)] Loss: 0.364005\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [40960/60000 (68%)] Loss: 0.520895\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [49920/60000 (83%)] Loss: 0.501324\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [42240/60000 (70%)] Loss: 0.366064\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)] Loss: 0.360235\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [43520/60000 (72%)] Loss: 0.371251\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [56320/60000 (94%)] Loss: 0.210196\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [48640/60000 (81%)] Loss: 0.318281\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)] Loss: 0.330193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [49920/60000 (83%)] Loss: 0.486769\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [58880/60000 (98%)] Loss: 0.351426\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.283496\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [52480/60000 (87%)] Loss: 0.209717\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [44800/60000 (75%)] Loss: 0.390540\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [53760/60000 (90%)] Loss: 0.347663\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [46080/60000 (77%)] Loss: 0.265893\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [55040/60000 (92%)] Loss: 0.269994\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [47360/60000 (79%)] Loss: 0.372668\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [56320/60000 (94%)] Loss: 0.250747\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [48640/60000 (81%)] Loss: 0.364005\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [52480/60000 (87%)] Loss: 0.225519\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [53760/60000 (90%)] Loss: 0.352145\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)] Loss: 0.251417\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [49920/60000 (83%)] Loss: 0.501324\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [58880/60000 (98%)] Loss: 0.301311\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.360235\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [52480/60000 (87%)] Loss: 0.209717\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [53760/60000 (90%)] Loss: 0.347663\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 1#011Test set: Average loss: 0.1039, Accuracy: 96.66%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [55040/60000 (92%)] Loss: 0.303695\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [56320/60000 (94%)] Loss: 0.210196\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [1280/60000 (2%)] Loss: 0.209730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)] Loss: 0.330193\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [2560/60000 (4%)] Loss: 0.304984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [58880/60000 (98%)] Loss: 0.351426\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3840/60000 (6%)] Loss: 0.199499\u001b[0m\n",
      "\u001b[34mEpoch: 1#011Test set: Average loss: 0.1039, Accuracy: 96.66%\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [5120/60000 (9%)] Loss: 0.401021\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 1#011Test set: Average loss: 0.1009, Accuracy: 96.72%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [55040/60000 (92%)] Loss: 0.269994\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [56320/60000 (94%)] Loss: 0.250747\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)] Loss: 0.252782\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 1 with accuracy 0.967\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [7680/60000 (13%)] Loss: 0.216688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1280/60000 (2%)] Loss: 0.209730\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [8960/60000 (15%)] Loss: 0.178151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [2560/60000 (4%)] Loss: 0.304984\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [1280/60000 (2%)] Loss: 0.214298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [57600/60000 (96%)] Loss: 0.251417\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [2560/60000 (4%)] Loss: 0.290787\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [58880/60000 (98%)] Loss: 0.301311\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [3840/60000 (6%)] Loss: 0.213391\u001b[0m\n",
      "\u001b[35mEpoch: 1#011Test set: Average loss: 0.1009, Accuracy: 96.72%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [5120/60000 (9%)] Loss: 0.392873\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [10240/60000 (17%)] Loss: 0.231830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3840/60000 (6%)] Loss: 0.199499\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [11520/60000 (19%)] Loss: 0.280316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [5120/60000 (9%)] Loss: 0.401021\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)] Loss: 0.312926\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)] Loss: 0.252782\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [14080/60000 (23%)] Loss: 0.269695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [7680/60000 (13%)] Loss: 0.216688\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)] Loss: 0.246687\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 1 with accuracy 0.967\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [7680/60000 (13%)] Loss: 0.223289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [1280/60000 (2%)] Loss: 0.214298\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [8960/60000 (15%)] Loss: 0.210123\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [2560/60000 (4%)] Loss: 0.290787\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [15360/60000 (26%)] Loss: 0.347661\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [8960/60000 (15%)] Loss: 0.178151\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [16640/60000 (28%)] Loss: 0.306761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10240/60000 (17%)] Loss: 0.231830\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [17920/60000 (30%)] Loss: 0.460816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [11520/60000 (19%)] Loss: 0.280316\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [10240/60000 (17%)] Loss: 0.254181\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [3840/60000 (6%)] Loss: 0.213391\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [11520/60000 (19%)] Loss: 0.332309\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [5120/60000 (9%)] Loss: 0.392873\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)] Loss: 0.307060\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [6400/60000 (11%)] Loss: 0.246687\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [14080/60000 (23%)] Loss: 0.234522\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [7680/60000 (13%)] Loss: 0.223289\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)] Loss: 0.222247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.312926\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [20480/60000 (34%)] Loss: 0.227669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [14080/60000 (23%)] Loss: 0.269695\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [21760/60000 (36%)] Loss: 0.327632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [15360/60000 (26%)] Loss: 0.347661\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [15360/60000 (26%)] Loss: 0.339093\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [8960/60000 (15%)] Loss: 0.210123\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [16640/60000 (28%)] Loss: 0.245661\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [10240/60000 (17%)] Loss: 0.254181\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [17920/60000 (30%)] Loss: 0.432615\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [11520/60000 (19%)] Loss: 0.332309\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [23040/60000 (38%)] Loss: 0.317443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [16640/60000 (28%)] Loss: 0.306761\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [24320/60000 (41%)] Loss: 0.220086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [17920/60000 (30%)] Loss: 0.460816\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)] Loss: 0.265498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)] Loss: 0.222247\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [26880/60000 (45%)] Loss: 0.247961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [20480/60000 (34%)] Loss: 0.227669\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)] Loss: 0.198854\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.307060\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [20480/60000 (34%)] Loss: 0.213824\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [14080/60000 (23%)] Loss: 0.234522\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [21760/60000 (36%)] Loss: 0.314102\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [15360/60000 (26%)] Loss: 0.339093\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [23040/60000 (38%)] Loss: 0.335624\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [16640/60000 (28%)] Loss: 0.245661\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [28160/60000 (47%)] Loss: 0.238893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [21760/60000 (36%)] Loss: 0.327632\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [29440/60000 (49%)] Loss: 0.294660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [23040/60000 (38%)] Loss: 0.317443\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [30720/60000 (51%)] Loss: 0.344225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [24320/60000 (41%)] Loss: 0.220086\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)] Loss: 0.209114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.265498\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [24320/60000 (41%)] Loss: 0.193320\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [17920/60000 (30%)] Loss: 0.432615\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)] Loss: 0.287256\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19200/60000 (32%)] Loss: 0.198854\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [26880/60000 (45%)] Loss: 0.252581\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [20480/60000 (34%)] Loss: 0.213824\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [33280/60000 (55%)] Loss: 0.169486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [26880/60000 (45%)] Loss: 0.247961\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [34560/60000 (58%)] Loss: 0.296823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [28160/60000 (47%)] Loss: 0.238893\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [35840/60000 (60%)] Loss: 0.152341\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [29440/60000 (49%)] Loss: 0.294660\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [37120/60000 (62%)] Loss: 0.199128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30720/60000 (51%)] Loss: 0.344225\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [28160/60000 (47%)] Loss: 0.240523\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [21760/60000 (36%)] Loss: 0.314102\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [29440/60000 (49%)] Loss: 0.220678\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [23040/60000 (38%)] Loss: 0.335624\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [30720/60000 (51%)] Loss: 0.293372\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [24320/60000 (41%)] Loss: 0.193320\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)] Loss: 0.174633\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.287256\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)] Loss: 0.363643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)] Loss: 0.209114\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [39680/60000 (66%)] Loss: 0.245445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [33280/60000 (55%)] Loss: 0.169486\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [40960/60000 (68%)] Loss: 0.430386\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [34560/60000 (58%)] Loss: 0.296823\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [33280/60000 (55%)] Loss: 0.169669\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [26880/60000 (45%)] Loss: 0.252581\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [34560/60000 (58%)] Loss: 0.313022\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [28160/60000 (47%)] Loss: 0.240523\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [35840/60000 (60%)] Loss: 0.138339\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [29440/60000 (49%)] Loss: 0.220678\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [42240/60000 (70%)] Loss: 0.360309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [35840/60000 (60%)] Loss: 0.152341\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [43520/60000 (72%)] Loss: 0.306472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [37120/60000 (62%)] Loss: 0.199128\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)] Loss: 0.313499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.363643\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [46080/60000 (77%)] Loss: 0.137043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [39680/60000 (66%)] Loss: 0.245445\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [37120/60000 (62%)] Loss: 0.143277\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [30720/60000 (51%)] Loss: 0.293372\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)] Loss: 0.372441\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [32000/60000 (53%)] Loss: 0.174633\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [39680/60000 (66%)] Loss: 0.299792\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [33280/60000 (55%)] Loss: 0.169669\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [40960/60000 (68%)] Loss: 0.394795\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [34560/60000 (58%)] Loss: 0.313022\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [47360/60000 (79%)] Loss: 0.251836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [40960/60000 (68%)] Loss: 0.430386\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [48640/60000 (81%)] Loss: 0.271775\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [42240/60000 (70%)] Loss: 0.360309\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [49920/60000 (83%)] Loss: 0.337152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [43520/60000 (72%)] Loss: 0.306472\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)] Loss: 0.168223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)] Loss: 0.313499\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [42240/60000 (70%)] Loss: 0.396911\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [35840/60000 (60%)] Loss: 0.138339\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [43520/60000 (72%)] Loss: 0.380243\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [37120/60000 (62%)] Loss: 0.143277\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)] Loss: 0.322291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.372441\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [46080/60000 (77%)] Loss: 0.171335\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [39680/60000 (66%)] Loss: 0.299792\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [52480/60000 (87%)] Loss: 0.165330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [46080/60000 (77%)] Loss: 0.137043\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [53760/60000 (90%)] Loss: 0.200318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [47360/60000 (79%)] Loss: 0.251836\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [55040/60000 (92%)] Loss: 0.180058\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [48640/60000 (81%)] Loss: 0.271775\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [47360/60000 (79%)] Loss: 0.249739\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [40960/60000 (68%)] Loss: 0.394795\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [48640/60000 (81%)] Loss: 0.223127\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [42240/60000 (70%)] Loss: 0.396911\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [49920/60000 (83%)] Loss: 0.378296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [43520/60000 (72%)] Loss: 0.380243\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [56320/60000 (94%)] Loss: 0.154052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [49920/60000 (83%)] Loss: 0.337152\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)] Loss: 0.171224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.168223\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [58880/60000 (98%)] Loss: 0.196067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [52480/60000 (87%)] Loss: 0.165330\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)] Loss: 0.208687\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [44800/60000 (75%)] Loss: 0.322291\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [52480/60000 (87%)] Loss: 0.164748\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [46080/60000 (77%)] Loss: 0.171335\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [53760/60000 (90%)] Loss: 0.224119\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [47360/60000 (79%)] Loss: 0.249739\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [55040/60000 (92%)] Loss: 0.283097\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [48640/60000 (81%)] Loss: 0.223127\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [56320/60000 (94%)] Loss: 0.144348\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [49920/60000 (83%)] Loss: 0.378296\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)] Loss: 0.217741\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.208687\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [58880/60000 (98%)] Loss: 0.221039\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [52480/60000 (87%)] Loss: 0.164748\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 2#011Test set: Average loss: 0.0804, Accuracy: 97.45%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [53760/60000 (90%)] Loss: 0.200318\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [55040/60000 (92%)] Loss: 0.180058\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [1280/60000 (2%)] Loss: 0.102961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [56320/60000 (94%)] Loss: 0.154052\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [2560/60000 (4%)] Loss: 0.272192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)] Loss: 0.171224\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [3840/60000 (6%)] Loss: 0.185304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [58880/60000 (98%)] Loss: 0.196067\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [5120/60000 (9%)] Loss: 0.281725\u001b[0m\n",
      "\u001b[34mEpoch: 2#011Test set: Average loss: 0.0804, Accuracy: 97.45%\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)] Loss: 0.190508\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [7680/60000 (13%)] Loss: 0.186032\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 2 with accuracy 0.975\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [8960/60000 (15%)] Loss: 0.184971\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1280/60000 (2%)] Loss: 0.102961\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [10240/60000 (17%)] Loss: 0.232639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [2560/60000 (4%)] Loss: 0.272192\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 2#011Test set: Average loss: 0.0824, Accuracy: 97.38%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [53760/60000 (90%)] Loss: 0.224119\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [55040/60000 (92%)] Loss: 0.283097\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [1280/60000 (2%)] Loss: 0.114933\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [56320/60000 (94%)] Loss: 0.144348\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [2560/60000 (4%)] Loss: 0.235265\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [57600/60000 (96%)] Loss: 0.217741\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [11520/60000 (19%)] Loss: 0.278280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [3840/60000 (6%)] Loss: 0.185304\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)] Loss: 0.251233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [5120/60000 (9%)] Loss: 0.281725\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [14080/60000 (23%)] Loss: 0.249488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)] Loss: 0.190508\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [3840/60000 (6%)] Loss: 0.187078\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [58880/60000 (98%)] Loss: 0.221039\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [5120/60000 (9%)] Loss: 0.316805\u001b[0m\n",
      "\u001b[35mEpoch: 2#011Test set: Average loss: 0.0824, Accuracy: 97.38%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)] Loss: 0.195064\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [7680/60000 (13%)] Loss: 0.175977\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 2 with accuracy 0.974\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [15360/60000 (26%)] Loss: 0.209715\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [7680/60000 (13%)] Loss: 0.186032\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [16640/60000 (28%)] Loss: 0.338632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [8960/60000 (15%)] Loss: 0.184971\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [17920/60000 (30%)] Loss: 0.330170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [10240/60000 (17%)] Loss: 0.232639\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)] Loss: 0.153021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [11520/60000 (19%)] Loss: 0.278280\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [8960/60000 (15%)] Loss: 0.237728\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [1280/60000 (2%)] Loss: 0.114933\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [10240/60000 (17%)] Loss: 0.239382\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [2560/60000 (4%)] Loss: 0.235265\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [11520/60000 (19%)] Loss: 0.259820\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [3840/60000 (6%)] Loss: 0.187078\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [20480/60000 (34%)] Loss: 0.136784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.251233\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [21760/60000 (36%)] Loss: 0.304526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [14080/60000 (23%)] Loss: 0.249488\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [23040/60000 (38%)] Loss: 0.197292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [15360/60000 (26%)] Loss: 0.209715\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [24320/60000 (41%)] Loss: 0.201261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [16640/60000 (28%)] Loss: 0.338632\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)] Loss: 0.157722\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [5120/60000 (9%)] Loss: 0.316805\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [14080/60000 (23%)] Loss: 0.274392\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [6400/60000 (11%)] Loss: 0.195064\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [15360/60000 (26%)] Loss: 0.230796\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [7680/60000 (13%)] Loss: 0.175977\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [16640/60000 (28%)] Loss: 0.332474\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [8960/60000 (15%)] Loss: 0.237728\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)] Loss: 0.223493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [17920/60000 (30%)] Loss: 0.330170\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [26880/60000 (45%)] Loss: 0.237302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)] Loss: 0.153021\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [28160/60000 (47%)] Loss: 0.249490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [20480/60000 (34%)] Loss: 0.136784\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [29440/60000 (49%)] Loss: 0.240892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [21760/60000 (36%)] Loss: 0.304526\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [17920/60000 (30%)] Loss: 0.398585\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [10240/60000 (17%)] Loss: 0.239382\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)] Loss: 0.180010\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [11520/60000 (19%)] Loss: 0.259820\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [20480/60000 (34%)] Loss: 0.165639\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.157722\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [21760/60000 (36%)] Loss: 0.297916\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [14080/60000 (23%)] Loss: 0.274392\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [30720/60000 (51%)] Loss: 0.197965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [23040/60000 (38%)] Loss: 0.197292\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)] Loss: 0.198591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [24320/60000 (41%)] Loss: 0.201261\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [33280/60000 (55%)] Loss: 0.178137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.223493\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [23040/60000 (38%)] Loss: 0.206077\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [15360/60000 (26%)] Loss: 0.230796\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [24320/60000 (41%)] Loss: 0.206557\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [16640/60000 (28%)] Loss: 0.332474\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)] Loss: 0.225261\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [17920/60000 (30%)] Loss: 0.398585\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [34560/60000 (58%)] Loss: 0.194367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [26880/60000 (45%)] Loss: 0.237302\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [35840/60000 (60%)] Loss: 0.120671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [28160/60000 (47%)] Loss: 0.249490\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [37120/60000 (62%)] Loss: 0.308744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [29440/60000 (49%)] Loss: 0.240892\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)] Loss: 0.301644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [30720/60000 (51%)] Loss: 0.197965\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [26880/60000 (45%)] Loss: 0.240188\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19200/60000 (32%)] Loss: 0.180010\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [28160/60000 (47%)] Loss: 0.283369\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [20480/60000 (34%)] Loss: 0.165639\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [29440/60000 (49%)] Loss: 0.270792\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [21760/60000 (36%)] Loss: 0.297916\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [30720/60000 (51%)] Loss: 0.190321\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [23040/60000 (38%)] Loss: 0.206077\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [39680/60000 (66%)] Loss: 0.229818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)] Loss: 0.198591\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [40960/60000 (68%)] Loss: 0.330362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [33280/60000 (55%)] Loss: 0.178137\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [42240/60000 (70%)] Loss: 0.238626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [34560/60000 (58%)] Loss: 0.194367\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [43520/60000 (72%)] Loss: 0.200158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [35840/60000 (60%)] Loss: 0.120671\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)] Loss: 0.216505\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [24320/60000 (41%)] Loss: 0.206557\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [33280/60000 (55%)] Loss: 0.189754\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.225261\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [34560/60000 (58%)] Loss: 0.195986\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [26880/60000 (45%)] Loss: 0.240188\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [35840/60000 (60%)] Loss: 0.147896\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [28160/60000 (47%)] Loss: 0.283369\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)] Loss: 0.283326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [37120/60000 (62%)] Loss: 0.308744\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [46080/60000 (77%)] Loss: 0.117717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.301644\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [47360/60000 (79%)] Loss: 0.190814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [39680/60000 (66%)] Loss: 0.229818\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [48640/60000 (81%)] Loss: 0.262714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [40960/60000 (68%)] Loss: 0.330362\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [37120/60000 (62%)] Loss: 0.178229\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [29440/60000 (49%)] Loss: 0.270792\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)] Loss: 0.310071\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [30720/60000 (51%)] Loss: 0.190321\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [39680/60000 (66%)] Loss: 0.242710\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [32000/60000 (53%)] Loss: 0.216505\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [49920/60000 (83%)] Loss: 0.199710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [42240/60000 (70%)] Loss: 0.238626\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)] Loss: 0.136030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [43520/60000 (72%)] Loss: 0.200158\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [52480/60000 (87%)] Loss: 0.207217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)] Loss: 0.283326\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [40960/60000 (68%)] Loss: 0.354865\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [33280/60000 (55%)] Loss: 0.189754\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [42240/60000 (70%)] Loss: 0.241838\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [34560/60000 (58%)] Loss: 0.195986\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [43520/60000 (72%)] Loss: 0.236597\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [35840/60000 (60%)] Loss: 0.147896\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)] Loss: 0.283691\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [37120/60000 (62%)] Loss: 0.178229\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [53760/60000 (90%)] Loss: 0.199240\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [46080/60000 (77%)] Loss: 0.117717\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [55040/60000 (92%)] Loss: 0.102370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [47360/60000 (79%)] Loss: 0.190814\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [56320/60000 (94%)] Loss: 0.143249\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [48640/60000 (81%)] Loss: 0.262714\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)] Loss: 0.155006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [49920/60000 (83%)] Loss: 0.199710\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [46080/60000 (77%)] Loss: 0.115764\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.310071\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [47360/60000 (79%)] Loss: 0.159979\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [39680/60000 (66%)] Loss: 0.242710\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [48640/60000 (81%)] Loss: 0.311096\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [40960/60000 (68%)] Loss: 0.354865\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [49920/60000 (83%)] Loss: 0.323307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [42240/60000 (70%)] Loss: 0.241838\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [58880/60000 (98%)] Loss: 0.144104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.136030\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)] Loss: 0.128087\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [43520/60000 (72%)] Loss: 0.236597\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [52480/60000 (87%)] Loss: 0.185924\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [44800/60000 (75%)] Loss: 0.283691\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [53760/60000 (90%)] Loss: 0.210258\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [46080/60000 (77%)] Loss: 0.115764\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [55040/60000 (92%)] Loss: 0.126094\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [47360/60000 (79%)] Loss: 0.159979\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [56320/60000 (94%)] Loss: 0.139574\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [48640/60000 (81%)] Loss: 0.311096\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)] Loss: 0.199880\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [49920/60000 (83%)] Loss: 0.323307\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [58880/60000 (98%)] Loss: 0.101907\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.128087\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 3#011Test set: Average loss: 0.0616, Accuracy: 98.16%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [52480/60000 (87%)] Loss: 0.207217\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [53760/60000 (90%)] Loss: 0.199240\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [1280/60000 (2%)] Loss: 0.095174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [55040/60000 (92%)] Loss: 0.102370\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [2560/60000 (4%)] Loss: 0.228059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [56320/60000 (94%)] Loss: 0.143249\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [3840/60000 (6%)] Loss: 0.182460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)] Loss: 0.155006\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [5120/60000 (9%)] Loss: 0.262042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [58880/60000 (98%)] Loss: 0.144104\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)] Loss: 0.091826\u001b[0m\n",
      "\u001b[34mEpoch: 3#011Test set: Average loss: 0.0616, Accuracy: 98.16%\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [7680/60000 (13%)] Loss: 0.095640\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [8960/60000 (15%)] Loss: 0.228003\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 3 with accuracy 0.982\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [10240/60000 (17%)] Loss: 0.160114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1280/60000 (2%)] Loss: 0.095174\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [11520/60000 (19%)] Loss: 0.283730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [2560/60000 (4%)] Loss: 0.228059\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 3#011Test set: Average loss: 0.0619, Accuracy: 98.09%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [52480/60000 (87%)] Loss: 0.185924\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [53760/60000 (90%)] Loss: 0.210258\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [1280/60000 (2%)] Loss: 0.147156\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [55040/60000 (92%)] Loss: 0.126094\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [2560/60000 (4%)] Loss: 0.185186\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [56320/60000 (94%)] Loss: 0.139574\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)] Loss: 0.207395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [3840/60000 (6%)] Loss: 0.182460\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [14080/60000 (23%)] Loss: 0.284823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [5120/60000 (9%)] Loss: 0.262042\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [15360/60000 (26%)] Loss: 0.194228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)] Loss: 0.091826\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [16640/60000 (28%)] Loss: 0.278829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [7680/60000 (13%)] Loss: 0.095640\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [3840/60000 (6%)] Loss: 0.211845\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [57600/60000 (96%)] Loss: 0.199880\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [5120/60000 (9%)] Loss: 0.276200\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [58880/60000 (98%)] Loss: 0.101907\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)] Loss: 0.135561\u001b[0m\n",
      "\u001b[35mEpoch: 3#011Test set: Average loss: 0.0619, Accuracy: 98.09%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [7680/60000 (13%)] Loss: 0.140168\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [17920/60000 (30%)] Loss: 0.250806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [8960/60000 (15%)] Loss: 0.228003\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)] Loss: 0.117316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [10240/60000 (17%)] Loss: 0.160114\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [20480/60000 (34%)] Loss: 0.176588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [11520/60000 (19%)] Loss: 0.283730\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [8960/60000 (15%)] Loss: 0.237851\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 3 with accuracy 0.981\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [10240/60000 (17%)] Loss: 0.125445\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [1280/60000 (2%)] Loss: 0.147156\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [11520/60000 (19%)] Loss: 0.274460\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [2560/60000 (4%)] Loss: 0.185186\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)] Loss: 0.172070\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [3840/60000 (6%)] Loss: 0.211845\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [21760/60000 (36%)] Loss: 0.347648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)] Loss: 0.207395\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [23040/60000 (38%)] Loss: 0.172337\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [14080/60000 (23%)] Loss: 0.284823\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [24320/60000 (41%)] Loss: 0.130342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [15360/60000 (26%)] Loss: 0.194228\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)] Loss: 0.146434\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [16640/60000 (28%)] Loss: 0.278829\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [14080/60000 (23%)] Loss: 0.259951\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [5120/60000 (9%)] Loss: 0.276200\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [15360/60000 (26%)] Loss: 0.205552\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [6400/60000 (11%)] Loss: 0.135561\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [16640/60000 (28%)] Loss: 0.195434\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [7680/60000 (13%)] Loss: 0.140168\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [26880/60000 (45%)] Loss: 0.171671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [17920/60000 (30%)] Loss: 0.250806\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [28160/60000 (47%)] Loss: 0.244585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)] Loss: 0.117316\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [29440/60000 (49%)] Loss: 0.204975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [20480/60000 (34%)] Loss: 0.176588\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [30720/60000 (51%)] Loss: 0.232403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [21760/60000 (36%)] Loss: 0.347648\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [17920/60000 (30%)] Loss: 0.298054\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [8960/60000 (15%)] Loss: 0.237851\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)] Loss: 0.128879\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [10240/60000 (17%)] Loss: 0.125445\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [20480/60000 (34%)] Loss: 0.200708\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [11520/60000 (19%)] Loss: 0.274460\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [21760/60000 (36%)] Loss: 0.259349\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12800/60000 (21%)] Loss: 0.172070\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)] Loss: 0.114733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [23040/60000 (38%)] Loss: 0.172337\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [33280/60000 (55%)] Loss: 0.133682\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [24320/60000 (41%)] Loss: 0.130342\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [34560/60000 (58%)] Loss: 0.157107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)] Loss: 0.146434\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [23040/60000 (38%)] Loss: 0.162822\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [14080/60000 (23%)] Loss: 0.259951\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [24320/60000 (41%)] Loss: 0.110319\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [15360/60000 (26%)] Loss: 0.205552\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)] Loss: 0.157612\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [16640/60000 (28%)] Loss: 0.195434\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [35840/60000 (60%)] Loss: 0.165738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [26880/60000 (45%)] Loss: 0.171671\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [37120/60000 (62%)] Loss: 0.149218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [28160/60000 (47%)] Loss: 0.244585\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)] Loss: 0.338235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [29440/60000 (49%)] Loss: 0.204975\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [39680/60000 (66%)] Loss: 0.254598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [30720/60000 (51%)] Loss: 0.232403\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [26880/60000 (45%)] Loss: 0.181932\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [17920/60000 (30%)] Loss: 0.298054\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [28160/60000 (47%)] Loss: 0.225790\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [19200/60000 (32%)] Loss: 0.128879\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [29440/60000 (49%)] Loss: 0.191849\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [20480/60000 (34%)] Loss: 0.200708\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [30720/60000 (51%)] Loss: 0.230269\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [21760/60000 (36%)] Loss: 0.259349\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [40960/60000 (68%)] Loss: 0.374526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)] Loss: 0.114733\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [42240/60000 (70%)] Loss: 0.157519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [33280/60000 (55%)] Loss: 0.133682\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [43520/60000 (72%)] Loss: 0.108761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [34560/60000 (58%)] Loss: 0.157107\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)] Loss: 0.295110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [35840/60000 (60%)] Loss: 0.165738\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)] Loss: 0.096426\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [23040/60000 (38%)] Loss: 0.162822\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [33280/60000 (55%)] Loss: 0.225634\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [24320/60000 (41%)] Loss: 0.110319\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [34560/60000 (58%)] Loss: 0.143710\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [25600/60000 (43%)] Loss: 0.157612\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [46080/60000 (77%)] Loss: 0.100381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [37120/60000 (62%)] Loss: 0.149218\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [47360/60000 (79%)] Loss: 0.166522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)] Loss: 0.338235\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [48640/60000 (81%)] Loss: 0.134158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [39680/60000 (66%)] Loss: 0.254598\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [49920/60000 (83%)] Loss: 0.269420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [40960/60000 (68%)] Loss: 0.374526\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [35840/60000 (60%)] Loss: 0.178613\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [26880/60000 (45%)] Loss: 0.181932\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [37120/60000 (62%)] Loss: 0.140536\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [28160/60000 (47%)] Loss: 0.225790\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)] Loss: 0.321777\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [29440/60000 (49%)] Loss: 0.191849\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [39680/60000 (66%)] Loss: 0.227478\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [30720/60000 (51%)] Loss: 0.230269\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)] Loss: 0.095272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [42240/60000 (70%)] Loss: 0.157519\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [52480/60000 (87%)] Loss: 0.115544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [43520/60000 (72%)] Loss: 0.108761\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [53760/60000 (90%)] Loss: 0.122203\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)] Loss: 0.295110\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [55040/60000 (92%)] Loss: 0.183227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [46080/60000 (77%)] Loss: 0.100381\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [40960/60000 (68%)] Loss: 0.336442\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [32000/60000 (53%)] Loss: 0.096426\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [42240/60000 (70%)] Loss: 0.171377\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [33280/60000 (55%)] Loss: 0.225634\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [43520/60000 (72%)] Loss: 0.181335\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [34560/60000 (58%)] Loss: 0.143710\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)] Loss: 0.302314\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [35840/60000 (60%)] Loss: 0.178613\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [56320/60000 (94%)] Loss: 0.121450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [47360/60000 (79%)] Loss: 0.166522\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)] Loss: 0.120973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [48640/60000 (81%)] Loss: 0.134158\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [58880/60000 (98%)] Loss: 0.149621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [49920/60000 (83%)] Loss: 0.269420\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [46080/60000 (77%)] Loss: 0.091949\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [37120/60000 (62%)] Loss: 0.140536\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [47360/60000 (79%)] Loss: 0.145747\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [38400/60000 (64%)] Loss: 0.321777\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [48640/60000 (81%)] Loss: 0.150285\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [39680/60000 (66%)] Loss: 0.227478\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [49920/60000 (83%)] Loss: 0.262628\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [40960/60000 (68%)] Loss: 0.336442\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)] Loss: 0.126311\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [42240/60000 (70%)] Loss: 0.171377\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [52480/60000 (87%)] Loss: 0.079749\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [43520/60000 (72%)] Loss: 0.181335\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [53760/60000 (90%)] Loss: 0.147968\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [44800/60000 (75%)] Loss: 0.302314\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 4#011Test set: Average loss: 0.0649, Accuracy: 97.98%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)] Loss: 0.095272\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [52480/60000 (87%)] Loss: 0.115544\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [55040/60000 (92%)] Loss: 0.171761\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [46080/60000 (77%)] Loss: 0.091949\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [56320/60000 (94%)] Loss: 0.110769\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [47360/60000 (79%)] Loss: 0.145747\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)] Loss: 0.103240\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [48640/60000 (81%)] Loss: 0.150285\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [1280/60000 (2%)] Loss: 0.095645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [53760/60000 (90%)] Loss: 0.122203\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [2560/60000 (4%)] Loss: 0.118477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [55040/60000 (92%)] Loss: 0.183227\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [3840/60000 (6%)] Loss: 0.143730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [56320/60000 (94%)] Loss: 0.121450\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [58880/60000 (98%)] Loss: 0.138886\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [49920/60000 (83%)] Loss: 0.262628\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [5120/60000 (9%)] Loss: 0.167499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)] Loss: 0.120973\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)] Loss: 0.105995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [58880/60000 (98%)] Loss: 0.149621\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [7680/60000 (13%)] Loss: 0.139151\u001b[0m\n",
      "\u001b[34mEpoch: 4#011Test set: Average loss: 0.0649, Accuracy: 97.98%\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [8960/60000 (15%)] Loss: 0.214593\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [10240/60000 (17%)] Loss: 0.123416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [1280/60000 (2%)] Loss: 0.095645\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [11520/60000 (19%)] Loss: 0.167604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [2560/60000 (4%)] Loss: 0.118477\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)] Loss: 0.155970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [3840/60000 (6%)] Loss: 0.143730\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [14080/60000 (23%)] Loss: 0.156049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [5120/60000 (9%)] Loss: 0.167499\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 4#011Test set: Average loss: 0.0577, Accuracy: 98.40%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [51200/60000 (85%)] Loss: 0.126311\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [52480/60000 (87%)] Loss: 0.079749\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [1280/60000 (2%)] Loss: 0.101780\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [53760/60000 (90%)] Loss: 0.147968\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [2560/60000 (4%)] Loss: 0.166870\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [55040/60000 (92%)] Loss: 0.171761\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [15360/60000 (26%)] Loss: 0.166959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)] Loss: 0.105995\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [16640/60000 (28%)] Loss: 0.144861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [7680/60000 (13%)] Loss: 0.139151\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [17920/60000 (30%)] Loss: 0.157666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [8960/60000 (15%)] Loss: 0.214593\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [3840/60000 (6%)] Loss: 0.130608\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [56320/60000 (94%)] Loss: 0.110769\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [5120/60000 (9%)] Loss: 0.211148\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [57600/60000 (96%)] Loss: 0.103240\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)] Loss: 0.144547\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [58880/60000 (98%)] Loss: 0.138886\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)] Loss: 0.089601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [10240/60000 (17%)] Loss: 0.123416\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [20480/60000 (34%)] Loss: 0.149701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [11520/60000 (19%)] Loss: 0.167604\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [21760/60000 (36%)] Loss: 0.215212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)] Loss: 0.155970\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [23040/60000 (38%)] Loss: 0.151343\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [14080/60000 (23%)] Loss: 0.156049\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [7680/60000 (13%)] Loss: 0.098419\u001b[0m\n",
      "\u001b[35mEpoch: 4#011Test set: Average loss: 0.0577, Accuracy: 98.40%\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [8960/60000 (15%)] Loss: 0.174023\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [10240/60000 (17%)] Loss: 0.190274\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 4 with accuracy 0.984\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [11520/60000 (19%)] Loss: 0.199806\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [1280/60000 (2%)] Loss: 0.101780\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [24320/60000 (41%)] Loss: 0.138039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [15360/60000 (26%)] Loss: 0.166959\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)] Loss: 0.192443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [16640/60000 (28%)] Loss: 0.144861\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [26880/60000 (45%)] Loss: 0.218353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [17920/60000 (30%)] Loss: 0.157666\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [28160/60000 (47%)] Loss: 0.211752\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)] Loss: 0.089601\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)] Loss: 0.171720\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [2560/60000 (4%)] Loss: 0.166870\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [14080/60000 (23%)] Loss: 0.141580\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [3840/60000 (6%)] Loss: 0.130608\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [15360/60000 (26%)] Loss: 0.230838\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [5120/60000 (9%)] Loss: 0.211148\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [29440/60000 (49%)] Loss: 0.110454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [20480/60000 (34%)] Loss: 0.149701\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [30720/60000 (51%)] Loss: 0.181939\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [21760/60000 (36%)] Loss: 0.215212\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)] Loss: 0.080278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [23040/60000 (38%)] Loss: 0.151343\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [33280/60000 (55%)] Loss: 0.067539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [24320/60000 (41%)] Loss: 0.138039\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [16640/60000 (28%)] Loss: 0.124031\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [6400/60000 (11%)] Loss: 0.144547\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [17920/60000 (30%)] Loss: 0.163453\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [7680/60000 (13%)] Loss: 0.098419\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)] Loss: 0.068638\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [8960/60000 (15%)] Loss: 0.174023\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [20480/60000 (34%)] Loss: 0.154877\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [10240/60000 (17%)] Loss: 0.190274\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [34560/60000 (58%)] Loss: 0.133761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)] Loss: 0.192443\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [35840/60000 (60%)] Loss: 0.161556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [26880/60000 (45%)] Loss: 0.218353\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [37120/60000 (62%)] Loss: 0.094935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [28160/60000 (47%)] Loss: 0.211752\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)] Loss: 0.238646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [29440/60000 (49%)] Loss: 0.110454\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [21760/60000 (36%)] Loss: 0.174760\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [11520/60000 (19%)] Loss: 0.199806\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [23040/60000 (38%)] Loss: 0.132801\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12800/60000 (21%)] Loss: 0.171720\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [24320/60000 (41%)] Loss: 0.130076\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [14080/60000 (23%)] Loss: 0.141580\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)] Loss: 0.138535\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [15360/60000 (26%)] Loss: 0.230838\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [39680/60000 (66%)] Loss: 0.422760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [30720/60000 (51%)] Loss: 0.181939\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [40960/60000 (68%)] Loss: 0.331939\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)] Loss: 0.080278\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [42240/60000 (70%)] Loss: 0.141517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [33280/60000 (55%)] Loss: 0.067539\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [26880/60000 (45%)] Loss: 0.216104\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [16640/60000 (28%)] Loss: 0.124031\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [28160/60000 (47%)] Loss: 0.159770\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [17920/60000 (30%)] Loss: 0.163453\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [29440/60000 (49%)] Loss: 0.123465\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [19200/60000 (32%)] Loss: 0.068638\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [43520/60000 (72%)] Loss: 0.123129\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [34560/60000 (58%)] Loss: 0.133761\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)] Loss: 0.194073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [35840/60000 (60%)] Loss: 0.161556\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [46080/60000 (77%)] Loss: 0.146433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [37120/60000 (62%)] Loss: 0.094935\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [47360/60000 (79%)] Loss: 0.206178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)] Loss: 0.238646\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [30720/60000 (51%)] Loss: 0.181012\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [20480/60000 (34%)] Loss: 0.154877\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)] Loss: 0.095113\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [21760/60000 (36%)] Loss: 0.174760\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [33280/60000 (55%)] Loss: 0.077620\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [23040/60000 (38%)] Loss: 0.132801\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [34560/60000 (58%)] Loss: 0.177491\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [24320/60000 (41%)] Loss: 0.130076\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [48640/60000 (81%)] Loss: 0.163138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [39680/60000 (66%)] Loss: 0.422760\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [49920/60000 (83%)] Loss: 0.241283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [40960/60000 (68%)] Loss: 0.331939\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)] Loss: 0.098390\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [42240/60000 (70%)] Loss: 0.141517\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [52480/60000 (87%)] Loss: 0.165863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [43520/60000 (72%)] Loss: 0.123129\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [35840/60000 (60%)] Loss: 0.192800\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [25600/60000 (43%)] Loss: 0.138535\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [37120/60000 (62%)] Loss: 0.105824\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [26880/60000 (45%)] Loss: 0.216104\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)] Loss: 0.228664\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [28160/60000 (47%)] Loss: 0.159770\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [39680/60000 (66%)] Loss: 0.386290\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [29440/60000 (49%)] Loss: 0.123465\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [53760/60000 (90%)] Loss: 0.280477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)] Loss: 0.194073\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [55040/60000 (92%)] Loss: 0.111042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [46080/60000 (77%)] Loss: 0.146433\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [56320/60000 (94%)] Loss: 0.162965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [47360/60000 (79%)] Loss: 0.206178\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)] Loss: 0.175598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [48640/60000 (81%)] Loss: 0.163138\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [40960/60000 (68%)] Loss: 0.333088\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [30720/60000 (51%)] Loss: 0.181012\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [42240/60000 (70%)] Loss: 0.178845\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [32000/60000 (53%)] Loss: 0.095113\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [43520/60000 (72%)] Loss: 0.144971\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [33280/60000 (55%)] Loss: 0.077620\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)] Loss: 0.249882\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [34560/60000 (58%)] Loss: 0.177491\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [58880/60000 (98%)] Loss: 0.261657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [49920/60000 (83%)] Loss: 0.241283\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [46080/60000 (77%)] Loss: 0.131082\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [35840/60000 (60%)] Loss: 0.192800\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [47360/60000 (79%)] Loss: 0.196552\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [37120/60000 (62%)] Loss: 0.105824\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [48640/60000 (81%)] Loss: 0.156091\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [38400/60000 (64%)] Loss: 0.228664\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [49920/60000 (83%)] Loss: 0.306252\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [39680/60000 (66%)] Loss: 0.386290\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)] Loss: 0.107695\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [40960/60000 (68%)] Loss: 0.333088\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [52480/60000 (87%)] Loss: 0.223330\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [42240/60000 (70%)] Loss: 0.178845\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [53760/60000 (90%)] Loss: 0.238855\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [43520/60000 (72%)] Loss: 0.144971\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 5#011Test set: Average loss: 0.0534, Accuracy: 98.35%\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)] Loss: 0.098390\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [52480/60000 (87%)] Loss: 0.165863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [53760/60000 (90%)] Loss: 0.280477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [55040/60000 (92%)] Loss: 0.111042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [56320/60000 (94%)] Loss: 0.162965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)] Loss: 0.175598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [58880/60000 (98%)] Loss: 0.261657\u001b[0m\n",
      "\u001b[34mEpoch: 5#011Test set: Average loss: 0.0534, Accuracy: 98.35%\n",
      "\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 5 with accuracy 0.984\u001b[0m\n",
      "\u001b[34m2020-02-17 11:40:22,654 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [55040/60000 (92%)] Loss: 0.105349\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [44800/60000 (75%)] Loss: 0.249882\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [56320/60000 (94%)] Loss: 0.119510\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [46080/60000 (77%)] Loss: 0.131082\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)] Loss: 0.236276\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [47360/60000 (79%)] Loss: 0.196552\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [58880/60000 (98%)] Loss: 0.227414\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [48640/60000 (81%)] Loss: 0.156091\u001b[0m\n",
      "\u001b[35mINFO:__main__:Epoch: 5#011Test set: Average loss: 0.0508, Accuracy: 98.46%\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [49920/60000 (83%)] Loss: 0.306252\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [51200/60000 (85%)] Loss: 0.107695\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [52480/60000 (87%)] Loss: 0.223330\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [53760/60000 (90%)] Loss: 0.238855\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [55040/60000 (92%)] Loss: 0.105349\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [56320/60000 (94%)] Loss: 0.119510\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [57600/60000 (96%)] Loss: 0.236276\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [58880/60000 (98%)] Loss: 0.227414\u001b[0m\n",
      "\u001b[35mEpoch: 5#011Test set: Average loss: 0.0508, Accuracy: 98.46%\n",
      "\u001b[0m\n",
      "\u001b[35mSaving checkpoint for epoch 5 with accuracy 0.985\u001b[0m\n",
      "\u001b[35m2020-02-17 11:40:25,882 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 388\n",
      "Billable seconds: 130\n",
      "Managed Spot Training savings: 66.5%\n"
     ]
    }
   ],
   "source": [
    "best = tuner.best_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tuning_objective_metric': '\"average test loss\"',\n",
       " 'backend': '\"nccl\"',\n",
       " 'batch-size': '\"128\"',\n",
       " 'epochs': '5',\n",
       " 'lr': '0.07896950695095702',\n",
       " 'sagemaker_container_log_level': '20',\n",
       " 'sagemaker_enable_cloudwatch_metrics': 'false',\n",
       " 'sagemaker_estimator_class_name': '\"PyTorch\"',\n",
       " 'sagemaker_estimator_module': '\"sagemaker.pytorch.estimator\"',\n",
       " 'sagemaker_job_name': '\"pytorch-training-2020-02-17-11-28-26-417\"',\n",
       " 'sagemaker_program': '\"mnist_ckpoint.py\"',\n",
       " 'sagemaker_region': '\"us-east-1\"',\n",
       " 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-113147044314/pytorch-training-2020-02-17-11-28-26-417/source/sourcedir.tar.gz\"'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test loss: 0.05079999938607216\n",
      "ObjectiveMetric: 0.05079999938607216\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([f\"{m['MetricName']}: {m['Value']}\"for m in best.latest_training_job.describe()['FinalMetricDataList']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Host\n",
    "### Create endpoint\n",
    "After training, we need to use the `PyTorch` estimator object to create a `PyTorchModel` object and set a different `entry_point`, otherwise, the training script `mnist_ckpoint.py` will be used for inference. (Note that the new `entry_point` must be under the same `source_dir` as `mnist_ckpoint.py`). Then we use the `PyTorchModel` object to deploy a `PyTorchPredictor`. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "An implementation of `model_fn` is required for inference script. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `transform_fm` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "Here's an example of the inference script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel_def\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Net\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = Net()\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job.  Here we will deploy the model to a single ```ml.p3.2xlarge``` instance. Notice that endpoint deployment can take up to 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a PyTorchModel object with a different entry_point\n",
    "model = best.create_model(entry_point='inference.py', source_dir='code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model to an instance\n",
    "predictor_gpu = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge', wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint characteristics:\n",
      "\tAccept: application/x-npy\n",
      "\tContent Type: application/x-npy\n",
      "\tSerializer: <sagemaker.predictor._NPYSerializer object at 0x7fb924f0ea58>\n",
      "\tDeserializer: <sagemaker.predictor._NumpyDeserializer object at 0x7fb924f0e898>\n",
      "\tEndpoint: pytorch-training-200217-1128-004-d84ef244\n"
     ]
    }
   ],
   "source": [
    "print(\"Endpoint characteristics:\\n\"\n",
    "      f\"\\tAccept: {predictor_gpu.accept}\\n\"\n",
    "      f\"\\tContent Type: {predictor_gpu.content_type}\\n\"\n",
    "      f\"\\tSerializer: {predictor_gpu.serializer}\\n\"\n",
    "      f\"\\tDeserializer: {predictor_gpu.deserializer}\\n\"\n",
    "      f\"\\tEndpoint: {predictor_gpu.endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Evaluate\n",
    "We can now use this predictor to classify hand-written digits. Drawing into the image box loads the pixel data into a `data` variable in this notebook, which we can then pass to the `predictor`. Notice that the drawing box only works under the original Jupyter interface (not Jupyter Lab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/Javascript\">\n",
       "    var pixels = [];\n",
       "    for (var i = 0; i < 28*28; i++) pixels[i] = 0;\n",
       "    var click = 0;\n",
       "    var canvas = document.querySelector(\"canvas\");\n",
       "    canvas.addEventListener(\"mousemove\", function(e){\n",
       "        if (e.buttons == 1) {\n",
       "            click = 1;\n",
       "            canvas.getContext(\"2d\").fillStyle = \"rgb(0,0,0)\";\n",
       "            canvas.getContext(\"2d\").fillRect(e.offsetX, e.offsetY, 8, 8);\n",
       "            x = Math.floor(e.offsetY * 0.2);\n",
       "            y = Math.floor(e.offsetX * 0.2) + 1;\n",
       "            for (var dy = 0; dy < 2; dy++){\n",
       "                for (var dx = 0; dx < 2; dx++){\n",
       "                    if ((x + dx < 28) && (y + dy < 28)){\n",
       "                        pixels[(y+dy)+(x+dx)*28] = 1;\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "        } else {\n",
       "            if (click == 1) set_value();\n",
       "            click = 0;\n",
       "        }\n",
       "    });\n",
       "    function clear_value(){\n",
       "        canvas.getContext(\"2d\").fillStyle = \"rgb(255,255,255)\";\n",
       "        canvas.getContext(\"2d\").fillRect(0, 0, 140, 140);\n",
       "        for (var i = 0; i < 28*28; i++) pixels[i] = 0;\n",
       "    }\n",
       "\n",
       "    function set_value(){\n",
       "        var result = \"[[\"\n",
       "        for (var i = 0; i < 28; i++) {\n",
       "            result += \"[\"\n",
       "            for (var j = 0; j < 28; j++) {\n",
       "                result += pixels [i * 28 + j]\n",
       "                if (j < 27) {\n",
       "                    result += \", \"\n",
       "                }\n",
       "            }\n",
       "            result += \"]\"\n",
       "            if (i < 27) {\n",
       "                result += \", \"\n",
       "            }\n",
       "        }\n",
       "        result += \"]]\"\n",
       "        var kernel = IPython.notebook.kernel;\n",
       "        kernel.execute(\"data = \" + result)\n",
       "    }\n",
       "</script>\n",
       "Draw a digit (0-9) inside the box\n",
       "<table>\n",
       "<td style=\"border-style: none;\">\n",
       "<div style=\"border: solid 2px #666; width: 143px; height: 144px;\">\n",
       "<canvas width=\"140\" height=\"140\"></canvas>\n",
       "</div></td>\n",
       "<td style=\"border-style: none;\">\n",
       "<button onclick=\"clear_value()\">Clear</button>\n",
       "</td>\n",
       "</table>\n",
       "\n",
       "<!-- This work has been modified from the original and is licensed under the Apache 2.0 License. -->\n",
       "\n",
       "<!--\n",
       "                                     Apache License\n",
       "                           Version 2.0, January 2004\n",
       "                        http://www.apache.org/licenses/\n",
       "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
       "   1. Definitions.\n",
       "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
       "      and distribution as defined by Sections 1 through 9 of this document.\n",
       "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
       "      the copyright owner that is granting the License.\n",
       "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
       "      other entities that control, are controlled by, or are under common\n",
       "      control with that entity. For the purposes of this definition,\n",
       "      \"control\" means (i) the power, direct or indirect, to cause the\n",
       "      direction or management of such entity, whether by contract or\n",
       "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
       "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
       "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
       "      exercising permissions granted by this License.\n",
       "      \"Source\" form shall mean the preferred form for making modifications,\n",
       "      including but not limited to software source code, documentation\n",
       "      source, and configuration files.\n",
       "      \"Object\" form shall mean any form resulting from mechanical\n",
       "      transformation or translation of a Source form, including but\n",
       "      not limited to compiled object code, generated documentation,\n",
       "      and conversions to other media types.\n",
       "      \"Work\" shall mean the work of authorship, whether in Source or\n",
       "      Object form, made available under the License, as indicated by a\n",
       "      copyright notice that is included in or attached to the work\n",
       "      (an example is provided in the Appendix below).\n",
       "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
       "      form, that is based on (or derived from) the Work and for which the\n",
       "      editorial revisions, annotations, elaborations, or other modifications\n",
       "      represent, as a whole, an original work of authorship. For the purposes\n",
       "      of this License, Derivative Works shall not include works that remain\n",
       "      separable from, or merely link (or bind by name) to the interfaces of,\n",
       "      the Work and Derivative Works thereof.\n",
       "      \"Contribution\" shall mean any work of authorship, including\n",
       "      the original version of the Work and any modifications or additions\n",
       "      to that Work or Derivative Works thereof, that is intentionally\n",
       "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
       "      or by an individual or Legal Entity authorized to submit on behalf of\n",
       "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
       "      means any form of electronic, verbal, or written communication sent\n",
       "      to the Licensor or its representatives, including but not limited to\n",
       "      communication on electronic mailing lists, source code control systems,\n",
       "      and issue tracking systems that are managed by, or on behalf of, the\n",
       "      Licensor for the purpose of discussing and improving the Work, but\n",
       "      excluding communication that is conspicuously marked or otherwise\n",
       "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
       "      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
       "      on behalf of whom a Contribution has been received by Licensor and\n",
       "      subsequently incorporated within the Work.\n",
       "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
       "      this License, each Contributor hereby grants to You a perpetual,\n",
       "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
       "      copyright license to reproduce, prepare Derivative Works of,\n",
       "      publicly display, publicly perform, sublicense, and distribute the\n",
       "      Work and such Derivative Works in Source or Object form.\n",
       "   3. Grant of Patent License. Subject to the terms and conditions of\n",
       "      this License, each Contributor hereby grants to You a perpetual,\n",
       "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
       "      (except as stated in this section) patent license to make, have made,\n",
       "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
       "      where such license applies only to those patent claims licensable\n",
       "      by such Contributor that are necessarily infringed by their\n",
       "      Contribution(s) alone or by combination of their Contribution(s)\n",
       "      with the Work to which such Contribution(s) was submitted. If You\n",
       "      institute patent litigation against any entity (including a\n",
       "      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
       "      or a Contribution incorporated within the Work constitutes direct\n",
       "      or contributory patent infringement, then any patent licenses\n",
       "      granted to You under this License for that Work shall terminate\n",
       "      as of the date such litigation is filed.\n",
       "   4. Redistribution. You may reproduce and distribute copies of the\n",
       "      Work or Derivative Works thereof in any medium, with or without\n",
       "      modifications, and in Source or Object form, provided that You\n",
       "      meet the following conditions:\n",
       "      (a) You must give any other recipients of the Work or\n",
       "          Derivative Works a copy of this License; and\n",
       "      (b) You must cause any modified files to carry prominent notices\n",
       "          stating that You changed the files; and\n",
       "      (c) You must retain, in the Source form of any Derivative Works\n",
       "          that You distribute, all copyright, patent, trademark, and\n",
       "          attribution notices from the Source form of the Work,\n",
       "          excluding those notices that do not pertain to any part of\n",
       "          the Derivative Works; and\n",
       "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
       "          distribution, then any Derivative Works that You distribute must\n",
       "          include a readable copy of the attribution notices contained\n",
       "          within such NOTICE file, excluding those notices that do not\n",
       "          pertain to any part of the Derivative Works, in at least one\n",
       "          of the following places: within a NOTICE text file distributed\n",
       "          as part of the Derivative Works; within the Source form or\n",
       "          documentation, if provided along with the Derivative Works; or,\n",
       "          within a display generated by the Derivative Works, if and\n",
       "          wherever such third-party notices normally appear. The contents\n",
       "          of the NOTICE file are for informational purposes only and\n",
       "          do not modify the License. You may add Your own attribution\n",
       "          notices within Derivative Works that You distribute, alongside\n",
       "          or as an addendum to the NOTICE text from the Work, provided\n",
       "          that such additional attribution notices cannot be construed\n",
       "          as modifying the License.\n",
       "      You may add Your own copyright statement to Your modifications and\n",
       "      may provide additional or different license terms and conditions\n",
       "      for use, reproduction, or distribution of Your modifications, or\n",
       "      for any such Derivative Works as a whole, provided Your use,\n",
       "      reproduction, and distribution of the Work otherwise complies with\n",
       "      the conditions stated in this License.\n",
       "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
       "      any Contribution intentionally submitted for inclusion in the Work\n",
       "      by You to the Licensor shall be under the terms and conditions of\n",
       "      this License, without any additional terms or conditions.\n",
       "      Notwithstanding the above, nothing herein shall supersede or modify\n",
       "      the terms of any separate license agreement you may have executed\n",
       "      with Licensor regarding such Contributions.\n",
       "   6. Trademarks. This License does not grant permission to use the trade\n",
       "      names, trademarks, service marks, or product names of the Licensor,\n",
       "      except as required for reasonable and customary use in describing the\n",
       "      origin of the Work and reproducing the content of the NOTICE file.\n",
       "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
       "      agreed to in writing, Licensor provides the Work (and each\n",
       "      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
       "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
       "      implied, including, without limitation, any warranties or conditions\n",
       "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
       "      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
       "      appropriateness of using or redistributing the Work and assume any\n",
       "      risks associated with Your exercise of permissions under this License.\n",
       "   8. Limitation of Liability. In no event and under no legal theory,\n",
       "      whether in tort (including negligence), contract, or otherwise,\n",
       "      unless required by applicable law (such as deliberate and grossly\n",
       "      negligent acts) or agreed to in writing, shall any Contributor be\n",
       "      liable to You for damages, including any direct, indirect, special,\n",
       "      incidental, or consequential damages of any character arising as a\n",
       "      result of this License or out of the use or inability to use the\n",
       "      Work (including but not limited to damages for loss of goodwill,\n",
       "      work stoppage, computer failure or malfunction, or any and all\n",
       "      other commercial damages or losses), even if such Contributor\n",
       "      has been advised of the possibility of such damages.\n",
       "   9. Accepting Warranty or Additional Liability. While redistributing\n",
       "      the Work or Derivative Works thereof, You may choose to offer,\n",
       "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
       "      or other liability obligations and/or rights consistent with this\n",
       "      License. However, in accepting such obligations, You may act only\n",
       "      on Your own behalf and on Your sole responsibility, not on behalf\n",
       "      of any other Contributor, and only if You agree to indemnify,\n",
       "      defend, and hold each Contributor harmless for any liability\n",
       "      incurred by, or claims asserted against, such Contributor by reason\n",
       "      of your accepting any such warranty or additional liability.\n",
       "   END OF TERMS AND CONDITIONS\n",
       "   APPENDIX: How to apply the Apache License to your work.\n",
       "      To apply the Apache License to your work, attach the following\n",
       "      boilerplate notice, with the fields enclosed by brackets \"{}\"\n",
       "      replaced with your own identifying information. (Don't include\n",
       "      the brackets!)  The text should be enclosed in the appropriate\n",
       "      comment syntax for the file format. We also recommend that a\n",
       "      file or class name and description of purpose be included on the\n",
       "      same \"printed page\" as the copyright notice for easier\n",
       "      identification within third-party archives.\n",
       "   Copyright {yyyy} {name of copyright owner}\n",
       "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "   you may not use this file except in compliance with the License.\n",
       "   You may obtain a copy of the License at\n",
       "       http://www.apache.org/licenses/LICENSE-2.0\n",
       "   Unless required by applicable law or agreed to in writing, software\n",
       "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "   See the License for the specific language governing permissions and\n",
       "   limitations under the License.\n",
       "-->\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "CPU times: user 7.47 ms, sys: 4.08 ms, total: 11.6 ms\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image = np.array([data], dtype=np.float32)\n",
    "response = predictor_gpu.predict(image)\n",
    "prediction = response.argmax(axis=1)[0]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictor Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take some measures of how efficient each instance type is. Given that the model itself is quite simple, we should not see much difference between GPU and CPU powered instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2 ms ± 837 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "results = predictor_gpu.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_gpu.delete_endpoint()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: pytorch-training-200217-1128-004-d84ef244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "predictor_cpu = model.deploy(initial_instance_count=1, instance_type='ml.m5.2xlarge', wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 s ± 61.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "results = [predictor_cpu.predict(image) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_cpu.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always remember to delete the endpoints after trying them out. Otherwise they'll remain active and generate additional costs in your account (even if idle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1:12:28.351083\n",
      "Data Load: 0:00:04.658180\n",
      "Training: 0:14:20.437503\n",
      "HPO: 0:19:08.737477\n",
      "Deploy: 0:09:32.900177\n"
     ]
    }
   ],
   "source": [
    "total_duration = end_time - start_time\n",
    "data_duration = train_start_time - data_start_time\n",
    "train_duration = hpo_start_time - train_start_time\n",
    "hpo_duration = deploy_start_time - hpo_start_time\n",
    "deploy_duration = eval_start_time - deploy_start_time\n",
    "\n",
    "print(f\"Total: {total_duration}\\nData Load: {data_duration}\\nTraining: {train_duration}\\nHPO: {hpo_duration}\\nDeploy: {deploy_duration}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "notice": "Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
